---
title             : "Incentives for Registered Reports from a risk sensitivity perspective"
shorttitle        : "Risk-sensitive publication strategies"

author: 
  - name          : "Anne M. Scheel"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Den Dolech 1, Atlas 9.417, 5600 MB, Eindhoven, The Netherlands"
    email         : "a.m.scheel@tue.nl"
  - name          : "Leo Tiokhin"
    affiliation   : "1"
  - name          : "DaniÃ«l Lakens"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Eindhoven University of Technology"
    
note: "test"

author_note: "test"

#abstract: >
# xxx Abstract xxx

  
#keywords          : "Publication bias, Registered Reports, hypothesis testing"
#wordcount         : "5870"

header-includes:
  - \usepackage{float}
  - \usepackage{framed}
  - \usepackage{caption}
  - \usepackage{setspace}
  - \usepackage{amsmath}
  - \usepackage{wrapfig}
  - \captionsetup[figure]{font={stretch=1, small}, skip=10pt}
  - \captionsetup[textbox]{name=Box,labelsep=period,labelfont=it}
  - \newfloat{textbox}{thp}{lop}
  - \floatname{textbox}{Box}
  - \usepackage[most]{tcolorbox}
  - \definecolor{electricviolet}{rgb}{0.56, 0.0, 1.0}


bibliography      : ["rr-risk-sensitivity.bib","rr-risk-sensitivity_software.bib"]
# IMPORTANT: To successfully knit this document, prr.bib must be edited manually:
# All instances of "howpublished" must be changed to "url". This is an issue for
# three references: Goldacre 2016, Mitchell 2014, and RRR (nd).

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : true
numbersections    : false 
mask              : true
replace_ampersands: no 

documentclass     : "apa6"
lang              : "en-UK"
class             : "man"
output            : papaja::apa6_pdf
---

```{r include = FALSE}
library("papaja")
library("bookdown")
library("rmarkdown")
library("knitr")
library("here")
library("ggplot2")
library("stringr")
```

Registered Reports are an article format designed to reduce publication bias and 'questionable research practices' (QRPs), which distort the published record of research findings in many scientific disciplines [@Chambers2013;@Gopalakrishna2022;@Kepes2022;@Stefan2023;@Franco2014;@deVries2018;@Gerber2001;@Dickersin1993;@Fraser2018;@John2012].
In this format, peer review takes place before data collection and the decision to publish is made before authors, reviewers, and editors know the study results.
In addition to preventing editors from selectively rejecting unfavourable results (in particular negative or null results), this is thought to remove incentives for authors to hide, embellish, or misrepresent results because publication no longer depends on them
<!-- the study's findings  -->
[@Chambers2015].
Initial evidence from psychology and neighbouring disciplines shows that Registered Reports indeed contain much higher rates of negative results than the standard literature [@Scheel2021a;@Allen2019;@OMahony2023].

Advocates of the format have argued that the pre-data publication guarantee should make Registered Reports particularly attractive to researchers [e.g., @Chambers2022].
The argument is that Registered Reports reduce uncertainty about whether and where a study will be published before authors have invested in conducting the study, and that such risk reduction is appealing 
<!-- thus provide relative safety  -->
in a research climate that involves substantial publication pressure in many countries and disciplines [@vanDalen2012;@vanDalen2021;@Tijdink2013;@Waaijer2018;@Miller2011a;@Paruzel-Czachura2021;@Gopalakrishna2022].
However, if strategic concerns about publishability indeed influence researchers' choices for or against Registered Reports, it is unlikely that they would always cause risk aversion (i.e., favouring Registered Reports as a low-risk option).
Researchers' willingness to take risks regarding publication success may instead vary depending on factors such as available resources, time pressure, or competition.
This could create situations in which Registered Reports remain unpopular and would never gain traction without additional incentives or interventions.
And indeed, although uptake is growing exponentially [@Chambers2022], the market share of Registered Reports is currently still much smaller than one might expect if authors saw them as unreservedly beneficial for their careers.
<!-- their career-relevant benefits always exceeded those of standard publications. -->
Here, we examine these possibilities with an agent-based simulation, modelling authors' choices between publication formats as decision making under risk to identify circumstances in which Registered Reports might be used highly selectively, or not at all.


<!-- Here, we present a simulation model to examine these possibilities and identify circumstances under which Registered Reports can be expected to be particularly popular or unpopular. -->
<!-- Here, we use a simulation model based on risk-sensitivity theory to examine how such dynamics could affect the popularity of Registered Reports in different research areas, career stages, and employment situations. -->
<!-- Assuming that a) authors can anticipate the eventual 'publication value' of a study (e.g., which journals may publish it) at the outset of a project and b) this value depends in part on the results of the study in standard reports (due to publication bias),  -->
<!-- (e.g., across career stages, employment conditions, research fields),  -->

<!-- Although strategic concerns about publishability may plausibly influence researchers' choices for or against Registered reports, the consequences are likely more complex. -->
<!-- In particular, it is unlikely that such strategic behaviour would always be risk-averse (i.e., favouring Registered Reports as a low-risk option). -->
<!-- Researchers' willingness to take risks regarding publication sucess may instead vary across career stages, employment conditions, and research fields, potentially creating situations in which Registered Reports would never gain traction without additional incentives or interventions. -->

<!-- An important factor that has received little attention to date is why and in what situations authors choose Registered Reports over the standard publication route. -->
<!-- In the context of the substantial pressure to publish documented in many countries and disciplines [@vanDalen2012;@vanDalen2021;@Tijdink2013;@Waaijer2018;@Miller2011a;@Paruzel-Czachura2021;@Gopalakrishna2022], it has been argued that the pre-data publication guarantee of Registered Reports is advantageous for researchers because it reduces uncertainty about the  -->
<!-- publishability of a study  -->
<!-- and its results at the outset of a project -->
<!-- 'return on investment' of a study -->
<!-- before it is conducted [e.g., @Chambers2022]. -->
<!-- But if strategic concerns about publishability indeed influence researchers' choices for or against Registered Reports, the consequences might be more complex. -->


<!-- Given that standard publications may be less costly or sometimes yield higher payoffs than Registered Reports (e.g., in terms of journal prestige), there should be situations in which authors prefer the greater risk associated with them. -->
<!-- Strategic use of the format might also influence the types of research questions studied in Registered Reports, which can complicate meta-scientific comparisons with the standard literature. -->
<!-- For example, if the choice is influenced by authors' anticipation of which results will be obtained,  -->

## Design and intended functions of Registered Reports
<!-- Registered Reports are an article format designed to make publication decisions independent from the reported research results [@Chambers2013]. -->
<!-- combat publication bias by moving the peer-review process to the planning stage of a study, thus separating  -->
<!-- separate the publication decision from a study's results  -->
<!-- by moving the peer-review process to the planning stage of the study  -->
The review process of Registered Reports is split into two stages.
At Stage 1, reviewers evaluate a pre-study protocol containing the research questions, hypotheses, methods, and planned analyses of a proposed study.
In case of a positive decision, the journal issues an `in-principle acceptance' and commits to publishing the eventual report, regardless of the direction of the results.
Only after in-principle acceptance has been issued do authors move on to data collection and analysis and eventually complete the manuscript.
<!-- Once authors have collected and analysed the data and written up the results,  -->
At Stage 2, the final report (now including the results) is subjected to a second round of peer review, but this time only to ensure that the study was carried out as planned, that the data pass any pre-specified quality checks, and that authors' conclusions are justified by the evidence.
Manuscripts can still be rejected at Stage 2, but only for substantial violations of the Stage-1 protocol or data that are uninterpretable or uninformative (e.g., caused by equipment failure), not for the direction or statistical significance of the results.

Through this process, Registered Reports address publication bias as well as  so-called `questionable research practices' (QRPs).
These two problems are considered important contributors to psychology's replication crisis [@Ferguson2012;@Wagenmakers2012] and to research waste in the biomedical sciences [@Chalmers2009] because they skew the available evidence for scientific claims, causing overconfidence and inflated rates of false-positive inferences.
Publication bias can result from editors and reviewers disproportionately rejecting submissions with negative results ['reviewer bias',@Atkinson1982;@Greenwald1975;@Mahoney1977] or from researchers failing to submit negative results for publication ['file-drawering',@Ensinck2023;@Franco2014;@Rosenthal1979].
In Registered Reports, the 
<!-- virtual publication guarantee  -->
in-principle acceptance issued at Stage 1 addresses both of these issues: 
Editors and reviewers cannot reject the Stage-2 report based on the direction of the results, which also reduces the incentives for authors to file-drawer the study in case of negative results.
<!-- Although authors are still free to withdraw and file-drawer their report in case of negative results, the publication guarantee incentives for doing so are -->
<!-- This guarantee also reduces the incentive for authors to file-drawer the study in case of negative results. -->
QRPs are practices that exploit undisclosed flexibility in data collection and analysis, for example when analysing different justifiable combinations of variables, subsamples, and decision criteria, and only reporting the ones with favourable results, or by presenting _post hoc_ inferences as having been predicted _a priori_ [@Kerr1998;@Gopalakrishna2022;@Simmons2011;@John2012;@Fiedler2016;@Agnoli2017;@Fraser2018].
Registered Reports minimise the risk of QRPs via the two-stage review process, in which the Stage-1 protocol acts as a preregistration and reviewers' task during Stage-2 review is to flag any undisclosed deviations from it.

## Efficacy of Registered Reports

Registered Reports were first launched in 2013 at the journal *Cortex* [@Chambers2013] and are now offered by over 300 journals, predominantly in the behavioural sciences and life sciences (see [cos.io/rr](cos.io/rr)).
Nearly 600 Registered Reports had been published by 2021, with uptake growing exponentially [@Chambers2022].
In Chapter 2, we analysed the first cohort of published Registered Reports in psychology and showed that the first hypothesis reported in these articles was supported in only $44\%$ of cases, compared to $96\%$ in a random sample of standard reports [@Scheel2021a].
Similarly low proportions of positive results were found in partially overlapping samples of Registered Reports in psychology and neuroscience [$39.5\%$, @Allen2019] and in psychology, neuroscience, health, and education [$50.1\%$, @OMahony2023].
These findings suggest that Registered Reports indeed reduce biases that inflate the rate of positive results in the standard literature.
However, the existing estimates are based on purely observational evidence and may thus be confounded by other systematic differences between Registered Reports and standard reports.

Systematic differences would act as confounders if they affected either the probability of a positive result when testing a true hypothesis (statistical power) or the base rate of true hypotheses.
The first option is not supported by current evidence:
A study comparing Registered Reports with matched controls found that Registered Reports have higher median sample sizes and, in blind reviews, are judged to be more rigorous in methodology and analysis and of higher overall quality [@Soderberg2021].
Based on this finding, the increased amount of negative results in Registered Reports is unlikely to be an artifact of lower statistical power or poorer methods.
But the second option$\,$---$\,$a difference in the rate of true hypotheses, or the (prior) probability that the tested hypothesis is true$\,$---$\,$has not yet been directly studied.
<!-- The idea that hypotheses tested in Registered Reports are less often true is not implausible. -->
<!-- It is not implausible to think  -->
The idea that Registered Reports might contain fewer true hypotheses has some plausibility:
If researchers expect that negative results are difficult to publish in standard reports but pose no problem in Registered Reports, they might selectively choose the Registered Report route when studying hypotheses that they think 
<!-- are unlikely to 'work'. -->
will yield negative results.
If researchers additionally perceive the standard publication route as less costly (e.g., more habitual, more flexible, faster, requiring lower sample sizes, etc.), standard reports would plausibly remain the preferred option for hypotheses that researchers are more certain are true and will yield publishable results.

Such an effect could explain why both we and @Allen2019 found that replication studies in the Registered Reports literature had descriptively lower rates of positive results than original studies, although the difference was not significant in either case ($39\%$ vs $50\%$ in Scheel et al., 2021, and $34\%$ vs $45.5\%$ in Allen & Mehler, 2019, though note that the studied samples partially overlap).
As we discussed in Chapter 2, replication attempts may more often than novel research be driven by the suspicion that the tested hypothesis is not true (and that the result of the original study was a false positive).
It might also partially explain differences between our results and those of @OMahony2023, who compared Registered Reports to standard reports that were matched on based on the publishing journal, time of publication, and to a lesser extent research topic, design, and studied population.
<!-- (though last three factors had lower priority). -->
O'Mahony finds a difference in the positive result rate of Registered Reports and standard reports half as large as the one in our study (`r 76-50` vs `r 96-44` percentage points), which compared Registered Reports with a random sample of standard reports (matched only on discipline).
Matching articles more closely could lead to more comparable prior probabilities of the hypotheses tested in both formats and thus account for part of this discrepancy.
However, the two studies also differ in the target population and estimand (O'Mahony analysed all tested hypotheses whereas Scheel et al. focused on the first hypothesis per article), which makes the estimates difficult to compare.

Although differences between hypotheses tested in Registered Reports and standard reports remain speculative at this point, this consideration highlights the importance of understanding the costs and benefits of Registered Reports from the authors' perspective.
If current incentives cause Registered Reports to be used selectively in specific situations or for specific research questions, meta-scientists studying this emerging literature would need to take such factors into account.
Selective use could also lead to a situation in which Registered Reports become associated with certain types of results (e.g., negative results) and devalued if these results are deemed less interesting or important by the research community, making the format unattractive in the long run. 
<!-- Perhaps even more importantly,  -->
More generally, a better understanding of the incentives driving researchers' publication choices can help determine where, when, and by whom Registered Reports are likely to be used or avoided.
Such knowledge could then be used to identify areas in which Registered Reports may not gain popularity naturally and anticipate the need for further intervention (e.g., via policy) when there is a demand for unbiased results. 


## Author incentives for Registered Reports

Registered Reports are generally thought to '[neutralise] bad incentives' [@Chambers2013, p. 609], in particular the incentive to exaggerate or misrepresent a study's results in order to make them more publishable in the standard literature.
This assumption is conditioned on the format: 
Once authors have decided to take the Registered Report route, they can improve their publication chances only via the proposed research question and methods in Stage-1 review, and editors have an interest in selecting informative study designs because they are bound to publishing the study's results even when they turn out negative.
In contrast to standard reports, the results are thus no longer a main target to 'hack' or select on, which should make them less biased and more trustworthy.

The incentives for choosing the Registered Reports route in the first place, however, are less clear.
Advocates of the format have argued that it 'serve[s] the interests of individual scientists' [p. 12, @Chambers2022] because it reduces scientists' risk of investing in research projects whose results turn out to be difficult to publish.
The argument is based on the assumptions that researchers a) are under pressure to amass journal publications [which still are a central currency for hiring and promotion decisions, @Muller2014;@vanDalen2012] and b) face shortfalls in publication output when their studies yield negative results (which are more difficult to publish in the standard literature due to publication bias).
The following quote from a talk by Chris Chambers ([September 2021](https://youtu.be/FiVI3cwVMZI?list=PLChfyH8TVDGmYENpXUDPaeeq2SLh8q9dt&t=1047)) summarises this sentiment:

> And the second main benefit, the one that really is the main big one, the big draw, is that as a researcher you can get your paper accepted before you even start your research and regardless of how the results turn out in the end. So no more playing the *p*-value lottery, gambling on certain results going a certain way, otherwise you won't have your PhD or you won't get your next fellowship or your next grant$\,$---$\,$takes all of that pointless, and I think quite foolish, gambling out of the equation (...)^[https://youtu.be/FiVI3cwVMZI?list=PLChfyH8TVDGmYENpXUDPaeeq2SLh8q9dt&t=1047, from minute 17:27]

But would researchers ever prefer to gamble?
Typically, authors care not only about their studies being published at all, but also about the reputation of the publishing journal as well as citation rates [which are causally influenced by journal rank, @Traag2021].
In standard reports, the career-relevant payoffs associated with a publication can thus vary from very low, for example when authors file-drawer a manuscript because the chances of success do not justify the cost of repeated submissions and revisions [@Ensinck2023],
<!-- is rejected by all consulted journals or when publication would require excessively costly revisions,  -->
to very high, for example when a manuscript is published in an extremely high-impact journal like *Nature* or *Science* and frequently cited. 
Compared to this, the payoffs from Registered Reports have lower variance:
Registered Reports minimise not only the chances of a very low payoff (no publication at all), but also those of a very high payoff (a highly-cited publication in a top journal, unless the Registered Report is conducted at a top journal).
Therefore, as long as the payoff associated with a published Registered Report is not always on par with the best possible outcome of the standard publication route,
<!-- (not every Registered Report will be published in _Nature_),  -->
there will be situations in which the standard route$\,$---$\,$`taking the gamble'$\,$---$\,$is more beneficial for researchers.

## Publication strategies as decision making under risk

Which are those situations?
<!-- When do Registered Reports serve the interests of individual scientists less well than standard reports? -->
Because the payoffs of Registered Reports and the standard publication route differ in variance, authors' choice between the two formats represents _decision making under risk_.
<!-- As explained above, the payoffs of Registered Reports and the standard publication route differ in variance. -->
<!-- This means that authors' choice between the two formats represents _decision making under risk_. -->
<!-- With this framing, we can use  -->
This framing allows us to use tools from the literature on risk-sensitive behaviour to study when Registered Reports serve the interests of individual scientists less well than standard reports.
<!-- for analysing under which circumstances researches may prefer one format over the other. -->
Here, we use risk-sensitivity theory to model factors that influence risk preferences and simulate their effects on researchers' publication strategies.
Following @Winterhalder1999, we define _risk_ as 'unpredictable variation in the outcome of a behavior, with consequences for an organism's fitness or utility' (p. 302).
_Risk aversion_ thus means preferring a low-variance option over a high-variance option, and _risk proneness_ the reverse.^[Note that these definitions differ from those used in expected utility theory, where risk aversion, risk proneness, and risk indifference are defined as concave-down, convex-up, and linear utility functions, respectively.]
Organisms are _risk sensitive_ when they are not only sensitive to the average of outcomes of different behavioural options but also to their variance.


<!-- The 'gambling' invoked here means that because important aspects of a study's results vary in ways that (absent QRPs or fraud) are outside of the researcher's direct control -->
<!-- refers to researchers having no direct control over important aspects of a study's results (absent QRPs or fraud). -->
<!-- Assuming that study results influence publication success in the standard literature, and that publication success influences career success, this renders the standard publication route a gamble for researchers' career prospects. -->
<!-- The same is not true for Registered Reports, where publication success depends almost entirely on factors that are under the researchers' control and can be known at the outset of a project (e.g., interestingness of the research question or methodological strength). -->
<!-- Put another way, the career-related payoffs of the Registered Reports route vary less than those of the standard route. -->

<!-- From this perspective, research results$\,$---$\,$which, absent QRPs and fraud, are not under the researcher's direct control$\,$---$\,$affect the variance of (career-relevant) publication payoffs in standard reports, but not in Registered Reports. -->
<!-- Like humans in general, -->
<!-- Researchers, like other humans, are likely sensitive to variance in payoffs, especially ones that matter for their careers. -->
<!-- But would they always prefer low payoff variance? -->
<!-- Typically, authors care not only about their studies being published at all, but also about the reputation of the publishing journal as well as citation rates [which are causally influenced by journal rank, @Traag2021]. -->
<!-- In standard reports, payoffs associated with a publication may thus vary from very low, for example when authors file-drawer a manuscript because the chances of success do not justify the cost of repeated submissions and revisions [@Ensinck2023], -->
<!-- is rejected by all consulted journals or when publication would require excessively costly revisions,  -->
<!-- to very high, for example when a manuscript is published in an extremely high-impact journal like *Nature* or *Science* and frequently cited.  -->
<!-- In Registered Reports, the pre-data publication guarantee also means that the publishing journal is fixed from the outset of a project.^[ADD NOTE ABOUT PCI] -->
<!-- This not only puts a limit not the worst possible outcome (no publication),^[ADD NOTE SAYING THAT RRS CAN STILL BE REJECTED AT STAGE 2, BUT THAT THIS IS MUCH LESS LIKELY THAN IN SRS] but also on the best possible outcome (publication in a top journal, unless the Registered Report is conducted at a top journal). -->

<!-- This question is important for meta-scientists who analyse published Registered Reports as well as for stakeholders who want the format to spread. -->
<!-- In this study, we try to answer it using tools from the literature on risk-sensitive behaviour. -->
<!-- If Registered Reports do not always serve the interests of individual scientists, understanding the factors that influence their attractiveness  -->
<!-- Understanding when Registered Reports do vs do not serve the interests of individual scientists is important for meta-scientists as well as for stakeholders who want the format to spread. -->

Risk-sensitivity theory is a normative theory developed in behavioural ecology to explain the foraging behaviour of animals.
It was originally designed to determine the optimal food-acquisition strategy for an animal faced with a choice between a relatively stable (low-variance) food source and a risky (high-variance) source that sometimes yields large payoffs and sometimes small payoffs (or none at all). 
Organisms are predicted to be sensitive to such differences in risk when payoffs (e.g., the amount of food) have non-linear consequences for the organism's survival or reproductive fitness.
This is the case when, for example, additional increments of food yield smaller and smaller returns for an animal's fitness, or when amounts below a certain threshold would cause starvation.
<!-- For example, a well-fed animal should prefer foraging in a low-risk patch that always provides just enough food for satiation than in a high-risk patch that sometimes provides twice as much food and sometimes none at all, because additional calories beyond satiation have little additional benefits for the animal's fitness. -->
<!-- In contrast, an animal close to starvation should prefer the high-risk patch if the low-risk patch does not offer enough calories for it to survive the day. -->
In psychology and economics, analogous problems in human decision-making are usually studied with utility-based theories, most prominently expected utility theory and prospect theory.
The predictions of all three theories overlap substantially, but risk-sensitivity theory uses fitness instead of utility as its central currency.
This overcomes weaknesses of expected utility theory and prospect theory caused by the conceptual vagueness of utility [e.g., 'utility is whatever is maximised by human choices', @Cubitt2001].
Despite its initially narrow scope, risk-sensitivity theory has proven itself as a powerful framework for explaining risk-sensitive behaviour in a wide range of situations and species, including humans  [@Kacelnik1996;@Winterhalder1999;@Mishra2014].

## The present study

In the following, we apply 
<!-- key concepts from  -->
risk-sensitivity theory to 
<!-- the publication strategies  -->
the situation of researchers faced with the choice of conducting a Registered Report or pursuing the standard publication route.
Using a simulation model, we explore how four aspects of academic careers and incentive structures that are relevant to risk sensitivity may affect researchers' publication strategies:
whether additional publications yield decreasing or increasing returns for career success, empirical pace (the frequency at which studies can be completed), publication targets that must be met to continue or further one's career, and competition.
<!-- In this chapter, we use a simulation model to explore how properties of academic careers and academic incentive structures that are relevant to risk sensitivity may affect the strategies of researchers choosing between Registered Reports and the standard publication format.  -->
Our goal is to understand in which circumstances Registered Reports should be particularly attractive, particularly unattractive, or particularly prone to selective use.
The results of this analysis may help anticipate research fields and career stages in which the format is unlikely to take foot without additional changes to norms, incentives, or policy, and flag situations in which the results of published Registered Reports may be particularly difficult to compare to the normal literature.
The following sections outline central concepts of risk-sensitivity theory, relate them to characteristics of academic careers, and describe an evolutionary simulation model in which their effects on researchers' risk-sensitive publication decisions are examined.


<!-- Utility can be conceptually vague, which harms the predictive value -->
<!-- Utility can be conceptually vague and introduce circularity problems ('humans make choices that maximise utility; utility is whatever is maximised by human choices'). -->
<!-- Compared to utility, fitness is better anchored in j -->
<!-- Readers from psychology or economics may be more familiar with prospect theory and expected utility theory. -->
<!-- Compared to popular theories on decision making under risk in humans, in particular expected utility theory and prospect theory,  -->

<!-- But would researchers ever choose the gamble over the safe publication? -->
<!-- Unless the net benefit of a Registered Report is always at least as valuable as the best possible outcome that could be achieved through the standard publication route, the answer is 'probably yes'. -->
<!-- Authors deciding between Registered Reports and the standard publication route face the choice between a payoff with low variability (a relatively safe publication in the journal the Stage-1 protocol was submitted to) and a payoff with high variability (anywhere between no publication and a high-impact publication, or even several publications if the project yields enough 'fodder').  -->

<!-- Peer-reviewed publications are a central currency for the careers of academic researchers, both in terms of publication quantity and publication impact [@vanDalen2012;@Muller2014].  -->
<!-- In the standard publication model, researchers face uncertainty about whether and where they will be able to publish the results of their study.  -->
<!-- Translated into currency terms, the career benefit a researcher receives for conducting a study can vary extremely$\,$---$\,$from near zero when the resulting manuscript is rejected by all consulted journals (or when the author file-drawers the study because the chances of success do not justify the cost of repeated submissions and revisions) to an extremely high, perhaps career-making amount when a manuscript is published in a very high-impact journal like *Nature* or *Science*.  -->
<!-- In other words, success in the standard system is highly variable and highly volatile since it hinges on the one factor that is supposed to be outside of researchers' control --- the study results. -->
<!-- This unfortunate combination can be excessively stressful for researchers (especially junior scientists without secure positions) and tempt them to hype, spin, or even fabricate their results. -->

<!-- Registered Reports are designed to serve the research community and other consumers of the scientific literature by protecting against publication bias and QRPs. -->
<!-- A key selling point, however, is that they are thought to 'serve the interests of individual scientists' [p. 12, @Chambers2022] at the same time. -->
<!-- The underlying argument is that because scientists a) need to amass journal publications (which still are a central currency for hiring and promotion decisions) and b) face shortfalls in publication output when their studies yield negative results (which are more difficult to publish in the standard literature due to publication bias), a publication guarantee before data collection -->
<!-- has begun  -->
<!-- should be highly valuable. -->


<!-- * Humans are risk-sensitive, therefore researchers should be too -->
<!-- Like virtually all other organisms, humans are sensitive to risk but do not universally try to minimise  (risk aversion) or maximise (risk proneness) it. -->

<!-- * What does the argument entail? -->
<!--   + RRs are 'safer' than non-RRs (less variance, less risk) -->
<!--   + (safer = better) -->
<!--   + However, the argument that RRs are always more attractive for authors only holds if a) RRs are always worth at least as much as a normal publication or b) authors are always risk averse -->



<!-- Selective use of Registered Reports would need to be taken into account by meta-scientists studying this emerging literature, especially if the factors determining authors' choices also affect the reported results. -->
<!-- The above highlights the importance of understanding of when, where, and by whom Registered Reports are most likely to be used.  -->
<!-- First, knowing which factors influence researchers' choice between Registered Reports and the standard publication route is crucial for interpreting meta-scientific studies that compare the two formats, especially if these factors also affect study results (such as the prior probability of tested hypotheses). -->
<!-- Second, such knowledge can help identify research contexts in which Registered Reports are particularly unlikely to be used and would not gain traction by themselves -->
<!-- and anticipate the need for further intervention (e.g., via policy) when there is a demand for unbiased results.  -->
<!-- In this chapter, we present a simulation model to examine in which situations Registered Reports can be expected to be particularly popular or unpopular. -->
<!-- We focus on a key feature of Registered Reports: the results-independent publication guarantee as an incentive for authors. -->

<!-- Such confounds could also lead to a situation in which Registered Reports become associated with certain types of results (e.g., negative results) and devalued if these results are deemed less interesting or important by the research community, making the format unsustainable in the long run. ``To do: add note that this is likely what happened to several 'journals of negative results' that shut down due to lack of interest.`` -->
<!-- impact the uptake of Registered Reports in the first place. -->

<!-- ``To do:`` -->
<!--   + various journals of negative results to reduce publication bias (these never seem to be successful though and always shut down after a while; add examples/references) -->
<!--   + **Registered Reports** to reduce QRPs and publication bias at the same time (most powerful reform proposal to date) -->


<!-- 'a publishing option that neutralises bad incentives' [p. 609, @Chambers2013] -->
<!-- 'As we look into the next decade, we believe RRs are showing all the signs of becoming a powerful antidote to reporting and publication bias, realigning incentives to ensure that the practices that are best for science$\,$---$\,$transparent, reproducible, accurate reporting$\,$---$\,$also serve the interests of individual scientists.' [p. 12, @Chambers2022] -->

<!-- Other reforms aimed at reducing false positives and improving reproducibility, such as preregistration or open data and code, require an additional effort from authors (writing a preregistration document, preparing a shareable dataset), that will  -->

<!-- However, although it is objectively true that Registered Reports provide more certainty about eventual publication success early in a project, this certainty may not always be preferred over the 'gamble'  -->
<!-- But if the standard publications are indeed a gamble and Registered Reports a safe alternative, does it follow that Registered Reports  -->

<!-- This key innovation is thought to 'neutralise(s) bad incentives' [p. 609, @Chambers2013] that might otherwise tempt authors to hype, spin, p-hack, or even fabricate results in order to increase the publication value of their research. -->

<!-- In light of this, one may think that Registered Reports would be extremely popular among researchers.  -->
<!-- But despite an accelerating growth of publications since 2013, the format's 'market share' of the scientific literature is still minuscule. -->


# Conceptual application of risk-sensitivity theory to publication decisions

This section describes general factors that affect the role of risk for individual's fitness and connects these factors to relevant elements of academic careers. 
In this context, risk-sensitivity theory's focus on reproductive fitness as the central outcome may be seen as misguided.
But although researchers do not forage, grow, reproduce, and die in the *biological* sense (except in their role as human beings in general, of course), they undoubtedly are concerned with factors that influence 1) their survival and 2) the propagation of their traits in an *academic* sense.
Even if we were to assume that researchers are not consciously trying to maximise their 'academic fitness', a competitive job market will by definition select for individuals whose past behaviour increased their prospects.
Such competition can create bottlenecks between early-career and tenured positions in many academic disciplines, which inevitably induce a selection pressure for career-promoting behaviours [@Smaldino2016, for a similar approach, see also @Higginson2016]. 

In applying risk-sensitivity theory to researchers' publishing behaviour, we will therefore conceptualise fitness as career success.
<!-- use a general notion of career success as the central outcome variable in place of reproductive fitness. -->
This decision does not imply that career success is the only or the proximal motivation for researchers' behaviour in practice, just as evolutionary theory does not imply that reproductive success is the only or the proximal motivation for human behaviour in everyday life.
However, we do assume that selection for career-promoting behaviours has a noticeable impact on research practice.

<!-- ``To do:`` -->
<!-- * Refer back to a (not yet existing) section above to say that human decision making is a product of evolution -->

(ref:fitnessplot) Consequences of non-linear fitness functions. Payoffs $b_-$, $b_{safe}$, and $b_+$ are converted into fitness with a diminishing (blue), linear (grey), or increasing (red) returns function.
\par\vspace{.8\baselineskip}
```{r fitnessplot, echo=FALSE, warning=FALSE, out.width="50%", fig.cap="(ref:fitnessplot)", fig.align='center'}
#mainplot

#####
# Important alternative code and settings when using journal mode:
# 1. set fig.height=5 in chunk options (it's 4 in manuscript mode)
# 2. use the plot code below instead of `mainplot`
#    (otherwise font will be too small)!

source(here("code", "plot_fitnesscurves.R"))
fitness_plot
```
\par\vspace{.5\baselineskip}

#### Non-linear fitness functions 

The first and perhaps most ubiquitous factor leading individuals to be risk sensitive are non-linear relationships between the outcomes of an individual's behaviour (e.g., harvested food items, publications) and its reproductive success [@Kacelnik1997].
Consider two options, $O_{safe}$ and $O_{risky}$. 
$O_{safe}$ always gives the same payoff $b_{safe}$, whereas $O_{risky}$ gives either a low payoff $b_-$ or a high payoff $b_+$, each with probability $\frac{1}{2}$.
When $b_{safe} = \frac{(b_- + b_+)}{2}$, $O_{safe}$ and $O_{risky}$ have the same expected payoff.
However, we would only expect an individual to be indifferent between the two options if the consequences of their payoffs for the individual's fitness are linear.
When the function relating payoffs to fitness is instead convex or concave (yielding increasing or diminishing returns, respectively), the expected fitness of $O_{safe}$ and $O_{risky}$ will differ and shift the individual's preference towards risk proneness or risk aversion.
An illustration of this example is shown in Figure\ \@ref(fig:fitnessplot):
While the payoffs $b_-$, $b_{safe}$, and $b_+$ are equidistant on the x-axis, $b_{safe}$ is associated with greater fitness than the average of $b_-$ and $b_+$ when the function is concave, and with lower fitness when the function is convex.
In other words, $O_{safe}$ has greater expected fitness than $O_{risky}$ when returns are diminishing, and $O_{risky}$ has greater expected fitness than $O_{safe}$ when returns are increasing.

Non-linear relationships are arguably the norm in the natural world and linear relationships the exception. 
This plausibly holds for academia as well, where the effect of publication success on researchers' career success might change over time:
For early-career researchers, small increases in the number or impact of publications may have an accelerated effect on career success, whereas established professors may care little about any one additional publication to their record.


#### Number of decision events before evaluation

A second risk-relevant factor considered here is the number of decision events taking place before an individual's fitness is evaluated.
When a risky option is chosen repeatedly, the average of the accumulating payoffs gets closer and closer to the long-run expected payoff.
This means that the danger of loosing out completely by only acquiring the lowest possible payoff of the risky option diminishes, making the risky option relatively more attractive. 
However, this relationship only holds for repeated decision events *before* an individual's fitness is evaluated.
When fitness is evaluated after a single decision event, a risky option is more likely to yield an extreme outcome that translates to zero fitness (i.e., death or an ultimate failure to reproduce).

In situations like this, when a single risky decision might cost an individual's life or offspring, average fitness is best described by the geometric mean instead of the arithmetic mean [@Haaland2019].
The geometric mean is more sensitive to variance because it is multiplicative, capturing the fact that one failure to reproduce can end a genetic lineage.
This circumstance has been shown to produce bet-hedging:
Risk-averse strategies may be more adaptive across many generations even when more risk-prone strategies produce better outcomes in any one generation, simply because risk-proneness is also more likely to lead to extinction by sheer bad luck [@Haaland2019].
While average fitness across generations is best represented with the geometric mean, average fitness *within* a generation is better captured by the arithmetic mean, reflecting the additive accumulation of payoffs from decision events before fitness is evaluated.
Therefore, as the number of decision events per generation (i.e., before fitness is evaluated) increases, the variance-sensitive geometric mean of acquired payoffs becomes relatively less important and the less variance-sensitive arithmetic mean becomes more important.
Consequently, an individual's behaviour should switch from relative risk-aversion to relative risk-proneness.

For the purpose of the present study, 'decision events' refer to researchers' decisions of whether to conduct a Registered Report or pursue the standard publication route.
Because Registered Reports must be submitted before data collection, such decisions occur whenever researchers start a new empirical project that they later may want to publish.^[At the current moment, most researchers likely never consciously consider Registered Reports as a publication option. However, the fact that they _could_ nonetheless renders their pursuit of standard publications a choice, albeit an implicit one.]
The number of decision events before evaluation thus reflects the number of empirical projects that a researcher can conduct before their publication record is considered for hiring, promotion, or grant funding decisions.
We will call this parameter 'empirical pace'.

<!-- Across the academic landscape, empirical pace may vary for several reasons. -->
<!-- In the academic world, decision events before fitness is evaluated ('per generation') may vary for several reasons. -->
Key factors influencing empirical pace are the time and resources required to conduct a study and the time and resources researchers have available.
Empirical pace may thus differ between research areas that vary in speed and/or cost of data collection (e.g., a field relying on online questionnaires _vs_ a field relying on fMRI studies) or between research labs that vary in funding and manpower.
Even career stage might affect empirical pace to some extent, for example because career progress often comes with increased funding and the supervision of junior researchers whose efforts boost the supervisors' output [@Muller2014], and because junior researchers often have short-term contracts that limit the available time for producing research output before their CVs are evaluated for the next application.

(ref:varianceplot) Survival thresholds. When fitness drops to zero below the low threshold (dashed line), individuals should be risk-averse because the outcomes of the low-risk option (narrow distribution) are guaranteed to lie above the threshold and the outcomes of the high-risk option (wide distribution) have a non-negligible risk of falling below the threshold. When fitness drops to zero below the high threshold (dotted line), individuals should be risk-prone because only the high-risk option provides a chance of passing the threshold.

```{r varianceplot, echo=FALSE, warning=FALSE, fig.cap="(ref:varianceplot)", out.width="60%", fig.align='center'}

ggplot(data.frame(x=c(0,1)), aes(x)) + 
  scale_x_continuous(name="payoff", limits = c(0, 1),
                     breaks = c(.33, .67),
                     labels = c("low threshold", "high threshold"),
                     expand = c(0, 0)) +
  scale_y_continuous(name="probability", breaks = c(), 
                     labels = c(), expand = c(0, 0)) +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  coord_fixed(ratio = 1/12)+ 
  annotate("segment", x = .33, xend = .33, y =  0, yend = 7, 
           linetype = "dashed", colour = "grey") +
  annotate("segment", x = .67, xend = .67, y =  0, yend = 7, 
           linetype = "dotted", colour = "grey") +
  stat_function(fun=dnorm, 
                args = list(mean = .5, sd = .1), geom="line") + 
  stat_function(fun=dnorm, 
                args = list(mean = .5, sd = .04), geom="line")
```


#### Survival thresholds and competition

A final important factor for risk-sensitive behaviour are thresholds for survival and reproduction [@Winterhalder1999;@Hurly2003].
Survival thresholds are cutoff points below which an individual's fitness drops to zero, for example due to starvation.
Risk-sensitivity theory predicts that an individual will be risk averse when the resources provided by a low-variance option are sufficient to meet the threshold and risk-prone when they are not [@Mishra2014].
For example, a hummingbird that needs to acquire a certain amount of calories to survive the night will prefer a low-risk food source if the expected amount of calories is above the threshold, but avoid the low-risk source if only a higher-risk source provides a chance of survival.
One such situation is depicted in Figure\ \@ref(fig:varianceplot).

Although comparable cutoff points in academic careers may have somewhat less severe consequences, they certainly exist:
The number and impact of a researcher's publications are often explicit criteria in decisions that are central to the individual's career, such as whether they will be awarded a PhD, whether they will receive grant funding, whether they will be offered a tenure-track position, or whether they will be granted tenure. 
In some of these situations, the cutoff points are absolute and thus resemble survival thresholds in the biological sense, for example PhD regulations that determine a minimal number of peer-reviewed publications for a candidate to be awarded with a doctorate, or tenure contracts that specify minimal publication targets.
In other situations, the cutoff points are relative and depend on the number of eligible candidates, for example when grant funding is awarded to the 10 highest-ranked research proposals or a job is offered to the best candidate from a pool of applicants.
In cases like these, one individual's success diminishes the chances of another --- they represent *competition*. 
In the following, survival thresholds and competition will be treated as separate concepts to examine their differential effects on researchers' publication behaviour.


<!-- Across the academic landscape, differences in empirical pace may thus reflect  -->
<!-- Empirical pace may be affected by several factors. -->
<!-- First, the number of studies one can conduct before being evaluated naturally depends on how much time one has available -->
<!-- before a relevant selection event (award of a PhD or grant, job application, tenure decision) takes place. -->
<!-- This parameter likely varies with career stage: -->
<!-- A PhD student usually has three to four years to achieve a certain required publication output, a postdoc may work on a short-term contract of two years or even one year (after which their CV must be strong enough for the next application), and an assistant professor may have around seven years for receiving tenure.  -->
<!-- Second, empirical productivity depends on available resources, such that researchers in labs with more manpower and better funding can complete more projects per time. -->
<!-- ^[Supporting this idea, a recent analysis of the research output of all clinical psychology chairs in Germany between 2013 and 2022 found that ]  -->
<!-- This factor may also be associated with academic seniority because career progress often comes with greater research funds and the supervision of students and junior researchers whose efforts boost the supervisors' output [@Muller2014]. -->
<!-- Third and finally, it likely varies across research areas as a function of speed and cost of data collection and analysis. -->
<!-- While some fields rely on data sets that may take mere hours to collect and/or be very cheap (e.g., research based on online questionnaires), others require enormous investments in time and/or money (e.g., longitudinal studies or studies requiring expensive equipment like fMRI). -->
<!-- Irrespective of career stage, researchers in fields with fast and cheap data may thus be able to complete many more research projects per time unit than researchers who use slow and expensive data. -->




Each of the risk-relevant factors described above$\,$---$\,$non-linear fitness functions, empirical pace, survival thresholds, and competition$\,$---$\,$likely impacts researchers' decision strategies, including their choices between low-risk and high-risk publication options.
To better understand when a low-risk option like Registered Reports should be particularly attractive or unattractive, we examine the individual and interactive effects of these factors in a simulation model.

# Simulation model
We develop an evolutionary agent-based model which simulates a population of researchers who test hypotheses, (attempt to) publish the results either as Registered Reports or as standard reports, accumulate the payoffs for successful publications, and pass their publication strategies on to the next generation of researchers.

#### Research phase
Consider a population of $n = 500$ researchers. 
Each researcher has a fixed publication strategy $s$, the so-called submission  threshold. 
In each round of the research phase, researchers randomly choose a hypothesis to test in a study.
Hypotheses are true with prior probability $p$, which is uniformly distributed between 0 and 1^[Prior distributions in academic research are likely not uniform, but a realistic cross-disciplinary distribution is hard to establish. For example, molecular epidemiology may deal with predominantly false hypotheses [@Wacholder2004], whereas the social sciences may more commonly test hypotheses that are trivially true [e.g., because they are based on hidden tautologies, @Wallach1994]. The uniform distribution thus represents a pragmatic, agnostic choice that is useful for understanding the basic mechanisms at play.] and known to the researcher. 
Before testing their chosen hypothesis, a researcher compares the prior $p$ of their hypothesis with their publication strategy $s$.
When $p < s$, the researcher chooses to play it safe and conduct a Registered Report to test the hypothesis.
When $p \geq s$, the researcher chooses to gamble and test the hypothesis in a regular study which is then submitted as a standard report.

For simplicity, we assume that $p$ is an ideal objective prior and that researchers' hypothesis tests are free from additional sources of error.
Thus, when a researcher tests hypothesis $i$, they obtain a positive result with probability $p_i$ and a negative result with probability $1-p_i$.
If the researcher chose to submit a Registered Report, their study is published regardless of the result and the researcher receives a payoff $b_{RR}$.
However, if the researcher chose to submit a standard report, they face rampant publication bias: 
Only positive results are publishable as standard reports and yield a payoff $b_{SR+} = 1$, whereas negative results are rejected or file-drawered and yield no payoff, $b_{SR-} = 0$.
For all variations of the model tested here, we assume that the payoff for a Registered Report falls between these bounds, such that $b_{SR-} < b_{RR} < b_{SR+}$. 
This assumption reflects the following considerations:

1. Due to publication bias in the standard literature, negative results are less valuable than positive results ($b_{SR-} < b_{SR+}$), for example because they do not lead to a publication at all, because only very low-impact journals are willing to publish them, or because getting them published requires a lot of extra effort (e.g., via frequent resubmissions following rejection or substantial revisions demanded by reviewers), which diminishes the net reward.
2. For these same reasons, Registered Reports are on average more valuable than standard reports with negative results ($b_{SR-} < b_{RR}$), for example because Registered Reports are offered by journals that may display publication bias for standard reports (rejecting standard report submissions with negative results), or simply because Registered Reports need to be resubmitted less often or require less extensive revisions.
3. On average, standard reports with positive results are more valuable than Registered Reports ($b_{RR} < b_{SR+}$), for example because many high-impact journals do not (yet) offer Registered Reports, because not registering one's study *a priori* makes it easier to spin the results to appear more impactful and thus increases the chances to be published in a high-impact journal, or because Registered Reports may require more effort due to their stricter quality criteria, lowering the net reward.
While proponents of Registered Reports may argue that the format has such tremendous advantages that authors' resulting career benefits are superior to any alternative, this chapter is predicated on the assumption that most researchers currently do not share this view.
Once this changes, the present investigation may happily become redundant.

This entire research cycle$\,$---$\,$choosing a hypothesis, choosing a publication route by comparing its prior $p$ to one's publication strategy $s$, testing the hypothesis, and receiving payoff $b_{RR}$ for a Registered Report or $b_{SR-}$ or $b_{SR+}$ for a positive and negative standard report, respectively$\,$---$\,$is repeated $m$ times.

#### Evaluation phase
At the end of the research phase, researchers' accumulated publication payoffs $b_1 + b_2 + ... + b_m$ are translated into fitness $f$.
Fitness is calculated with a function characterised by exponent $\epsilon$, which determines the shape of the function. $\epsilon = 1$ yields a linear function, $0 < \epsilon < 1$ yields a concave function with diminishing returns, and $\epsilon > 1$ yields a convex function with increasing returns (illustrated in Figure\ \@ref(fig:fitnessplot)):

\begin{align}
f = (\sum_{i=1}^{m} b_i)^\epsilon
\end{align}

However, two situations may cause a researcher's fitness to fall to zero even when their accumulated payoffs are non-zero.
First, the sum of their payoffs may fall below an absolute survival threshold $\delta$, for example when a researcher fails to meet an agreed publication target by the time their 'tenure clock' runs out.
Thus, when $\sum_{i=1}^{m} b_i < \delta$, $f = 0$.
Second, the sum of their payoffs may fall below a relative threshold $\gamma$, which reflects the intensity of competition (e.g., for scarce research grants or positions).
$\gamma$ is the proportion of researchers who are considered for reproduction.
When $\gamma = 1$, all researchers in the population are considered for reproduction and their fitness is calculated according to Eq. 1.
When $\gamma < 1$, the $(1 - \gamma)*500$ least successful researchers receive zero fitness and cannot reproduce.^[In the simulation, $\gamma$ is applied *after* fitness has been calculated, not before. This change has purely technical reasons and leads to the same result as applying $\gamma$ to accumulated payoffs and then calculating fitness because all fitness functions are monotonic increasing and fitness functions do not vary within a population. That is, applying the fitness function does not affect the rank order of researchers in the population.]
For example, $\gamma = 0.1$ means that only those researchers with accumulated payoffs in the top $10\%$ of the population can reproduce, and the fitness of the remaining $90\%$ is set to zero.

| Parameter | Definition                                 | Value [range]      |
|:-----:|:----------------------------------|:------------|
|$n$              | population size                            | 500                |
|$g$              | number of generations                      | 250                |
|$p$              | prior probability of hypotheses            | uniform [0--1]     |
|$b_{SR-}$        | payoff for negative standard report        | 0                  |
|$b_{SR+}$        | payoff for positive standard report        | 1                  |
|$b_{RR}$         | payoff for Registered Report               | [.1, .2, ..., .9]  |
|$\epsilon$       | fitness function exponent                  | [0.2, 1, 5]    |
|$m$              | research cycles per generation ('empirical pace') | [1, 2, 4, 8, 16, 32] |
|$\delta$         | survival threshold below which fitness = 0, expressed as proportion of m  | [0, .25, .5, .75]          |
|$\gamma$         | proportion of most successful researchers selected for reproduction (competition) | [1, .9, .5, .1, .05, .01]   |

Table: Parameter definitions and values

#### Reproduction phase
Finally, the researchers in the current population retire and a new (non-overlapping) generation of researchers is created.
A researcher in the new generation inherits their publication strategy $s$ from a researcher in the previous generation with the probability of the previous researcher's fitness (i.e., the new generation's publication strategies are sampled with replacement from the previous generation, probability-weighted by fitness).
The new generation's publication strategies are inherited with a small amount of random noise, such that $s_{new} = s_{old} + w$, with $w \sim N(\mu = 0, \sigma = 0.01)$.
Authors of similar evolutionary agent-based models have described such hereditary transmission as reflecting mentorship and teaching (e.g., when established professors advise mentees to copy their strategies) or simply a generic social learning process in which successful researchers are more likely to be imitated by others [@Smaldino2016].
Although this interpretation may be useful, the main purpose of this aspect of the model is purely technical and not specifically intended to reflect reality$\,$---$\,$it simply provides the machinery for determining which publication strategies are optimal in the various situations we are investigating.

<!-- This evolutionary dynamic of researchers passing on their traits to other researchers depending on their career success can be seen as reflecting mentorship and explicit teaching, such as when established professors advise their students to use the same strategies, or simply a generic social learning process in which successful researchers are more likely to be imitated by others. -->

### Outcome variable $s$
We study how the evolution of researchers' publication strategies $s$ is affected by the payoff for Registered Reports $b_{RR}$ (relative to the payoffs for standard reports, which are fixed at $b_{SR-} = 0$ and $b_{SR+} = 1$), by the shape of the fitness function determined by exponent $\epsilon$, by the number of research cycles per generation $m$, by survival threshold $\delta$, and by competition $\gamma$ (see Table 1 for an overview of the model parameters and their values considered in the simulation).
It is important to keep in mind that a researcher's publication strategy $s$ is 
<!-- A researcher's submission threshold $s$ is a *strategy*,  -->
not an absolute decision:
It determines *how* the choice between Registered Reports and standard reports is made, not which format is chosen.
As such, $s$ indicates the amount of risk a researcher is willing to take. 
Very low values of $s$ reflect risk proneness:
The researcher prefers to gamble and chooses the standard publication route for almost all hypotheses they encounter, using the Registered Report route only for hypotheses that are virtually guaranteed to be false (and yield negative results).
Very high values of $s$ reflect risk aversion: 
The researcher is unwilling to risk a negative result in a standard report and studies almost all hypotheses they encounter in the Registered Report format, reserving the standard publication route for hypotheses that are virtually guaranteed to be true (and yield positive results).

<!-- The evolved values of $s$ over many generations indicate the optimal strategy for a given set of parameter values. -->
### Simulation approach
We use the evolutionary mechanism of this agent-based model as a means for identifying optimal behaviour under different conditions.
But this goal can also be achieved in other ways.
One non-evolutionary alternative is to calculate expected fitness (i.e., the long-run average) for a wide range of $s$ and determine which strategy maximises it in each condition.
A drawback of this approach is that it does not account for population dynamics and therefore cannot easily simulate the effects of competition.
Because of this limitation, our study is based on the evolutionary model.
However, we validate all analyses except those involving competition on the expected-fitness model and show that both models produce virtually identical results (see Appendix).



(ref:evoplot) Evolution of publication strategy $s$ with 3 different payoffs for Registered Reports ($b_{RR}$). Simulations are based on a population of $n = 500$ researchers over 250 generations, with payoffs for standard reports fixed at 0 for negative results ($b_{SR-} = 0$) and 1 for positive results ($b_{SR+} = 1$), a linear fitness function $\epsilon = 1$, one research cycle per generation ($m = 1$), no survival threshold ($\delta = 0$) and no competition ($\gamma = 1$). Each condition was run 10 times. Thin lines represent the median publication strategy of the population in each run, shaded areas represent the inter-quartile range of publication strategies in the population in each run, and thick lines represent the median of run medians per condition.

```{r evoplot, echo=FALSE, warning=FALSE, fig.cap = "(ref:evoplot)", fig.align = 'center'}

# Important additional chunk option when using journal mode: fig.env="figure*"

knitr::include_graphics(here("plots", "plot_evo.png"))
```

# Simulation results
The results of the simulation models will be presented in order of increasing model complexity. 
We start by explaining the very simple scenarios shown in Figure \@ref(fig:evoplot) and Figure \@ref(fig:epsilonplot).
These scenarios are identical to situations discussed above and the results should thus be unsurprising.
However, while they may seem trivial to some, we hope that these explanations will help unfamiliar readers understand the basic functioning of our model as well as the less intuitive results presented later.

When interpreting the results below, one should bear in mind that the analysed parameter values are inherently arbitrary.
Although the model parameters are intended to capture important characteristics of real-world concepts, their values do not represent real-world units.
The goal of this analysis is to understand the relative effects of the model parameters in a simplified, artificial system, which means that the results are only meaningful in relation to each other. 

## Single research cycle per generation, linear fitness function
The first generation of researchers in each simulation run is initialised with randomly distributed publication strategies $s$ (drawn from a uniform distribution [0--1]), which are then allowed to evolve over the subsequent generations.
<!-- All variations of the simulation model reported here were run with a population size of $n = 500$ researchers over 250 generations and payoffs for negative and positive results in standard reports fixed at $b_{SR-} = 0$ and $b_{SR+} = 1$, respectively. -->
Figure \@ref(fig:evoplot) shows the effect of varying the payoffs for Registered Reports when the fitness function is linear ($\epsilon = 1$), with no survival threshold ($\delta = 0$), no competition ($\gamma = 1$), and one research cycle per generation ($m = 1$).
In this very simple scenario, evolved publication strategies ($s$) approximate the payoff for Registered Reports in each condition, indicating that the optimal publication strategy is always equal to $b_{RR}$ ($s_{optimal} = 0.2$ when $b_{RR} = 0.2$, $s_{optimal} = 0.5$ when $b_{RR} = 0.5$, $s_{optimal} = 0.8$ when $b_{RR} = 0.8$).
<!-- The overall pattern of results is unsurprising$\,$---$\,$the higher the payoff for Registered Reports, the more popular they are. -->
<!-- When $b_{RR}$ is low, Registered Reports are unpopular and only used for the least probable hypotheses; when $b_{RR}$ is high, Registered Reports are very  popular and only hypotheses with extremely high priors are studied in standard reports. -->
<!-- In this very simple case illustrated here, evolved publication strategies approximate the payoff for Registered Reports in each condition, indicating that the optimal publication strategy is always equal to $b_{RR}$ ($s_{optimal} = 0.2$ when $b_{RR} = 0.2$, $s_{optimal} = 0.5$ when $b_{RR} = 0.5$, $s_{optimal} = 0.8$ when $b_{RR} = 0.8$). -->
The reason behind this is the uniform distribution [0--1] of hypothesis priors, the payoff structure $b_{SR-} = 0$ and $b_{SR+} = 1$, and the linear fitness function ($\epsilon = 1$ means that fitness equals payoff).
In this constellation, the expected fitness obtained from a standard report is always equal to the prior of the tested hypothesis:

\begin{align}
E[f_{SR}] = (p * b_{SR+} + (1-p) * b_{SR-})^1 = p * 1 +  (1-p) * 0 = p
\end{align}

For example, testing a hypothesis with $p = 0.2$ in a standard report would yield the expected fitness $E[f_{SR}] = (0.2 * 1 +  0.8 * 0)^1 = 0.2$.
The optimal strategy is to submit a Registered Report whenever the expected fitness provided by a standard report is lower than the fitness provided by a Registered Report, $E[f_{SR}] < b_{RR}$, and thus whenever $p < b_{RR}$.
<!-- The strategy is optimal because it  -->
This ensures that researchers always get the best of both worlds, minimising shortfalls when priors are (too) low and maximising winning chances when priors are (sufficiently) high.
For example, $b_{RR} = 0.5$ is larger than $E[f_{SR}]$ for all hypotheses with $p < 0.5$ but smaller than $E[f_{SR}]$ for all hypotheses with $p > 0.5$.
In this situation, researchers who submit Registered Reports whenever $p<0.5$ and standard reports whenever $p>0.5$ protect themselves against losing a bad bet by instead taking the fixed payoff $b_{RR} = 0.5$, but always play a good bet and thus maximise their chances of winning $b_{SR+} = 1$.
Every alternative is inferior in the long run because researchers with $s > b_{RR}$ lose out on increased chances of publishing a standard report and researchers with $s < b_{RR}$ take unnecessary risks and go empty-handed too often.


(ref:epsilonplot) Effect of fitness functions on evolved publication strategies. Shown are median publication strategies in the final ($250^{th}$) generations of 50 runs for different values of $b_{RR}$ (x-axis) and different fitness functions (characterised by exponent $\epsilon$), with one research cycle per generation ($m = 1$), no survival threshold ($\delta = 0$) and no competition ($\gamma = 1$). Fitness functions with $\epsilon = 0.2$ and $\epsilon = 0.5$ (blue lines) are concave with diminishing returns, functions with $\epsilon = 2$ and $\epsilon = 5$ (red lines) are convex with increasing returns, and the function with $\epsilon = 1$ (grey line) is linear. Small dots represent median $s$ of the final generation in each run, large dots represent the median of these 50 run medians per condition. Error bars represent the $95\%$ capture probability around the median of medians.

```{r epsilonplot, echo=FALSE, warning=FALSE, fig.cap = "(ref:epsilonplot)", out.width="65%", fig.align='center'}

# Important additional chunk option when using journal mode: fig.env="figure*"

knitr::include_graphics(here("plots", "plot_b_epsilon.png"))
```

## Allowing for non-linear fitness functions
<!-- With this basic understanding of the payoff structure in hand, we can take a look at what happens when payoffs have non-linear consequences for researchers' fitness. -->
Arguably, the career benefits researchers receive from publications in the real world are rarely, if ever, linear.
In early career, we may assume a convex fitness function, with each addition to the short publication record of a young researcher yielding increasing returns for their prospects on the job market and their ability to obtain grant funding.
A notable exception may be PhD students who plan to leave academia after obtaining their degree, and for whom the career returns of publications exceeding the PhD requirements are thus strongly decreasing (concave fitness function).
Researchers who stay in academia may experience that the career returns for each additional publication begin to decrease as their publication record grows, meaning that advanced career stages may also be characterised by a concave fitness function. 

Figure \@ref(fig:epsilonplot) contrasts the effects of two concave fitness functions ($\epsilon = 0.2$ and $\epsilon = 0.5$, shown in blue shades) and two convex fitness functions ($\epsilon = 2$ and $\epsilon = 5$, shown in red shades) with a linear function ($\epsilon = 1$, grey line) for different payoffs for Registered Reports, in the same simple scenario with only one research cycle per generation.
The grey line for $\epsilon = 1$ represents the already familiar situation from Figure \@ref(fig:evoplot) above: 
When the fitness function is linear, the optimal strategy is $s_{optimal} = b_{RR}$.
<!-- , making Registered Reports relatively popular when they are worth more than 0.5 and relatively unpopular when they are worth less than 0.5. -->
Non-linear fitness functions deviate from this pattern exactly as expected based on Figure \@ref(fig:fitnessplot).
When additional payoffs yield diminishing returns ($\epsilon <1$), Registered Reports become more attractive even when they are worth less than the expected payoff for standard reports.
As explained above, this is because concave functions 'shrink' the difference between moderate and high payoffs relative to the difference between low and moderate payoffs.
Conversely, when additional payoffs yield increasing returns ($\epsilon > 1$), Registered Reports are unattractive unless their payoffs are almost as large as those for published standard reports because convex functions increase the difference between moderate and high payoffs relative to low versus moderate payoffs.

When different fitness functions are taken to reflect different career stages,
<!-- $\,$---$\,$such that senior researchers' returns on career success per publication (or per increment of publication impact) are diminishing and those of early-career researchers are increasing$\,$---$\,$ -->
this pattern suggests that Registered Reports should be more attractive for senior researchers and a tough sell for early-career researchers.
<!-- This observation is interesting because  -->
<!-- it seems at odds with -->
Interestingly, preliminary empirical evidence suggests the opposite:
Registered Reports appear to be more likely to have early-career researchers as first authors than standard reports [77% vs 67% in the journal _Cortex_, @Chambers2022].
One explanation for this counterintuitive result could be that Registered Reports are disproportionally used by early-career researchers who intend to leave academia and thus have a concave fitness function.
Alternatively, factors or dynamics not considered in this simulation may swamp out the effects of concave _vs_ convex fitness functions, such as younger researchers being more likely to adopt new methods.
<!-- One explanation for such data (if robust) could be that the effect of concave versus convex fitness functions is swamped out by factors unrelated to risk sensitivity (e.g., younger researchers being more likely to adopt new methods). -->
However, as we will see below, the effects of different fitness functions are not always as straightforward as in the simple case illustrated in Figure \@ref(fig:epsilonplot) but produce different results in interaction with other risk-related factors.



(ref:mplot) Effect of research cycles per generation on evolved publication strategies. Shown are median evolved publication strategies ($s$) after 250 generations in 50 runs (tile colour represents the median of 50 run medians) depending on the number of research cycles per generation ($m$, y-axis), different values of $b_{RR}$ (x-axis), and different fitness functions (characterised by exponent $\epsilon$) with no survival threshold ($\delta = 0$) and no competition ($\gamma = 1$).

```{r mplot, echo=FALSE, warning=FALSE, fig.cap = "(ref:mplot)", out.width="100%", fig.align='center'}

# Important additional chunk option when using journal mode: fig.env="figure*"

knitr::include_graphics(here("plots", "plot_m_evo_rocket.png"))
```


## Varying the number of research cycles per generation
The analyses presented so far focused on the simple case of one research cycle (or decision event) per generation, meaning that researchers' fitness was calculated based on the payoff from one single study.
As discussed above, increasing numbers of decision events prior to evaluation may make individuals more risk-prone because single negative outcomes are less catastrophic for reproduction [@Haaland2019].
However, Figure \@ref(fig:mplot) shows that this is not universally true$\,$---$\,$rather, the effect of increasing numbers of research cycles per generation ($m$) depends on the shape of the fitness function.
Moving up on the y-axis of each panel, we see that $s$ decreases (indicating greater risk proneness) only when the fitness function is concave ($\epsilon = 0.2$, left panel) but stay constant when it is linear ($\epsilon = 1$, middle panel) and even *increases* when it is convex ($\epsilon = 5$, right panel).

Why does $m$ appear to have opposite effects for concave and convex fitness functions?
<!-- (representing diminishing and increasing returns, respectively)? -->
<!-- To understand this pattern, -->
As a starting point, it helps to first consider only the bottom row of each panel, where $m = 1$.
These three rows contain the same results as the top, middle, and bottom curves in Figure \@ref(fig:epsilonplot) and show risk aversion when $\epsilon = 0.2$ (i.e., Registered Reports are attractive even when they yield a low payoff), risk proneness when $\epsilon = 5$ (Registered Reports are unattractive even when they yield a high payoff), and a linear strategy $s_{optimal} = b_{RR}$ when $\epsilon = 1$.
From this starting point, the two panels with non-linear fitness functions start to approximate the linear case as $m$ increases.
This pattern reflects the idea that fitness is better captured by the geometric mean when $m$ is low, and better captured by the arithmetic mean when $m$ is high [@Haaland2019].

To better understand this dynamic, let's consider two researchers with extreme submission strategies:
Regina Register conducts only Registered Reports ($s_{Regina} = 1$), Darren Daring conducts only standard reports ($s_{Darren} = 0$).
The payoff for Registered Reports is fixed at $b_{RR} = 0.5$.
After one research cycle, Regina receives a payoff of 0.5 and Darren receives either 0 or 1 (with 50/50 odds).
If fitness is calculated after this one round with $\epsilon = 0.2$ (concave function, yielding diminishing returns), Regina's fitness is $f_{Regina} = \frac{1}{2}^{\frac{1}{5}} = `r round(0.5^0.2, 2)`$, and Darren's fitness is either $f_{Darren-} = 0^{\frac{1}{5}} = 0$ or $f_{Darren+} = 1^{\frac{1}{5}} = 1$.
In a population of 100 Reginas and 100 Darrens, there will be roughly 50 lucky Darrens who get a positive result and 50 Darrens who get a negative result. 
Lucky Darrens have a narrow fitness advantage over all Reginas (1 versus `r round(0.5^0.2, 2)`), while unlucky Darrens lose to all Reginas by a wide margin (0 versus `r round(0.5^0.2, 2)`).
Since there are twice as many Reginas as lucky Darrens, the Regina strategy is relatively more successful.

Let's now consider the same scenario with $m = 4$ research cycles per generation.
Reginas receive the same payoff in every round and accumulate $b_{total} = \frac{1}{2} * 4 = 2$.
Lucky Darrens (who win every time) accumulate $b_{total} = 1*4 = 4$, while unlucky Darrens (who lose every time) again receive 0 total payoff.
Now, however, the probabilistic outcomes over 4 rounds lead to three additional versions of Darren: moderately lucky (winning 3/4 times), average (2/4, receiving the same total payoff as Reginas), and moderately unlucky (1/4).
<!-- These outcomes are more likely to occur than the edge cases of winning every time or losing every time, making lucky and unlucky Darrens the tail ends of the Darren frequency distribution. -->
Translating payoffs into fitness, the Regina strategy ($f_{Regina} = 2^\frac{1}{5} = `r round(2^0.2, 2)`$) still yields an enormous advantage compared to unlucky Darrens ($f_{Darren_{unlucky}} = 0$) and only a small disadvantage compared to lucky Darrens ($f_{Darren_{lucky}} = 4^\frac{1}{5} = `r round(4^0.2, 2)`$).
<!-- But this time, Reginas share their place with average Darrens -->
<!-- including average Darrens (winning half the time and losing half the time) who accumulate the same total payoff as Reginas, $b_{total} = (\frac{1}{2}*0 + \frac{1}{2}*1)*4 = 2$. -->
<!-- This translates into fitness values of 0 for unlucky Darrens, $2^{\frac{1}{5}} = `r round(2^0.2, 2)`$ for Reginas and average Darrens, and  $4^{\frac{1}{5}} = `r round(4^0.2, 2)`$ for lucky Darrens. -->
But this time, there are fewer Darrens who are less successful than Reginas because Reginas now share their place with average Darrens.
The relative fitness advantage of the Regina strategy thus decreases.
As the rate of research cycles per generation grows, the law of large numbers dictates that more and more Darrens achieve average total payoffs, while fewer and fewer Darrens achieve extreme total payoffs (winning 32 times in a row is much less probable than winning 4 times in a row).
This reduces the width of the Darren distribution until it approximates the Regina distribution$\,$---$\,$meaning that optimal publication strategies become identical to those optimal for a linear fitness function. 

When the fitness function is convex ($\epsilon = 5$, yielding increasing returns), the overall effect of increasing values of $m$ is the same, with the only difference that Reginas are initially disadvantaged (because their fitness distance to the lucky half of Darrens is much greater than than to the unlucky Darrens).
With larger $m$, more and more Darrens receive average total payoffs and share Regina's disadvantaged position (decreasing Regina's relative disadvantage), until the Darren distribution is again virtually equal to the Regina distribution.
Rather than causing absolute risk aversion, increasing values of $m$ thus counter the effect of $\epsilon$ and reduce the effects of concave and convex fitness functions to the linear case.
Consequently, the top rows ($m = 32$) of the top and bottom panels in Figure \@ref(fig:mplot) resemble the stable pattern across all $m$ shown in the middle panel.

Translated into terms of academic careers, this less intuitive pattern indicates that 
<!-- researchers who are  -->
being able to complete empirical studies at a higher rate$\,$---$\,$e.g., when working in a field where data collection is fast and cheap or when having more resources for data collection available$\,$---$\,$may cancel out the effects of different career stages.
This could partly explain why Registered Reports appear to be less popular among senior researchers [@Chambers2022] than we would expect based on the effects of different fitness functions alone:
Although additional publications likely yield diminishing returns in later career stages (concave fitness function), academic seniority often comes with resources that boost research output per time (e.g., more lab members).
<!-- may indicate that senior researchers$\,$---$\,$for whom additional publications likely have diminishing returns$\,$---$\,$are more risk prone than we might expect when only considering the fitness function, because academic seniority also brings resources that boost research output per time. -->
As a consequence, established professors may be relatively indifferent to Registered Reports.
Regarding junior researchers (for whom additional publications have increasing returns on career success), the results suggest that they may be especially reluctant to use Registered Reports when they have very limited time or resources to produce publications before an important selection event, such as on short-term postdoc contracts [@Muller2017].


(ref:deltaplot) Effect of survival thresholds on evolved publication strategies. Shown are median publication strategies ($s$) after 250 generations in 50 runs (tile colour represents the median of 50 run medians) depending on survival thresholds ($\delta$, shown as vertical yellow line), fitness functions (characterised by exponent $\epsilon$), numbers of research cycles per generation ($m$), and values of $b_{RR}$, in the absence of competition ($\gamma = 1$). Survival thresholds are set as proportions of $m$, i.e., as a percentage of the maximum possible payoff in each condition. To reproduce, researchers must accumulate a total payoff exceeding $\delta * m$.


```{r deltaplot, echo=FALSE, warning=FALSE, fig.cap = "(ref:deltaplot)", out.width="100%"}

# Important additional chunk option when using journal mode: fig.env="figure*"

knitr::include_graphics(here("plots", "plot_delta_tile_evo_epsilon_mako.png"))
```

## Absolute survival thresholds

The survival thresholds ($\delta$) in our model represent absolute publication targets that researchers must meet in order to progress in their career.
The clearest examples for such thresholds are PhD regulations and tenure agreements.
To be awarded with a PhD, many institutions and faculties require candidates to have a certain number of their thesis chapters published in peer-reviewed journals.
Similarly, tenure agreements may include publication targets in the form of a minimum number of peer-reviewed publications within a certain time, sometimes also specifying minimal journal ranks [@Liner2009].
Such requirements may represent low, medium, or high survival thresholds depending on how demanding they are (e.g., the proportion of thesis chapters that must be published).

We investigate the effects of survival thresholds representing 25%, 50%, and 75% of the maximum possible payoff researchers can achieve in one generation.
When $\delta > b_{RR}$, Registered Reports alone are not sufficient to reach the survival threshold ($b_{RR}$ values to the left of the yellow line in Figure \@ref(fig:deltaplot)).
For example, at $m = 4$, a survival threshold of 75% ($\delta = .75$) means that researchers must gain at least 3 points to be able to reproduce. 
When $b_{RR} = .7$, submitting four Registered Reports will only amount to `r apa_num(.7*4, digits = 1)` points in total, just short of meeting the threshold.
On the other hand, when $b_{RR} = .8$ (i.e., just above $\delta$), four Registered Reports would yield `r apa_num(.8*4, digits = 1)` points and thus ensure reproduction.
Choosing the standard route some of the time can increase fitness even further, but also increases the risk of not meeting the survival threshold.
As a consequence, one may intuitively expect Registered Reports to be popular whenever $\delta \leq b_{RR}$ and unpopular whenever $\delta > b_{RR}$.

Figure \@ref(fig:deltaplot) shows that this is true in many, but not all conditions.
First, we can see that survival thresholds have their biggest effect when the number of research cycles per generation is low$\,$---$\,$at high values of $m$, publication strategies are virtually unaffected in all conditions.
Second, survival thresholds have a stronger effect when the fitness function is linear ($\epsilon = 1$) or concave ($\epsilon = 0.2$).
In these two conditions, they produce very similar patterns:
The Registered Report route is almost never chosen when $b_{RR}$ is too low to meet the survival threshold (particularly at $\delta = .25$ and $\delta = .5$; less so at $\delta = .75$), and this effect tapers off as the number of research cycles increases.
Compared to baseline, the change is particularly striking for the concave fitness function ($\epsilon = 0.2$, left column in Fig. \@ref(fig:deltaplot)), where RRs are normally preferred at low $m$.
When the survival threshold is high ($\delta = .75$) or the fitness function is concave, we can also see that Registered Reports become _more_ popular than baseline when they are worth just enough to pass the survival threshold.
For the convex fitness function ($\epsilon = 5$) on the other hand, survival thresholds of 25% and 50% seem to have no effect at all.
Only a high threshold of 75% makes RRs even less popular when they have low value ($b_{RR}\leq 0.4$), especially when the number of research cycles is low.

What does this mean in practice?
In our model, fitness (according to the three different fitness functions) is calculated after the survival threshold has been met.
This is meant to mimic publication requirements that are expressed in raw numbers.
Importantly, it also means that our simulation shows which strategies during a PhD or on the tenure track lead to maximal fitness _after_ researchers have successfully obtained their PhD or have been granted tenure.
With this in mind, it becomes easier to understand the meaning of the different fitness functions.
As discussed above, PhD candidates plausibly receive increasing returns for additional publications (convex fitness function), unless they intend not to stay in academia, in which case returns are strongly decreasing (concave fitness function).
<!-- the fitness function is plausibly convex ($\epsilon > 1$), as every additional publication to their (usually short) record may yield increasing returns on the job market and when applying for grants. -->
<!-- One notable exception are candidates who do not intend to stay in academia, and for whose careers publications will not be a meaningful currency$\,$---$\,$here, publications beyond the survival threshold would instead yield strongly diminishing returns (concave fitness function, $\epsilon < 1$). -->
For researchers on the tenure track, the fitness function after achieving tenure is also likely concave, assuming a) that achieving tenure is one of the most important career goals for many (making further progress relatively less important) and b) that such individuals have already built up substantial publication records, to which any single addition makes less and less of a difference.
However, exceptions from this scenario may well exist, for example in situations where tenured researchers are under great pressure to obtain grant funding.

Translated to real-world scenarios, our results thus suggest the following implications: 
First, survival thresholds are almost irrelevant when 
<!-- empirical pace is high, meaning that  -->
researchers can complete large numbers of studies before they are evaluated (reflecting characteristics of the research field, available resources, or length of the evaluation period).
<!-- the empirical pace of a research area (the number of studies that can be completed in a given amount of time) is high. -->
Second, researchers with a convex fitness function$\,$---$\,$such as PhD candidates who are pursuing an academic career$\,$---$\,$are only affected by high survival thresholds, which lead them to choose Registered Reports even less often than normal when their value is low.
Third, researchers with a concave fitness function$\,$---$\,$such as tenure candidates or PhD students who aim for careers outside of academia$\,$---$\,$are highly sensitive to the value of Registered Reports:
They virtually never conduct Registered Reports when their value is too low for meeting the survival threshold, but strongly prefer them when their value is sufficient (especially when empirical pace is low and/or the survival threshold is high).



(ref:competitionplot) Effect of competition on evolved publication strategies. Shown are median evolved publication strategies ($s$) after 250 generations in 50 runs (tile colour represents the median of 50 run medians) depending on the intensity of competition ($\gamma$, y-axis), numbers of research cycles per generation ($m$), different values of $b_{RR}$ (x-axis), and different fitness functions (characterised by exponent $\epsilon$) with no survival threshold ($\delta = 0$). To reproduce, researchers must accumulate a total payoff in the top $\gamma$ proportion of the population.

```{r competitionplot, echo=FALSE, warning=FALSE, fig.cap = "(ref:competitionplot)", out.width="100%"}

# Important additional chunk option when using journal mode: fig.env="figure*"

knitr::include_graphics(here("plots", "plot_gamma_evo_epsilon_inferno.png"))
```

## Competition

Competition occurs whenever the demand for academic positions or grant funding exceeds the supply.
Figure \@ref(fig:competitionplot) shows that competition generally leads to an aversion of Registered Reports, as can be seen by the darkening of the plots when moving up from the bottom row of panels.
The only exception to this rule is very low competition: 
When the top 90% are allowed to reproduce (and only the bottom 10% are rejected, $\gamma = .9$), Registered Reports become more popular than they are in the absence of competition.
This effect is strongest for the concave fitness function ($\epsilon = 0.2$), where it holds for almost all values of $b_{RR}$ at very low numbers of $m$ and for high values of $b_{RR}$ at high numbers of $m$.
When the fitness function is linear or convex, Registered Reports are chosen more often only when both $b_{RR}$ and $m$ are high.
At higher levels of competition ($\gamma > .5$), the differences between the fitness functions disappear.
In all three cases, Registered Reports are essentially wiped out for low numbers of research cycles ($m$), and this effect increases with competition (the higher the competition, the higher $m$ must be for Registered Reports to still be viable). 
Intense competition also negatively affects Registered Reports at high numbers of $m$, but here the general pattern of the baseline condition (a linear increase of Registered Reports popularity with $b_{RR}$) remains intact.

Looking at the first three rows of panels in Figure \@ref(fig:competitionplot) (1%, 5%, and 10% competition), the extreme effect of competition at low $m$ appears to decrease slightly when competition is highest ($\gamma = .01$), indicated by the dark bar at the bottom of each panel becoming slightly lighter. 
This paradoxial result is not due to Registered Reports being more lucrative in those conditions.
Rather, competition is so extreme that the natural selection in our model starts operating more on chance than on individuals' traits.
Essentially, only individuals with the maximum possible payoff (publishing only standard reports with positive results) are able to reproduce. 
Most likely to receive this maximum payoff are individuals who investigate hypotheses with high prior probabilities.
In our model, this is not a trait that can be passed on, but determined by random chance.
<!-- This situation does not simply favour individuals with a smart publication strategy, but ones who got lucky and, for example, were assigned a hypothesis with a very high prior in each round (which then also yielded a positive result each time). -->
Among individuals who experience this kind of luck, the variance of publication strategy $s$ should be high:
A hypothesis with prior $p = .95$ will be submitted as a standard report and likely yield a positive result (and thus the maximum payoff) regardless of whether the researcher's publication strategy is as low as $s = .1$ or has high as $s = .9$.
The higher average $s$ at low $m$ under extreme competition thus reflects relaxed selection pressure on $s$.
This is also evident by the shades of the dark bar at the bottom of the panels for $\gamma = .01$ (Fig. \@ref(fig:competitionplot)), which fluctuate randomly for each level of $m$ rather than showing a specific pattern.
A clearer illustration of the effect can be found in Figure XXX in the appendix, which shows large increases in the variance of evolved publication strategies in these conditions.
At higher $m$, selection on $s$ stays intact simply because much fewer individuals will be very lucky 4, 8, 16, or 32 times in a row than once or twice in a row, and publication strategy thus remains an important factor.

This effect of relaxed selection is not an arbitrary feature of our model, but commonly encountered in natural populations [@Snyder2021].
In many species, luck can have an outsized impact on survival and reproduction, rendering the effects of individual traits relatively less important.
Luck does not eliminate natural selection^[This is also apparent in Figure XXX (Appendix): Although the variance of evolved $s$ increases dramatically with high competition, it never spans the entire range of $s$.], but it can significantly slow it.
XXX TRY LONGER SIM RUNS & REPORT HERE XXX
The phenomenon is related to one form of survivorship bias:
Looking at 'survivors' of a highly selective process, one may erroneously infer that specific observable traits or behaviours of such individuals were the cause of their success when those were actually merely coincidental.

In the academic world, researchers compete for tenured positions and grants.
The level of competition may vary between research areas, countries, institutions, grant programmes, and so on.
Our findings suggest that intense competition may be a significant threat for the viability of Registered Reports, regardless of career stage.
This effect is particularly extreme when very few research cycles can be completed before an evaluation event (e.g., in fields with low empirical pace, in labs with few resources, or on short-term contracts): 
In such situations, publication strategies that involve any amount of Registered Reports are only viable when competition is so high that success requires extraordinary luck.
In contrast, very low but non-zero levels of competition increase the popularity of Registered Reports, especially when their value is high, when the fitness function is concave (e.g., in later career stages), and when researchers can complete many studies before being evaluated.




# Discussion

In the artificial world of the model presented here, the standard publication route is a coin toss$\,$---$\,$the probability of obtaining a publishable result is 50% on average^[This is the case because we modelled the prior probability of tested hypotheses as being uniformly distributed between 0 and 1 and as being identical to the probability of obtaining a positive (i.e., publishable) result.], translating to an expected payoff of 0.5 points per study.
If Registered Reports are a safe alternative to this gamble and guarantee publication in every case, one might think that payoff-maximising researchers would prefer them whenever they are worth more than 
<!-- 0.5 points  -->
the expected payoff from standard reports and avoid them whenever they are worth less.
This intuition, however, rests on the assumption
<!-- a specific assumption: -->
that the career benefits researchers receive from publications are linear and involve no step changes.^[Linearity is violated when the fitness function is concave or convex ($\epsilon \neq |1|$), but also in the presence of survival thresholds or competition, because these effectively introduce a step-change in the fitness function (low but non-zero payoffs yield zero career benefits).]
We argue that this assumption is violated in many, if not all, real-world situations.
<!-- Our findings show that this expectation is violated in many situations. -->
Here, we investigated the impact of four factors that likely shape real-world situations: 
convex vs concave fitness functions (additional publications yielding either increasing or decreasing returns, reflecting early vs later career stages), empirical pace (reflecting differences in speed and cost of data collection, available resources, or available time), survival thresholds (reflecting absolute publication targets researchers must meet in a given time), and competition for jobs or grants.
Our results show that in isolation or combined, many of these factors 
<!-- Our results show that when this is the case,  -->
would lead researchers with career-maximising strategies to avoid Registered Reports$\,$---$\,$even when Registered Reports are worth more than the expected payoff from standard reports.

<!-- #### Situations in which Registered Reports are particularly unpopular -->
To summarise the results, it is useful to take
the middle panel of Figure \@ref(fig:mplot) ($\epsilon = 1$) as a baseline.
In this panel, publication payoffs translate into linear career benefits (the fitness curve is linear and there is no survival threshold and no competition), and the outcome is highly intuitive: 
Researchers prefer Registered Reports whenever they are worth more than 0.5 points, and their preference is exactly proportional to $b_{RR}$ and not affected by empirical pace.
Compared to this baseline, Registered Reports are _less_ popular when a) additional publications yield increasing returns (e.g., in early career) and empirical pace is low, b) when researchers face a survival threshold that cannot be met with Registered Reports alone, especially when publications yield decreasing returns once the threshold has been met (e.g., in advanced career stages) and empirical pace is low, and c) when there is substantial competition.
Competition has the most extreme effect and can cause a complete avoidance of Registered Reports when empirical pace is low.
Conversely, Registered Reports are _more_ popular than at baseline when a) additional publications yield decreasing returns and empirical pace is low, b) Registered Reports are worth just enough to reach a survival threshold and publications yield decreasing returns after the threshold is met, especially when empirical pace is low, and c) when there is very low but non-zero competition, especially when publications yield decreasing returns or empirical pace is high.

Looking at the interactions of the different factors, three observations stand out.
First, high empirical pace attenuates the effects of all other factors$\,$---$\,$at the highest pace we considered (32 research cycles before evaluation), outcomes are identical to baseline in almost all conditions. 
The only exception to this rule is high competition, but although Registered Reports are relatively less attractive in this condition, the basic pattern is preserved and they remain viable when their value is high.
Second, the effect of survival thresholds strongly depends on the shape of the fitness function, suggesting that publication targets may have the strongest impact in advanced career stages.
Third, the opposite is true for high competition, which cancels out the effects of different fitness functions and thus appears to have virtually the same impact across career stages.



<!-- * Popular: -->
<!--   + Concave fitness curve & low empirical pace -->
<!--   + Concave fitness curve & $b_{RR}$ just high enough to reach survival threshold, especially when threshold is high OR empirical pace is low -->
<!--   + low but non-zero competition -->

<!-- * Unpopular: -->
<!--   + Convex fitness curve & low empirical pace -->
<!--   + when $b_{RR}$ too low to reach survival threshold, especially when fitness curve is concave & empirical pace is low -->
<!--   + high competition ($>50\%$) --- extremely so if empirical pace is low -->

<!-- * General: -->
<!--   + Empirical pace attenuates differences between fitness curves -->
<!--   + High competition attenuates differences between fitness curves -->
<!--   + $\rightarrow$ researchers in fields with low empirical pace generally very sensitive to other conditions (fitness curve, survival thresholds, competition) -->

<!-- * Most sensitive to the value of RRs: -->
<!--   + Researchers in fields or labs with high empirical pace -->
<!--   + Tenure-track researchers who have to meet a publication target -->

<!-- In the artificial world of the model presented here$\,$---$\,$where publication bias is rampant, authors' net payoff for Registered Reports is lower than for standard publications, and where career progress depends entirely on publication success$\,$---$\,$, Registered Reports have a hard time. -->



## Implications

<!-- ~~* Short transition paragraph: when are RRs least attractive, this time in terms of real-world academia?~~ -->
<!-- ~~* How could RR popularity be increased? -->
<!--   + change situations â less competition, revise tenure requirementsâ¦  -->
<!--     + pretty big and not going to happen -->
<!--   + other approach: what does the model pick up here? Pay-off variance, pay-off means -->
<!--     + make RRs more valuable (mean) â high impact journals, donât prevent them from happening -->
<!--       + pay-off anheben -->
<!--       + pay-off = benefit minus costs; Kosten verringern ->Gefahr Verringerung QualitÃ¤t? Needs to be balanced~~ -->

Our model predicts Registered Reports to be least popular when low empirical pace is combined with intense competition or with publication targets that cannot be met with Registered Reports alone.
Translated to real-world academia, this suggests that fields or labs in which productivity is limited by lacking resources or the cost or speed of data collection (e.g., research relying on expensive or rare equipment, research on populations that are difficult to access) deserve special attention.
Researchers in such situations may avoid the format when they must achieve publication targets that ask for a minimum number of publications in high-impact journals (e.g., as part of a tenure agreement), or when facing substantial competition for job positions or pressure to obtain competitive grants (e.g., if salary or research time depend on bringing in grants).
When competition is high, such researchers may favour standard reports even if Registered Reports are almost as valuable as the best possible outcome from a standard report.
Given that the last decades have seen vast increases in PhD students but relatively stable numbers of tenured positions in many countries [@Cyranoski2011], substantial competition may in fact be the default in many research fields, which could be one explanation for the currently low market share of Registered Reports.

<!-- Another, less intuitive prediction of the model is that Registered Reports should be avoided by early-career researchers and preferred by more senior researchers, again particularly when empirical pace is low. -->
<!-- This is based on the assumption that additional publications provide increasing returns for a junior researcher's career but decreasing returns for a senior researcher's career. -->

<!-- High levels competition may in fact be the default in many scientific disciplines and countries, as the last decades have seen vast increases in PhD graduates but relatively stable numbers of tenured positions [@Cyranoski2011]. -->
\par\vspace{.8\baselineskip}
### Possible interventions to increase the popularity of Registered Reports
What would make Registered Reports more attractive?
<!-- Obviously, changing the situational factors just mentioned should increase the popularity of Registered Reports. -->
One answer, of course, is to change the just-mentioned situational factors that make Registered Reports unpopular.
However, 
<!-- of the three factors just mentioned, publication targets are the only one that may lend themselves to intervention -->
with the exception of tenure agreements and PhD regulations, these 
<!-- are not useful targets for intervention -->
factors are difficult to intervene on$\,$---$\,$competition and empirical pace cannot be changed easily, if at all.
A more feasible approach may be to change the payoff structure of Registered Reports relative to standard reports.
In the terms of our model, this could be achieved by increasing either the mean or the variance of the career-relevant payoffs that authors receive from a publication.
For simplicity, we treated payoffs as _net_ payoffs, meaning the difference between the benefits and costs of each publication route.
In reality,
<!-- This means that in reality,  -->
the payoffs associated with Registered Reports can thus be raised by either increasing their benefits or lowering their costs (or both) relative to those of standard reports.
<!-- publishing a Registered Report. -->
This implies three potential targets for intervention in total: 
the benefits of Registered Reports, the costs of Registered Reports, and the variance of the net payoff of Registered Reports, each relative to standard reports.
<!-- #### Increasing mean payoff -->
<!-- In the simulation, we varied only the mean payoff of Registered Reports and kept payoff variance fixed at 0. -->
<!-- Figures \@ref(fig:epsilonplot)--\@ref(fig:competitionplot) show that higher payoffs ($b_{RR}$) increase the popularity of Registered Reports unless empirical pace is low and researchers face either intense competition or publication targets that are even higher than the value of Registered Reports. -->
<!-- Our study was designed to investigate the consequences of the fact that whether and where a study is published is partly influenced by the study's results in standard reports, but not in Registered Reports. -->
<!-- We therefore focus on 1) the benefits associated with the prestige and impact of the publishing journal [assuming that these parameters are both directly relevant for authors and causally influence another relevant parameter, the citation rate, @Traag2021] and 2) the costs associated with producing a published manuscript, such as data collection and analysis, writing, preparing a journal submission, revisions, responding to reviewers, and so on. -->

<!-- If Registered Reports were worth as much or more than the maximum payoff that authors can expect from a standard report, they should always be favoured$\,$---$\,$in such a situation, the higher variance of standard reports would only increase the danger of a worse outcome. -->
<!-- In our model, the payoff stands for the _net_ value for authors' careers, meaning the difference between benefits and costs. -->
<!-- In reality, this net value could thus be improved by either raising the benefits or lowering the costs associated with Registered Reports (or both) relative to those of standard reports. -->
\par\vspace{0.4\baselineskip}
#### Increasing the benefits of Registered Reports
<!-- Our study was designed to investigate the consequences of the fact that  -->
The starting point of our study was that 
whether and where a study is published is partly influenced by the study's results in standard reports, but not in Registered Reports.
We thus focus on the author benefits associated with the prestige and impact of the publishing journal [assuming that these parameters are both directly relevant for authors and causally influence citations, another relevant parameter, @Traag2021].
<!-- In this regard, raising the benefits of Registered Reports could be achieved  -->
These benefits could be raised if more prestigious, high-impact journals offered the format.
<!-- In practice, one concern is that high-impact journals -->
High-impact journals may currently be reluctant to offer
Registered Reports for fear of being forced to publish studies with uninteresting results, which might be cited less often.
<!-- in-principle acceptance for studies before knowing the results for fear that articles with 'bad' results may be cited less often. -->
Even when offering the format in principle, the same concern may lead such journals to be prohibitively selective during Stage-1 review and reject nearly all proposals.
<!-- even when such journals offer Registered Reports in principle, their Stage-1 review process may be prohibitively selective, rejecting nearly all proposals. -->
<!-- This could happen if the aim of selecting high-impact studies to publish made editors too risk-averse to accept the inherent uncertainty about the eventual study results in Registered Reports. -->
Perhaps as the result of such a dynamic, the journal _Nature_ launched a Registered-Reports submission track in February 2023 [@Nature2023], but appears to have published at most one Registered Report by August 2024.^[We used the search function on _Nature_'s website to search for the string 'Registered Report' (entered without quotes) in research articles published since 22^nd^ February 2023 and then searched the full texts of the 72 search hits for the string 'registered'. None of the articles was unambiguously marked as a Registered Report. Only one article [@Aslett2024] contained the term 'registered report' and was phrased in a way that may be consistent with the Registered Reports format.]
In practice, journals who are willing to participate in raising the value of Registered Reports should thus strive for designing an editorial process which, if ambitious, does not set unrealistic standards.

<!-- Alternatively,  -->
The value of Registered Reports can also be raised by those who ultimately provide the 'career-relevant benefits' associated with a publication, namely faculty committees responsible for hiring and promotion decisions.
Placing a premium on Registered Reports in tenure agreements, promotion criteria, and hiring processes could increase the attractiveness of the format substantially.
This idea is in line with recent calls for greater emphasis on rigorous and transparent research methods in hiring, promotion, and tenure decisions [e.g., @Moher2018], for example by including so-called 'open-science statements' in job ads [@Schonbrodt2016;@Schonbrodt2018].
However, most such statements currently either do not define specific practices or mention only preregistration and not Registered Reports [@Schonbrodt2018].
Explicitly highlighting Registered Reports in job ads and weighting them more heavily than standard reports in hiring, promotion, and tenure decisions could therefore be a promising strategy.
<!-- for increasing their popularity. -->
<!-- To the extent that Registered Reports are valued more highly than other practices (e.g., because they are designed to reduce publication bias), saying so explicitly would be worthwhile, especially given that Registered Reports are likely more costly for authors. -->
<!-- To the extent that the scientific community _does_ value Registered Reports more than preregistration (e.g., because Registered Reports are designed to reduce publication bias), an explicit distinction would be worthwhile, especially given that Registered Reports are likely more costly for authors. -->

\par\vspace{0.4\baselineskip}
#### Decreasing the costs of Registered Reports
Compared to standard reports, Registered Reports may be more costly for authors due to the additional stage of peer review 
<!-- (potentially requiring authors to prepare and revise manuscripts, write cover letters, and respond to reviewers more often)  -->
and stricter requirements for methodological rigour and sample size.
For example, Registered Reports (but not standard reports) in _Nature Human Behaviour_ currently must provide sampling plans aiming for at least $95\%$ statistical power or a Bayes factor of 10 [@NatureHumanBehaviour].
However, although it may be relatively easy to lower such standards,
<!-- requirements may be relatively easy to change, -->
<!-- Changing such requirements might be easier than the solutions discussed above. -->
<!-- However,  -->
doing so would also lower the quality of published Registered Reports and thus partly defeat their purpose of providing high-quality evidence.
<!-- value of Registered Reports to the scientific community (providing high-quality evidence) and thus partly defeat their purpose. -->
<!-- partly defeat the purpose of Registered Reports and  -->
<!-- run counter to the main objective of Registered Reports, which is to publish studies based on their high-quality methods. -->
This problem illustrates that many of the additional costs associated with Registered Reports may be 'good costs' that increase the quality of the resulting publications [see also @Tiokhin2021a].
To preserve this quality, cost cutting may need to be confined to removing unnecessary inefficiencies, such as certain bureaucratic aspects of the submission or review process.


<!-- Even without explicit submission requirements, though, the Stage-1 review process may plausibly select for Registered Reports with higher average methodological rigour than standard reports published in the same journal$\,$---$\,$simply because methodological shortcomings can be compensated by impressive results in standard reports but not in Registered Reports. -->
<!-- These concerns notwithstanding, it may be possible to at least somewhat reduce the cost difference between Registered Reports and standard reports. -->
Alternatively, the relative costs of Registered Reports could be decreased by increasing the costs of standard reports.
<!-- A less obvious, but perhaps more effective way of achieving this could be to  -->
<!-- shift the focus away from Registered Reports and instead  -->
<!-- increase the costs of standard reports. -->
Going back to the example above, a high-impact journal like _Nature Human Behaviour_ could reasonably demand the same level of methodological rigour 
<!-- and standard of evidence  -->
from standard reports as from Registered Reports.
This 
<!-- should work in principle because it  -->
would reduce the marginal advantage of standard reports over Registered Reports in terms of the investment required from authors (making Registered Reports relatively more attractive), while at the same time raising the quality of all studies published by the journal.

<!-- authors need to choose between the two formats. -->
<!-- Therefore, what matters is not their absolute costs or benefits, but the relative difference between Registered Reports and standard reports. -->


\par\vspace{0.4\baselineskip}
#### Increasing payoff variance

In the classic Registered Reports model, authors must choose a journal before having full knowledge of the value of the eventual study (i.e., before results are known and the final manuscript is written up).
From the authors' perspective, the pre-data publication guarantee by the chosen journal puts a cap not only on the worst possible outcome, but also on the best possible outcome.^[In principle, authors are free to withdraw a Registered Report before publication and submit their manuscript elsewhere, but this strategy would incur additional costs and risks (a new review process with unknown outcome) and may be perceived as violating a social norm.]
Another approach to making Registered Reports more attractive is therefore to remove the upper cap and give authors more publication options after the research has been completed.
This could be made possible by a recent initiative:
<!-- which offers a journal-independent review process for Registered Reports. -->
<!-- A new initiative recently launched a journal-independent review process for Registered Reports that could make this possible. -->
In April 2021, the post-publication peer review platform *Peer Community In* (PCI) introduced a new model of Registered Reports in which authors are no longer tied to a specific journal.
_PCI Registered Reports_ offers authors the regular process of Stage-1 and Stage-2 review (including in-principle acceptance after Stage 1), but the end result of a successful submission is simply a preprint with a so-called 'recommendation' from _PCI_. 
Authors can subsequently publish their manuscript in one of several journals who partnered with _PCI_ and either rely on the _PCI_ review process alone or offer a streamlined review process for _PCI_-recommended preprints.
Alternatively, authors are free to submit to any other journal as if their manuscript were a standard report.
This innovation gives Registered-Reports authors significantly more freedom to capitalise on the results of their study because a submission to PCI Registered Reports does not preclude the chance of a high-impact publication.
As of August 2024, there are 35 journals which accept Registered Reports recommended by PCI without further review.
With the growth of this list, and particularly the inclusion of more high-impact journals, the _PCI Registered Reports_ model has the potential to change the incentive structure of Registered Reports in a profound way.

<!-- constitute a significant change to the relative incentives and risk structure of Registered Reports compared to standard reports that merits a closer investigation in the future. -->


<!-- the journal publishing the eventual study is fixed from the moment that authors are granted in-principle acceptance for their Stage-1 protocol. -->
<!-- Neither negative nor positive developments after this point  -->

<!-- Our study focuses on the difference in payoff variance between Registered Reports and standard reports$\,$---$\,$in particular, the fact that authors have at least some chance of obtaining a higher payoff from a standard report than from a Registered Report. -->
<!-- Another way of making Registered Reports more attractive is thus to increase their payoff variance as -->

<!-- #### Registered Reports and post-publication peer review  -->

<!-- \par\vspace{.5\baselineskip} -->
<!-- ### General implications for risk-taking in academic research -->

<!-- * Implications for risk taking more generally -->
<!--   + apply findings to different situations -->
<!--   + for example, implications for funding lotteries? -->

<!-- The core of our model is the choice between a high-risk option and a low-risk option, with 'risk' defined as unpredictable variability in outcomes. -->
<!-- Therefore, the simulation can also be applied to types of risk-sensitive behaviour in academia. -->
<!-- For example, the effects of competition on risk taking predicted by the model could be interesting in light of recent proposals for replacing traditional grant funding competitions with modified lotteries [@Fang2016;@Smaldino2019b;@Heyard2022] or even abolishing grants altogether and simply distributing the available funds evenly across all labs in a country [@Vaesen2017]. -->
<!-- In a modified lottery, grants are raffled between applicants whose proposals clear a certain quality threshold. -->
<!-- The results shown in Figure \@ref(fig:competitionplot) suggest that the height of this threshold may influence how much risk researchers are willing to take in their proposals. -->


<!-- Modified lotteries would represent low but non-zero competition if the quality threshold was set very low (e.g., excluding only the bottom 10% of proposals). -->
<!-- Whether such low-threshold lotteries would have a measurable effect on the popularity of Registered Reports is questionable because applicants' publication records are usually not a primary criterion for funding decisions [although they may have a sizeable influence on reviews, @Simsek2024]. -->
<!-- However, a more general interpretation of our results is  -->

<!-- We assume that the standard publication route always has greater risk than conducting a Registered Report. -->

<!-- Our model also predicts that low but non-zero competition _increases_ the popularity of Registered Reports relative to baseline. -->
<!-- This is because low competition sifts out only the worst-performing individuals, which in our model are those who preferred the risky option (standard reports) and were unlucky (obtained unpublishable results). -->

<!-- Translated to the real-world academic landscape, our model thus predicts Registered Reports to be least popular  -->
<!-- when researchers' productivity (empirical pace) is reduced by lacking resources or the speed of data collection (e.g., research relying on expensive or rare equipment, research on populations that are difficult to access) while researchers either face intense competition or publication targets that cannot be met with Registered Reports alone (e.g., on the tenure track). -->
<!-- might thus expect the following patterns. -->
<!-- In research areas where empirical pace is low because studies are excessively costly or slow to conduct (e.g., research relying on expensive or rare equipment, research on populations that are difficult to access), or in labs or institutions where empirical pace is low because of lacking resources, publication strategies are particularly susceptible to the influence of other factors. -->
<!-- Our model predicts that researchers in such fields or labs will be much less likely to use Registered Reports when they are early career, when they must achieve publication targets that cannot be met with Registered Reports alone (e.g., tenure agreements that ask for a minimal number of publications in high-ranking journals), or when they face substantial competition. -->
<!-- In contrast, researchers in fields or labs with very high empirical pace (e.g., research areas relying on online surveys) should remain relatively indifferent to Registered Reports, preferring them when they are worth more than the expected payoff from the standard publication route and avoiding them when they are worth less than that. -->
<!-- The only exception to this are situations with intense competition, which drives such researchers to avoid Registered Reports unless they are very valuable (e.g., when Registered Reports can be conducted at the best journal that researchers think would publish their study as a standard report). -->

<!-- High levels competition may in fact be the default in many disciplines and countries, as the last decades have seen vast increases in PhD graduates but relatively stable numbers of tenured positions [@Cyranoski2011]. -->
<!-- Given that the last decades have seen vast increases in PhD students but relatively stable numbers of tenured positions in many countries [@Cyranoski2011], substantial competition may in fact be the default in many research fields. -->

<!-- When researchers must meet a specific publication target, individuals for whom additional publications beyond the target have decreasing returns (e.g., assistant professors on the tenure track or PhD candidates who intend to leave academia after obtaining their degree) are predicted to be especially sensitive to the value of Registered Reports:  -->
<!-- They should strongly prefer Registered Reports when those are worth enough to meet the target, and strongly prefer the standard publication route otherwise. -->
<!-- Finally, intense competition$\,$---$\,$e.g., when researchers' publication records must be in the top $10\%$ of their cohort for them to get a job$\,$---$\,$is predicted to have a strong negative effect on the popularity of Registered Reports, regardless of career stage. -->

<!-- #### Researchers with low productivity are most susceptible to other factors -->
<!-- As discussed above, high empirical pace$\,$---$\,$being able to complete many research cycles before evaluation$\,$---$\,$appears to buffer the effects of other factors. -->
<!-- Conversely, our model thus predicts that researchers whose productivity is _low_, for example due to lacking resources or slow or costly data collection (e.g., when relying on expensive equipment or difficult-to-access populations), are most susceptible to other risk-related influences. -->

<!-- * Fields with low pace/labs with low resources are most susceptible to other factors -->
<!-- * Tenure track: value of RRs extremely important -->
<!-- * Grants: strategy to only sift out the worst application and raffle among the rest would favour RR-heavy strategy -->
<!--   + More generally: our model shows that such an approach produces risk aversion, which in general might be not so good for science -->
<!-- * competition: relate to competition for priority & potential interaction with up-front cost of RRs -->
<!-- * Increase mean payoff from RRs: increase value, decrease cost OR decrease value, increase costs of SRs  -->
<!-- * Increase outcome variance from RRs in the upper direction: PCI RRs -->


<!-- ``To do:`` -->
<!-- * Implications of results -->
<!--   + high competition may be the default: jobs, grants -->
<!--   + job selection not only on publication record. Beyond a certain threshold, more pubs may not matter much -> this basically means the fitness curve is concave -->
<!--   + *low* competition may actually be good for grants: supports proposals for lotteries (after sifting out low-quality proposals) but not egalitarian funding  distribution -->
<!--   + ECRs predicted to avoid RRs unless empirical pace is high or they want to leave academia -> conflicts with empirical finding suggesting that RRs are more likely to have junior first authors -->
<!--   + @Tian2016: there are cases of high survival thresholds -->


## Limitations and future directions
<!-- * ~~assignment of prior probability~~ -->
<!--   + prior probably not uniform across 0 and 1 -->
<!--     + many more wrong? but many also trivially true? -->
<!--     + can try different distributions -->

By design, our model is based on assumptions that simplify and exaggerate some aspects of real-world academia and ignore many others.
<!-- This allowed us to examine the basic functioning of the included factors,  -->
First, we use an extreme, one-dimensional concept of publication bias:
All positive results are published, all negative results remain unpublished, and results are determined only by the prior probability of the hypotheses.
Real-world publication decisions are of course based on many other factors as well, such as the relevance of the research question and the validity of the study design.
And unlike in our model, tests of hypotheses with higher priors will not always be more publishable, for the simple reason that positive results of trivial (or previously tested) hypotheses are usually not highly valued [although it has been argued that 
<!-- some hypotheses tested in prominent social psychology journals are  -->
research in social psychology is sometimes based on hidden tautologies, @Wallach1994].

A more ecologically valid approach may be to model publication bias as favouring results that shift prior beliefs [@Gross2021].
Adapting the model presented here to capture this concept of bias could be an interesting future direction.
However, the present version of the model allows a conservative interpretation in which the prior probability of hypotheses simply reflects authors' predictions of the eventual publication value of different research questions. 
This interpretation is still congruent with Registered Reports and standard reports differing in risk, because publication value depends more strongly on the study results in standard reports than in Registered Reports.
<!-- (even if not in the simplistic sense of positive test results having higher value). -->

Second, we assume that authors have perfect knowledge of the probability that they will obtain positive (or publishable) results.
<!-- However, unless authors were entirely ignorant (i.e., guessing at chance level), modelling more realistic beliefs by adding noise and even bias  -->
The assumption that authors have _some_ prior knowledge of the results they might obtain is the starting point of our study, because this would enable strategic decisions about when to (not) use Registered Reports.
As long as this assumption holds (i.e., authors are not completely ignorant), adding noise and even bias to authors' prior beliefs would have a diluting effect on the simulation results, but should not change the general pattern.
<!-- as long as authors are not entirely ignorant (i.e., guessing at chance level). -->
Things may get more complicated, however, when considering individual differences in 
<!-- if researchers differ in their  -->
prediction accuracy or bias.
In our model, researchers who are better at predicting the results (or publishability) of their studies would outperform researchers whose predictions are more noisy or biased.
In reality, certain biases may actually be beneficial, for example if overconfident individuals are also better at convincing editors and reviewers of the value of their studies.

<!-- Third, in contrast to the model, the prior probability of hypotheses tested across scientific disciplines is almost certainly not uniformly distributed between 0 and 1. -->
A third, related limitation is that although researchers in the model know the prior of their hypotheses, they have no control over
<!-- cannot choose  -->
which hypotheses they test (hypotheses are randomly allocated).
<!-- This relates to third limitation: -->
<!-- Although researchers in the model know the prior of their hypotheses, they cannot choose which hypotheses they want to test (hypotheses are randomly allocated). -->
Of course real researchers can choose their own research questions, and this freedom may influence their publication strategies.
In particular, researchers who are better at choosing research questions that are likely to result in high-impact publications (through talent or experience) may find Registered Reports less attractive.
This is an example of ability-based risk taking:
Individuals with traits or abilities that increase their expected payoff from a risky option^[This includes traits that increase the chances of winning, traits that increase the payoff when winning, or traits that buffer the impact of losses.] should be more risk-prone [@Barclay2018].
A more nefarious version of this idea is that Registered Reports may be relatively unpopular among researchers who are more inclined to using questionable research practices (or even fraud) to obtain publishable or impactful results.

Fourth, we make the simplifying assumption that researchers work alone.
Of course this is not true in most scientific disciplines, where team work is the default and most publications have more than one author.
As a consequence, publication decisions are usually made jointly by researchers who may have different career-related needs.
<!-- therefore be more mixed than the ones suggested by our simulation results. -->
For example, senior researchers may often take the needs of their PhD students into account, which could lead them to behave more in line with a convex fitness function (increasing returns).
This does not invalidate our results, but it means that 
real-world publication strategies can be mixtures of the individual strategies represented by our model.
An interesting related consideration is that researchers may be able to compensate for low empirical pace by forming larger teams, essentially sharing a smaller number of research projects with more colleagues.
Such an effect could cause publication strategies in fields with very slow and/or costly data collection to resemble those expected under higher empirical pace.

Finally, our model ignores the factor time.
A common reservation towards Registered Reports is the concern that they take longer to publish because authors must wait for the outcome of the Stage-1 review process before starting the data collection.
Standard reports, on the other hand, may occasionally have even longer publication delays, for example when they are rejected at several journals or when reviewers demand additional studies to be run.
It is thus plausible that the formats differ in mean and/or variance of publication delays, and such differences could affect researchers' behaviour.
<!-- Such differences could affect researchers' behaviour$\,$---$\,$as highlighted by the effect of empirical pace on the simulation results, publication speed can play an important role in academia. -->
<!-- Risk in the delay of rewards may have a different effect on behaviour than risk in the value of rewards. -->
Because humans tend to discount delayed rewards [@Odum2020], researchers who believe the standard publication route to be faster may have a stronger preference for it than predicted by our model.
To further investigate this possibility, data on the distribution of publication delays (from the beginning of a research project until publication) of Registered Reports and standard reports, as well as on researchers' beliefs regarding these delays, would be highly valuable.

<!-- Humans and other animals have been shown to discount delayed rewards [@Winterhalder1999;@Odum2020]. -->
<!-- Adding a time dimension to our model may therefore lead to different results. -->
<!-- If the minimum time to publication is indeed faster in standard reports, researchers may prefer them to Registered Reports even if the formats had the same publication delay on average, and even if Registered Reports had a higher expected value. -->
<!-- Future research  -->

<!-- , and it seems highly plausible that expectations about test results and publishability influence their decisions to some degree. -->
<!-- In terms of our model, this means that researchers can influence the risk they are taking even when pursuing the standard publication route -->

<!-- #### Ability-based risk taking  -->
<!-- The model presented in this chapter only considers the effects of situational factors on individuals' risk sensitivity. -->
<!-- However, risk sensitivity can also be influenced by individual differences, such that  -->
<!-- Such factors may be important to consider in the context of research and publication practices. -->

<!-- (also, this approach only focuses on hypothesis testing, which is widely used in psychology but by far not the only means of doing science). -->



<!-- * Narrow focus on one specific (and highly stylised) difference between Registered Reports and standard reports; there are many others. Model ignores a myriad other factors that influences who chooses Registered Reports for which studies when -->


<!-- * RRs may actually _slow_ the empirical pace, introducing an interaction that our model doesn't take into account -->

<!-- By design, our model ignores many aspects of real-world academia that likely influence the adoption of Registered Reports. -->
<!-- By design, our model is an unrealistic simplification that ignores many factors that likely influence the adoption of Registered Reports and exaggerates the ones it includes. -->
<!-- This allowed us toisolate a small set of variables and understand their individual effects and interactions under specific assumptions. -->

<!-- Although these assumptions are chosen to capture important aspects of some features of the real world and not deviate too far from others, they must be taken into account when interpreting the simulation results. -->
<!-- The results of the simulation must therefore be interpreted with these assumptions in mind. -->


<!-- The purpose of this simulation study was to understand the effects of a small set of factors on author incentives for Registered Reports. -->
<!-- Although this narrow focus ignores many other factors that play a role in real-world academia, the simulation results can be used to identify targets for further investigation. -->

<!-- be used as a starting point for further investigation. -->
<!-- the simulation results can help develop general expectations about the studied variables (making 'all else being equal' predictions) -->

<!-- The simulation results thus cannot and should not be mapped directly onto specific real-world situations. -->
<!-- The purpose of this study was instead to help form an intuition and develop general expectations about the effects of specific factors in isolation (i.e., all else being equal). -->

<!-- In addition to suggesting general trends, our results point to a number of situations in which these factors appear to drastically reduce the attractiveness of Registered Reports. -->
<!-- For meta-scientists and for advocates of the format, these situations are targets for further investigation. -->




## Conclusion

The basic mechanism underlying Registered Reports$\,$---$\,$publication decisions before results are known$\,$---$\,$is currently the most convincing proposal for curbing publication bias.
By selecting studies based only on the strength of the research question and methods, Registered Reports are indeed 'aligning what is beneficial for individual scientists with what is beneficial for science' [@Chambers2022, p. 29].
However, the incentives for choosing the Registered Reports format in the first place may be less aligned with the interests of scientists.
<!-- , whose careers depend in no small part on the quantity and impact of publications they can produce. -->
In this study, we examined the consequences of 
<!-- we made use of the fact that  -->
the pre-data publication guarantee in Registered Reports, which makes them a low-risk option compared to the standard publication route because it reduces the variance of publication outcomes.
<!-- which effectively reduces the variance of publication outcomes and makes Registered Reports a low-risk option compared to the standard publication route. -->
<!-- Our results show that this feature  -->
Our results show that this feature does not make the format universally more attractive.
Instead, many common situations in the academic ecosystem, such as publication targets and competition for tenured positions, may promote risk-prone publication strategies and lead researchers to avoid Registered Reports.

This suggests that the spread of Registered Reports to larger parts of the scientific literature in more disciplines is not simply a matter of time.
In psychology, where the format may be best known, a generous estimate puts the annual rate of published Registered Reports at less than $`r apa_num(600/600000*100, digits = 1)`\%$ of the literature.^[We estimate that 600,000 journal articles per year are published in psychology and assume that at most 600 of these are Registered Reports [a generous estimate based on the 591 Registered Reports that had reportedly been published across disciplines by 2021, @Chambers2022]. The estimate of total publications per year was obtained via https://lens.org by applying the filters `Year Published = 2020--2023`, `Publication Type = journal article`, and `Field of Study = Psychology`, and dividing the 2,448,670 resulting hits by 4.]
This figure is much lower than the estimated prevalence of preregistration [$7\%$ in 2022, @Hardwicke2024], a reform that was introduced around the same time with similar goals.
Increasing the uptake of the format may require additional interventions, such as placing greater value on Registered Reports in hiring, promotion, and tenure decisions, raising the methodological requirements for standard reports in journals that offer both formats, or supporting the journal-independent model of _PCI Registered Reports_ and encouraging more high-impact journals to subscribe to it.
<!-- Successful reforms to countering bias in the published literature  -->
To the extent that scientific communities or external stakeholders have a demand for the kind of low-bias, high-quality evidence that Registered Reports can offer, such measures may be a worthwhile investment.
<!-- changing these incentives may be a worthwhile investment. -->






<!-- Reforms must take the prevailing incentive structure into account -->
<!-- In some cases, however, RRs may need to be commissioned if there is a demand for them. Highlights role of funding agencies (add ref) -->



<!-- Around 600,000 journal articles per year are published in psychology alone.^[Estimate was obtained via https://lens.org by applying the filters `Year Published = 2020--2023`, `Publication Type = journal article`, and `Field of Study = Psychology`, and dividing the 2,448,670 resulting hits by 4.] -->
<!-- Compared to this, the 591 Registered Reports published by 2021 [@Chambers2022] are a tiny number$\,$---$\,$even assuming that all of them were psychological studies and published in just one year, they would represent less than $`r round(600/600000*100,1)`\%$ of the literature. -->
<!-- This figure is much lower than the estimated prevalence of preregistration in the psychology literature [$7\%$ 2002, $95\%$ CI = [$2.5\%$--$12\%$], @Hardwicke2024], a reform that was introduced around the same time with similar goals. -->
<!-- (reducing bias in published results). -->
<!-- other open-science practices that were introduced around the same time with similar goals (reducing bias in published results), such as preregistration (estimated prevalence $7\%$, $95\%$ CI = [$2.5\%$--$12\%$] or data sharing [$14\%$ [$7\%$--$21\%$], both estimates from @Hardwicke2024]. -->
<!-- The difference in adoption rates could reflect that Registered Reports require a more profound change to researchers' habitual workflow, are more costly in other ways (e.g., require larger samples), are not available at all journals (although more than 300 journals offer them at this point) or simply are less well known. -->
<!-- Nonetheless, one might expect the format to spread more quickly if it offered substantial advantages for researchers' careers. -->


<!-- The simulation results we presented show clearly that the lower risk associated with Registered Reports does not automatically make them more attractive. -->
<!-- Instead, we find many situations in which career-maximising researchers may favour the standard publication route or even avoid Registered Reports entirely. -->
<!-- Most prominently, our model predicts such complete avoidance when competition is high and empirical pace$\,$---$\,$the number of studies researchers can complete before evaluation$\,$---$\,$is low.  -->
<!-- In this constellation, Registered Reports are not sustainable even if their value is almost as high as the maximum payoff that authors can achieve through the standard route. -->
<!-- Substantial competition may in fact be the default in many disciplines and countries, as the last decades have seen vast increases in PhD graduates but relatively stable numbers of tenured positions [@Cyranoski2011]. -->
<!-- This factor might thus be part of the explanation for the very low market share of Registered Reports. -->
<!-- To investigate this hypothesis, two predictions could be tested empirically: -->
<!-- Two interesting question to study empirically would be 1) whether attitudes towards the format vary between researchers who experience different levels of competition (e.g., between regions or research fields) and 2) whether this effect, if present, is stronger in fields or labs where productivity is limited by slow data collection or lacking resources (low empirical pace). -->

<!-- The model we presented focuses on the difference in risk$\,$---$\,$unpredictable outcome variance$\,$---$\,$between Registered Reports and the standard publication route. -->
<!-- The goal of this study was to achieve a better understanding of the consequences of this difference and to identify situations in which Registered Reports may remain unattractive without further intervention, or be used in highly selective ways. -->


<!-- Could such an effect alone explain why Registered Reports have fewer positive results? -->
<!-- In Chapter 2, we showed that this is highly unlikely: -->
<!-- Assuming that Registered Reports and standard reports have the same statistical power, the hypotheses tested in Registered Reports would need to be roughly half as likely to be true [see Fig. 3 in @Scheel2021a]. -->
<!-- The same analysis provides indirect evidence for bias in the standard literature, as the extremely high rate of positive results in standard reports (96%) is simply incompatible with plausible estimates of prior probability and power [see also @Ingre2018]. -->
<!-- Combined with more direct evidence of publication bias and QRPs in the standard literature [@Franco2014;@Franco2016;@Agnoli2017;@John2012]  -->
<!-- as well as the face-valid mechanisms of Registered Reports to reduce the influence of these factors, our previous findings [@Scheel2021a] thus support the effectiveness of Registered Reports. -->
<!-- calculated which combinations of average prior probability^[Prior probability was conceptualised as the proportion of true hypotheses among all tested hypotheses.] and statistical power could produce the positive result rates observed in Registered Reports and in the standard literature [see Fig. 3 in @Scheel2021a]. -->
<!-- We think not: the difference we found in Chapter 2 is simply too large  -->
<!-- It is hard to imagine that such an effect alone could explain  -->
<!-- the very large difference in positive results that we found in Chapter 2 [@Scheel2021a], especially in light of direct [@Franco2014;@Franco2016;@Agnoli2017;@John2012] and indirect [@Ingre2018;@Scheel2021a] evidence of publication bias and QRPs in the standard literature and the face-valid mechanisms of Registered Reports to reduce the influence of these factors. -->
<!-- But it is possible that  bias and prior probability both contribute to the finding, which complicates its interpretation. -->
<!-- is responsible for the `r 96-44` percentage-point difference -->
<!-- would be large enough to account for the entire difference in positive results [`r 96-44` percentage points, @Scheel2021a] -->
<!-- However, we cannot rule out that the prior probability of tested hypotheses plays an additional role, which means that the `r 96-44` percentage points difference in positive results cannot be interpreted as a direct measure of the amount of bias in the standard literature. -->

<!-- From a statistical perspective, the positive result rate is determined by three factors: -->
<!-- the base rate of true hypotheses, statistical power, and bias.^[More precisely, the positive result rate is determined by the base rate of true hypotheses, statistical power, and the probability of obtaining a positive result when the tested hypothesis is false, the latter consisting of the nominal alpha level and bias. Because alpha usually remains fixed across researchers and studies, we only focus on bias here.] -->
<!-- Although it is highly plausible that Registered Reports and standard reports differ in bias [given the face-valid mechanism of Registered Reports as well as direct and indirect evidence for bias in the standard literature, e.g., @Franco2014;@Gopalakrishna2022;@Ingre2018;@Scheel2021a], one must consider possible differences on the other two factors. -->

<!-- How much of the difference between these figures and estimates of the positive result rate in the standard literature is so large [`r 96-44` percentage points in @Scheel2021a] can be attributed to the reduction of bias in Registered Reports? -->

<!-- presented initial evidence that published Registered Reports have a substantially lower rate of positive results than regular articles in psychology [$44\%$ vs $96\%$, @Scheel2021a]. -->
<!-- A study comparing Registered Reports with matched controls additionally found that Registered Reports have higher median sample sizes and, in blind reviews, are judged to be more rigorous in methodology and analysis and of higher overall quality [@Soderberg2021], which suggests that the increased amount of negative results is not an artifact of lower power.^[We cannot rule out that the effect sizes studied in Registered Reports are smaller than in standard reports, in which case the larger samples may not translate into higher statistical power. However, it seems highly unlikely that such a difference would be large enough to explain the entire difference in the positive result rate (`r 96-44` percentage points) reported in Chapter 2 (see Fig. 3 in Chapter 2).] -->

<!-- These findings seem to support the effectiveness of Registered Reports in reducing publication bias and QRPs.  -->
<!-- However, one remaining alternative explanation for the lower rate of positive results is that the hypotheses tested in the Registered Reports literature are less often true (have a lower prior probability) than those in the standard literature. -->
<!-- may on average have a lower prior probability  -->
<!-- , which would also cause more negative results. -->

<!-- In the storybook version of science, researchers are driven by pure curiosity, conduct empirical studies to learn about the natural world, and impartially record the results. -->
<!-- In reality, researchers are motivated and constrained by a wide range of psychological, social, political, and structural factors$\,$---$\,$and not all results are equally informative, interesting, newsworthy, or beneficial to their authors' career. -->
<!-- Some combination of these factors likely explains the observation that results do not seem to be recorded impartially in many scientific disciplines. -->
<!-- Specifically, negative results of statistical hypothesis tests are published less frequently than positive results [@Fanelli2010] -->

<!-- Although it is hard to imagine that such an effect would be large enough to account for the entire difference in positive results [`r 96-44` percentage points, @Scheel2021a], it does complicate the interpretation of the finding. -->

<!-- Initial evidence shows that published Registered Reports have a substantially lower rate of positive results than regular articles in psychology [$44\%$ vs $96\%$, @Scheel2021a] and in psychology, neuroscience, and the biomedical sciences [@Allen2019]. -->
<!-- A similarly low rate (39.5%) was reported by @Allen2019 in a less formalised survey of Registered Reports in psychology, neuroscience, and the biomedical sciences. -->
<!-- , @Allen2019 reported a similarly low rate of positive results (39.5%). -->

<!-- Do these figures mean that Registered Reports are effective in reducing bias? -->
<!-- Two alternative explanations are that Registered Reports have lower statistical power or  -->
<!-- A study comparing Registered Reports with matched controls found that Registered Reports have higher median sample sizes and are judged to be more rigorous in methodology and analysis and of higher overall quality in blind reviews [@Soderberg2021], suggesting that the increased amount of negative results is not an artifact of lower power^[We cannot rule out that the effect sizes studied in Registered Reports are smaller than in standard reports, in which case the larger samples may not translate into higher statistical power. However, it seems unlikely that such a difference would be large enough to explain the `r 96-44` percentage point difference in the positive result rate reported in Chapter 2.] or poor study design. -->
<!-- However, one remaining  -->

<!-- Can these findings be taken as evidence for the effectiveness of Registered Reports? -->
<!-- When comparing two literatures observationally as we did in Chapter 2 [@Scheel2021a], differences in the positive result rate can have three causes: differences in bias (i.e., publication bias and QRPs), differences in statistical power, and differences in the prior probability of the tested hypotheses.  -->
<!-- As just mentioned, differences in power are unlikely to be the cause in this case, because Registered Reports are often required to fulfill strict power requirements and have been found to have larger sample sizes than matched controls [@Soderberg2021].^[We cannot rule out that the effect sizes studied in Registered Reports are smaller than in standard reports, in which case the larger samples may not translate into higher statistical power. However, it seems unlikely that such a difference would be large enough to explain the `r 96-44` percentage point difference in the positive result rate reported in Chapter 2.] -->
<!-- This leaves bias and prior probability as possible culprits. -->
<!-- A difference in bias is the most plausible option, given that a) the extremely high positive result rate in the standard literature [around or exceeding 90%,@Motyl2017;@Scheel2021a;@Sterling1959;@Sterling1995] cannot be explained without assuming a substantial amount of bias [@Ingre2018;@Scheel2021a] and b) the design of Registered Reports places considerable barriers in the way of publication bias and QRPs. -->
<!-- However, a difference in prior probability is _also_ plausible, and there is currently no evidence speaking against this possibility. -->

<!-- It is tempting to interpret these findings as 1) supporting -->
<!-- the conclusion that Registered Reports indeed successfully decrease publication bias and QRPs (both of which inflate the rate of positive results in the standard literature) and 2) providing a direct measure for the amount of bias in the standard literature. -->
<!-- Although the premise that the standard literature is positively biased is widely accepted at this point, most of the evidence supporting it is independent of Registered Reports: -->
<!-- The positive result rate of this literature has repeatedly been found to be extremely high$\,$---$\,$around or exceeding 90% [@Motyl2017;@Scheel2021a;@Sterling1959;@Sterling1995]$\,$---$\,$ -->
<!-- and analyses have shown that such figures are incompatible with realistic assumptions about statistical power and the base rate of true hypotheses [@Ingre2018;@Scheel2021a]. -->

<!-- Drawing this inference, however, assumes that the prior probability of the hypotheses and the size of effects^[More precisely, the assumption is that _statistical power_ is the same across formats. Because Registered Reports tend to have larger sample sizes, the true size of the tested effects may actually be smaller than in standard reports, just not so much smaller as to decrease power below the average of standard reports.] tested are the same in both formats. -->
<!-- Is this assumption realistic? -->

<!-- Publishing a scientific article typically means writing up a report of a completed research project and submitting it to a  -->
<!-- In the standard model of scientific publication, the peer review process -->
<!-- peer reviewers and journal editors evaluate reports of completed research projects and decide whether to publish them. -->

<!-- In the standard model of scientific publishing, a researcher writes up a report of scientific work they have completed and submits it to a journal, where peer reviewers provide criticism, suggest improvements, evaluate the quality of the research, and help the journal editor decide whether or not to publish the manuscript. -->

<!-- Under-reporting of negative results skews the available evidence for scientific claims and can lead to overconfidence and an increased rate of false-positive inferences. -->
<!-- Evidence for such publication bias$\,$---$\,$negative results getting published at a lower rate than positive results$\,$---$\,$has been been found in several disciplines [@deVries2018,@Dickersin1993,@Sterling1959,@Csada1996,@Franco2016] and is seen as an important contributor to poor replicability of published studies in biomedical and psychological research [@Ferguson2012,@Chalmers2009]. -->
<!-- In 2013, the journal *Cortex* pioneered a new article format designed to combat publication bias by moving the peer-review process to the planning stage of a study, thus separating the publication decision from the study results [@Chambers2013]. -->
<!-- In these *Registered Reports*, the review process is split in two stages: -->
<!-- At Stage 1, reviewers evaluate a pre-study protocol containing the research questions, hypotheses, methods, and planned analyses of a proposed study. -->
<!-- In case of a positive decision, the journal issues an `in-principle acceptance' and commits to publishing the eventual report regardless of the direction of the results. -->
<!-- Once authors have collected and analysed the data and written up the results, the final report is submitted to a second stage of peer review, but this time only to ensure that the study was carried out as planned, that the data pass any pre-specified quality checks, and that authors' conclusions are justified by the evidence. -->

```{r include=FALSE}
r_refs(file = "rr-risk-sensitivity_software.bib")
my_citation <- cite_r(file = "rr-risk-sensitivity_software.bib")
```

## Disclosures
### Data, materials, and online resources
<!-- [Data](https://osf.io/aqr2s/) and code necessary to reproduce all analyses reported here, as well as the [Appendix](https://osf.io/qw798/), the [preregistration](https://osf.io/sy927/), and additional supplementary files, are available at <https://osf.io/dbhgr>.  -->
This manuscript was created using RStudio [1.2.5019, @RStudioTeam2019] and `r my_citation`.

<!-- ### Reporting -->
<!-- We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study. -->

<!-- ### Author Contributions -->
<!-- Conceptualisation: A.S. & D.L.; data curation, formal analysis, and software: A.S. & M.R.M.J.S.; investigation, methodology, and validation: A.S., M.R.M.J.S., & D.L; supervision: A.S & D.L.; visualisation and writing$\,$---$\,$original draft: A.S; writing$\,$---$\,$review and editing: A.S., M.R.M.J.S., & D.L. -->

<!-- ### Conflicts of Interest -->
<!-- The authors declare that they have no conflicts of interest with respect to the authorship or the publication of this article. -->

<!-- ### Acknowledgements -->
<!-- This work was funded by VIDI grant 452-17-013. We thank Chris Chambers, Emma Henderson, Leo Tiokhin, Stuart Ritchie, and Simine Vazire for valuable comments that helped improve this manuscript. -->


# References


\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}

