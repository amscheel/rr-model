
@article{Altmejd,
  title = {Predicting the {{Replicability}} of {{Social Science Lab Experiments}}},
  author = {Altmejd, Adam and Dreber, Anna and Forsell, Eskil and Ho, Teck Hua and Huber, Juergen and Imai, Taisuke and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Camerer, Colin},
  doi = {10.31222/osf.io/zamry},
  abstract = {We measure how accurately replication of experimental results can be predicted by a black-box statistical model. With data from four large- scale replication projects in experimental psychology and economics, and techniques from machine learning, we train a predictive model and study which variables drive predictable replication.The model predicts binary replication with a cross validated accuracy rate of 70\% (AUC of 0.79) and relative effect size with a Spearman {$\rho$} of 0.38. The accuracy level is similar to the market-aggregated beliefs of peer scientists (Camerer et al., 2016; Dreber et al., 2015). The predictive power is validated in a pre-registered out of sample test of the outcome of Camerer et al. (2018b), where 71\% (AUC of 0.73) of replications are predicted correctly and effect size correlations amount to {$\rho$} = 0.25.Basic features such as the sample and effect sizes in original papers, and whether reported effects are single-variable main effects or two- variable interactions, are predictive of successful replication. The models presented in this paper are simple tools to produce cheap, prognostic replicability metrics. These models could be useful in institutionalizing the process of evaluation of new findings and guiding resources to those direct replications that are likely to be most informative.},
  keywords = {meta-science,prediction markets,prior elicitation,prior probability,replicability,replication,replication crisis}
}

@article{Altmejd2019,
  title = {Predicting the Replicability of Social Science Lab Experiments},
  author = {Altmejd, Adam and Dreber, Anna and Forsell, Eskil and Huber, Juergen and Imai, Taisuke and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Camerer, Colin},
  editor = {Wicherts, Jelte M.},
  year = {2019},
  month = dec,
  journal = {PLOS ONE},
  volume = {14},
  number = {12},
  pages = {e0225826},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0225826},
  langid = {english}
}

@article{Barclay,
  title = {State-Dependent Risk-Taking},
  author = {Barclay, Pat and Mishra, Sandeep and Sparks, Adam Maxwell},
  journal = {Proceedings of the Royal Society B},
  publisher = {{The Royal Society}},
  doi = {10.1098/rspb.2018.0180},
  abstract = {Who takes risks, and when? The relative state model proposes two non-independent selection pressures governing risk-taking: need-based and ability-based. The need-based account suggests that actors...},
  copyright = {\textcopyright{} 2018 The Author(s)},
  langid = {english}
}

@article{Butler2007,
  title = {Assessing University Research: {{A}} Plea for a Balanced Approach},
  shorttitle = {Assessing University Research},
  author = {Butler, Linda},
  year = {2007},
  month = oct,
  journal = {Science and Public Policy},
  volume = {34},
  number = {8},
  pages = {565--574},
  publisher = {{Oxford Academic}},
  issn = {0302-3427},
  doi = {10.3152/030234207X254404},
  abstract = {Abstract. The use of quantitative performance measures to assess the quality of university research is being introduced in Australia and the UK. This paper pres},
  langid = {english}
}

@article{Camerer2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  abstract = {Camerer et al. carried out replications of 21 Science and Nature social science experiments, successfully replicating 13 out of 21 (62\%). Effect sizes of replications were about half of the size of the originals.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {meta-science,prediction markets,prior elicitation,prior probability,replicability,replication crisis}
}

@article{Clemen2000,
  title = {Assessing {{Dependence}}: {{Some Experimental Results}}},
  shorttitle = {Assessing {{Dependence}}},
  author = {Clemen, Robert T. and Fischer, Gregory W. and Winkler, Robert L.},
  year = {2000},
  month = aug,
  journal = {Management Science},
  volume = {46},
  number = {8},
  pages = {1100--1115},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.46.8.1100.12023},
  langid = {english},
  keywords = {correlation,expert elicitation,prior elicitation,prior probability,stats}
}

@article{Dreber2015,
  title = {Using Prediction Markets to Estimate the Reproducibility of Scientific Research},
  author = {Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus},
  year = {2015},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {50},
  pages = {15343--15347},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1516179112},
  langid = {english},
  keywords = {meta-science,prediction markets,prior elicitation,prior probability,replicability,replication crisis}
}

@article{Forsell2018,
  title = {Predicting Replication Outcomes in the {{Many Labs}} 2 Study},
  author = {Forsell, Eskil and Viganola, Domenico and Pfeiffer, Thomas and Almenberg, Johan and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus and Dreber, Anna},
  year = {2018},
  month = oct,
  journal = {Journal of Economic Psychology},
  issn = {0167-4870},
  doi = {10.1016/j.joep.2018.10.009},
  abstract = {Understanding and improving reproducibility is crucial for scientific progress. Prediction markets and related methods of eliciting peer beliefs are promising tools to predict replication outcomes. We invited researchers in the field of psychology to judge the replicability of 24 studies replicated in the large scale Many Labs 2 project. We elicited peer beliefs in prediction markets and surveys about two replication success metrics: the probability that the replication yields a statistically significant effect in the original direction (p\,{$<$}\,0.001), and the relative effect size of the replication. The prediction markets correctly predicted 75\% of the replication outcomes, and were highly correlated with the replication outcomes. Survey beliefs were also significantly correlated with replication outcomes, but had larger prediction errors. The prediction markets for relative effect sizes attracted little trading and thus did not work well. The survey beliefs about relative effect sizes performed better and were significantly correlated with observed relative effect sizes. The results suggest that replication outcomes can be predicted and that the elicitation of peer beliefs can increase our knowledge about scientific reproducibility and the dynamics of hypothesis testing.},
  keywords = {Beliefs,ManyLabs,meta-science,prediction markets,Prediction markets,prior elicitation,prior probability,replicability,replication crisis,Replications,Reproducibility}
}

@article{Garthwaite2005,
  title = {Statistical {{Methods}} for {{Eliciting Probability Distributions}}},
  author = {Garthwaite, Paul H and Kadane, Joseph B and O'Hagan, Anthony},
  year = {2005},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {470},
  pages = {680--701},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214505000000105},
  langid = {english}
}

@article{Gross2021,
  title = {Why Ex Post Peer Review Encourages High-Risk Research While Ex Ante Review Discourages It},
  author = {Gross, Kevin and Bergstrom, Carl T.},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.13282 [physics]},
  eprint = {2106.13282},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Peer review is an integral component of contemporary science. While peer review focuses attention on promising and interesting science, it also encourages scientists to pursue some questions at the expense of others. Here, we use ideas from forecasting assessment to examine how two modes of peer review -- ex ante review of proposals for future work and ex post review of completed science -- motivate scientists to favor some questions instead of others. Our main result is that ex ante and ex post peer review push investigators toward distinct sets of scientific questions. This tension arises because ex post review allows an investigator to leverage her own scientific beliefs to generate results that others will find surprising, whereas ex ante review does not. Moreover, ex ante review will favor different research questions depending on whether reviewers rank proposals in anticipation of changes to their own personal beliefs, or to the beliefs of their peers. The tension between ex ante and ex post review puts investigators in a bind, because most researchers need to find projects that will survive both. By unpacking the tension between these two modes of review, we can understand how they shape the landscape of science and how changes to peer review might shift scientific activity in unforeseen directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Digital Libraries,Physics - Physics and Society}
}

@article{Hemming2018,
  title = {A Practical Guide to Structured Expert Elicitation Using the {{IDEA}} Protocol},
  author = {Hemming, Victoria and Burgman, Mark A. and Hanea, Anca M. and McBride, Marissa F. and Wintle, Bonnie C.},
  year = {2018},
  journal = {Methods in Ecology and Evolution},
  volume = {9},
  number = {1},
  pages = {169--180},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12857},
  abstract = {Expert judgement informs a variety of important applications in conservation and natural resource management, including threatened species management, environmental impact assessment and structured decision-making. However, expert judgements can be prone to contextual biases. Structured elicitation protocols mitigate these biases, and improve the accuracy and transparency of the resulting judgements. Despite this, the elicitation of expert judgement within conservation and natural resource management remains largely informal. We suggest this may be attributed to financial and practical constraints, which are not addressed by many existing structured elicitation protocols. In this paper, we advocate that structured elicitation protocols must be adopted when expert judgements are used to inform science. In order to motivate a wider adoption of structured elicitation protocols, we outline the IDEA protocol. The protocol improves the accuracy of expert judgements and includes several key steps which may be familiar to many conservation researchers, such as the four-step elicitation, and a modified Delphi procedure (``Investigate,'' ``Discuss,'' ``Estimate'' and ``Aggregate''). It can also incorporate remote elicitation, making structured expert judgement accessible on a modest budget. The IDEA protocol has recently been outlined in the scientific literature; however, a detailed description has been missing. This paper fills that important gap by clearly outlining each of the steps required to prepare for and undertake an elicitation. While this paper focuses on the need for the IDEA protocol within conservation and natural resource management, the protocol (and the advice contained in this paper) is applicable to a broad range of scientific domains, as evidenced by its application to biosecurity, engineering and political forecasting. By clearly outlining the IDEA protocol, we hope that structured protocols will be more widely understood and adopted, resulting in improved judgements and increased transparency when expert judgement is required.},
  copyright = {\textcopyright{} 2017 The Authors. Methods in Ecology and Evolution \textcopyright{} 2017 British Ecological Society},
  langid = {english},
  keywords = {Delphi,expert elicitation,forecasting,four-step elicitation,IDEA protocol,prior elicitation,prior probability,quantitative estimates,structured expert judgement}
}

@article{Hoogeveen2020,
  title = {Laypeople {{Can Predict Which Social-Science Studies Will Be Replicated Successfully}}:},
  shorttitle = {Laypeople {{Can Predict Which Social-Science Studies Will Be Replicated Successfully}}},
  author = {Hoogeveen, Suzanne and Sarafoglou, Alexandra and Wagenmakers, Eric-Jan},
  year = {2020},
  month = aug,
  journal = {Advances in Methods and Practices in Psychological Science},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10.1177/2515245920919667},
  abstract = {Large-scale collaborative projects recently demonstrated that several key findings from the social-science literature could not be replicated successfully. Here...},
  copyright = {\textcopyright{} The Author(s) 2020},
  langid = {english}
}

@article{Hurly2003,
  title = {The Twin Threshold Model: Risk-Intermediate Foraging by Rufous Hummingbirds, {{Selasphorus}} Rufus},
  shorttitle = {The Twin Threshold Model},
  author = {Hurly, Andrew T.},
  year = {2003},
  month = oct,
  journal = {Animal Behaviour},
  volume = {66},
  number = {4},
  pages = {751--761},
  issn = {0003-3472},
  doi = {10.1006/anbe.2003.2278},
  abstract = {I developed two versions of the twin threshold model (TTM) to assess risk-sensitive foraging decisions by rufous hummingbirds. The model incorporates energy thresholds for both starvation and reproduction and assesses how three reward distributions with a common mean but different levels of variance interact with these critical thresholds to determine fitness. Fitness, a combination of survival and reproduction, is influenced by both the amount of variance in the distributions and the relative position of the common mean between the thresholds. The model predicts that risk-intermediate foraging is often the optimal policy, and that risk aversion is favoured as the common mean of the distributions approaches the starvation threshold, whereas risk preference is favoured as the common mean approaches the reproduction threshold. Tests with free-living hummingbirds supported these predictions. Hummingbirds were presented with three distributions of nectar rewards that had a common mean but Nil, Moderate or High levels of variance. Birds preferred intermediate levels of variance (Moderate) when presented with all three rewards simultaneously, and became more risk-averse as the mean of the distributions was decreased but more risk-prone as the mean was increased. Birds preferred Nil when it was paired with Moderate or with High, but preferred Moderate in the presence of Nil and High together. This reversal of preference is a violation of regularity, conventionally interpreted as irrational choice behaviour. I provide an alternative version of the TTM demonstrating that violations of regularity can occur when relative instead of absolute evaluation mechanisms are used.},
  langid = {english}
}

@article{Ingre2018,
  title = {Estimating Statistical Power, Posterior Probability and Publication Bias of Psychological Research Using the Observed Replication Rate},
  author = {Ingre, Michael and Nilsonne, Gustav},
  year = {2018},
  month = sep,
  journal = {Royal Society Open Science},
  volume = {5},
  number = {9},
  pages = {181190},
  issn = {2054-5703, 2054-5703},
  doi = {10.1098/rsos.181190},
  langid = {english}
}

@article{Johnson2010,
  title = {A Valid and Reliable Belief Elicitation Method for {{Bayesian}} Priors},
  author = {Johnson, Sindhu R. and Tomlinson, George A. and Hawker, Gillian A. and Granton, John T. and Grosbein, Haddas A. and Feldman, Brian M.},
  year = {2010},
  month = apr,
  journal = {Journal of Clinical Epidemiology},
  volume = {63},
  number = {4},
  pages = {370--383},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2009.08.005},
  abstract = {Objective Bayesian inference has the advantage of formally incorporating prior beliefs about the effect of an intervention into analyses of treatment effect through the use of prior probability distributions or ``priors.'' Multiple methods to elicit beliefs from experts for inclusion in a Bayesian study have been used; however, the measurement properties of these methods have been infrequently evaluated. The objectives of this study were to evaluate the feasibility, validity, and reliability of a belief elicitation method for Bayesian priors. Study Design and Setting A single-center, cross-sectional study using a sample of academic specialists who treat pulmonary hypertension patients was conducted to test the feasibility, face and construct validity, and reliability of a belief elicitation method. Using this method, participants expressed the probability of 3-year survival with and without warfarin. Applying adhesive dots or ``chips,'' each representing 5\% probability, in ``bins'' on a line, participants expressed their uncertainty and weight of belief about the effect of warfarin on 3-year survival. Results Of the 12 participants, 11 (92\%) reported that the belief elicitation method had face validity, 10 (83\%) found the questions clear, and 11 (92\%) found the response option easy to use. The median time to completion was 10 minutes (5\textendash 15 minutes). Internal validity testing found moderate agreement (weighted kappa=0.54\textendash 0.57). The intraclass correlation coefficient for test\textendash retest reliability was 0.93. Conclusion This method of belief elicitation for Bayesian priors is feasible, valid, and reliable. It can be considered for application in Bayesian clinical studies.},
  keywords = {Bayes,Bayesian,Belief elicitation,expert elicitation,prior elicitation,prior probability,Priors,Pulmonary hypertension,Reliability,stats,Validity}
}

@article{Kacelnik1997,
  title = {Risk-Sensitivity: Crossroads for Theories of Decision-Making.},
  shorttitle = {Risk-Sensitivity},
  author = {Kacelnik, A and Bateson, M.},
  year = {1997},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {1},
  number = {8},
  pages = {304--309},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/s1364-6613(97)01093-0},
  abstract = {Europe PMC is an archive of life sciences journal literature., Risk-sensitivity: crossroads for theories of decision-making.},
  langid = {english},
  pmid = {21223933}
}

@article{Landy2020,
  title = {Crowdsourcing Hypothesis Tests: {{Making}} Transparent How Design Choices Shape Research Results.},
  shorttitle = {Crowdsourcing Hypothesis Tests},
  author = {Landy, Justin F. and Jia, Miaolei (Liam) and Ding, Isabel L. and Viganola, Domenico and Tierney, Warren and Dreber, Anna and Johannesson, Magnus and Pfeiffer, Thomas and Ebersole, Charles R. and Gronau, Quentin F. and Ly, Alexander and {van den Bergh}, Don and Marsman, Maarten and Derks, Koen and Wagenmakers, Eric-Jan and Proctor, Andrew and Bartels, Daniel M. and Bauman, Christopher W. and Brady, William J. and Cheung, Felix and Cimpian, Andrei and Dohle, Simone and Donnellan, M. Brent and Hahn, Adam and Hall, Michael P. and {Jim{\'e}nez-Leal}, William and Johnson, David J. and Lucas, Richard E. and Monin, Beno{\^i}t and Montealegre, Andres and Mullen, Elizabeth and Pang, Jun and Ray, Jennifer and Reinero, Diego A. and Reynolds, Jesse and Sowden, Walter and Storage, Daniel and Su, Runkun and Tworek, Christina M. and Van Bavel, Jay J. and Walco, Daniel and Wills, Julian and Xu, Xiaobing and Yam, Kai Chi and Yang, Xiaoyu and Cunningham, William A. and Schweinsberg, Martin and Urwitz, Molly and Uhlmann, Eric L. and {The Crowdsourcing Hypothesis Tests Collaboration}},
  year = {2020},
  month = jan,
  journal = {Psychological Bulletin},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/bul0000220},
  langid = {english}
}

@article{Laudel2014,
  title = {Beyond Breakthrough Research: {{Epistemic}} Properties of Research and Their Consequences for Research Funding},
  shorttitle = {Beyond Breakthrough Research},
  author = {Laudel, Grit and Gl{\"a}ser, Jochen},
  year = {2014},
  month = sep,
  journal = {Research Policy},
  volume = {43},
  number = {7},
  pages = {1204--1216},
  issn = {0048-7333},
  doi = {10.1016/j.respol.2014.02.006},
  abstract = {The aim of this paper is to initiate a discussion about links between epistemic properties and institutional conditions for research by providing an exploratory analysis of such links featured by projects funded by the European Research Council (ERC). Our analysis identifies epistemic properties of research processes and links them to necessary and favourable conditions for research, and through these to institutional conditions provided by grants. Our findings enable the conclusion that there is research that is important for the progress of a field but is difficult to fund with common project grants. The predominance and standardisation of grant funding, which can be observed about many European countries, appears to reduce the chances of unconventional projects across all disciplines. Funding programmes of the `ERC-type' (featuring large and flexible budgets, long time horizons, and risk-tolerant selection processes) constitute an institutional innovation because they enable such research. However, while the ERC funding and other new funding schemes for exceptional research attempt to cover these requirements, they are unlikely to suffice.},
  langid = {english},
  keywords = {Epistemic properties of research,High risk – high reward research,Intellectual innovation,Research funding}
}

@article{Maki2006,
  title = {Models Are Experiments, Experiments Are Models},
  author = {M{\"a}ki, Uskali},
  year = {2006},
  month = aug,
  journal = {Journal of Economic Methodology},
  publisher = {{Taylor \& Francis Group}},
  doi = {10.1080/13501780500086255},
  abstract = {(2005). Models are experiments, experiments are models. Journal of Economic Methodology: Vol. 12, No. 2, pp. 303-315.},
  copyright = {Copyright Taylor and Francis Group, LLC},
  langid = {english}
}

@article{McElreath2015,
  title = {Replication, {{Communication}}, and the {{Population Dynamics}} of {{Scientific Discovery}}},
  author = {McElreath, Richard and Smaldino, Paul E.},
  year = {2015},
  month = aug,
  journal = {PLOS ONE},
  volume = {10},
  number = {8},
  pages = {e0136088},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0136088},
  abstract = {Many published research results are false (Ioannidis, 2005), and controversy continues over the roles of replication and publication policy in improving the reliability of research. Addressing these problems is frustrated by the lack of a formal framework that jointly represents hypothesis formation, replication, publication bias, and variation in research quality. We develop a mathematical model of scientific discovery that combines all of these elements. This model provides both a dynamic model of research as well as a formal framework for reasoning about the normative structure of science. We show that replication may serve as a ratchet that gradually separates true hypotheses from false, but the same factors that make initial findings unreliable also make replications unreliable. The most important factors in improving the reliability of research are the rate of false positives and the base rate of true hypotheses, and we offer suggestions for addressing each. Our results also bring clarity to verbal debates about the communication of research. Surprisingly, publication bias is not always an obstacle, but instead may have positive impacts\textemdash suppression of negative novel findings is often beneficial. We also find that communication of negative replications may aid true discovery even when attempts to replicate have diminished power. The model speaks constructively to ongoing debates about the design and conduct of science, focusing analysis and discussion on precise, internally consistent models, as well as highlighting the importance of population dynamics.},
  langid = {english},
  keywords = {Chemical elements,Mathematical models,Pigments,Population dynamics,Publication ethics,Research quality assessment,Research validity,Scientists}
}

@article{Mishra2010,
  title = {You Can't Always Get What You Want: {{The}} Motivational Effect of Need on Risk-Sensitive Decision-Making},
  shorttitle = {You Can't Always Get What You Want},
  author = {Mishra, Sandeep and Lalumi{\`e}re, Martin L.},
  year = {2010},
  month = jul,
  journal = {Journal of Experimental Social Psychology},
  volume = {46},
  number = {4},
  pages = {605--611},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2009.12.009},
  abstract = {Risky behavior in humans is typically considered irrational, reckless, and maladaptive. Risk-sensitivity theory, however, suggests that risky behavior may be adaptive in some circumstances: decision-makers should prefer high-risk options in situations of high need, when lower risk options are unlikely to meet those needs. This pattern of decision-making has been well established in the non-human animal literature, but little research has been conducted on humans. We demonstrate in a two-part experimental study that young men and women (n=115) behave as predicted by risk-sensitivity theory, shifting from risk-aversion to risk-proneness in situations of high need. This shift occurred whether decisions were made from description or from experience, and was observed controlling for sex and individual differences in general risk-taking propensity. This study is the first ecologically-relevant demonstration of risk-sensitive decision-making in humans.},
  langid = {english},
  keywords = {Decision-making,Ecological rationality,Individual differences,Need,Personality,Risk,Risk-sensitivity,Sex differences}
}

@article{Mishra2014,
  title = {Decision-{{Making Under Risk}}: {{Integrating Perspectives From Biology}}, {{Economics}}, and {{Psychology}}},
  shorttitle = {Decision-{{Making Under Risk}}},
  author = {Mishra, Sandeep},
  year = {2014},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {18},
  number = {3},
  pages = {280--307},
  issn = {1088-8683, 1532-7957},
  doi = {10.1177/1088868314530517},
  abstract = {Decision-making under risk has been variably characterized and examined in many different disciplines. However, interdisciplinary integration has not been forthcoming. Classic theories of decision-making have not been amply revised in light of greater empirical data on actual patterns of decision-making behavior. Furthermore, the meta-theoretical framework of evolution by natural selection has been largely ignored in theories of decision-making under risk in the human behavioral sciences. In this review, I critically examine four of the most influential theories of decision-making from economics, psychology, and biology: expected utility theory, prospect theory, risk-sensitivity theory, and heuristic approaches. I focus especially on risk-sensitivity theory, which offers a framework for understanding decision-making under risk that explicitly involves evolutionary considerations. I also review robust empirical evidence for individual differences and environmental/ situational factors that predict actual risky decision-making that any general theory must account for. Finally, I offer steps toward integrating various theoretical perspectives and empirical findings on risky decision-making.},
  langid = {english}
}

@article{Muller2014e,
  title = {Postdoctoral {{Life Scientists}} and {{Supervision Work}} in the {{Contemporary University}}: {{A Case Study}} of {{Changes}} in the {{Cultural Norms}} of {{Science}}},
  shorttitle = {Postdoctoral {{Life Scientists}} and {{Supervision Work}} in the {{Contemporary University}}},
  author = {M{\"u}ller, Ruth},
  year = {2014},
  month = sep,
  journal = {Minerva},
  volume = {52},
  number = {3},
  pages = {329--349},
  issn = {1573-1871},
  doi = {10.1007/s11024-014-9257-y},
  abstract = {This paper explores the ways in which postdoctoral life scientists engage in supervision work in academic institutions in Austria. Reward systems and career conditions in academic institutions in most European and other OECD countries have changed significantly during the last two decades. While an increasing focus is put on evaluating research performances, little reward is attached to excellent performances in mentoring and advising students. Postdoctoral scientists mostly inhabit fragile institutional positions and experience harsh competition, as the number of available senior positions is small compared to that of young scientists striving for an academic career. To prevail in this competition, publications and mobility are key. Educational work is rarely rewarded. Nevertheless, postdocs play a key role in educating PhD students, as overburdened senior scientists often pass on practical supervision duties to their postdoctoral fellows. This paper shows how under these conditions, postdocs reframe the students they supervise as potential resources for co-authored publications. What might look like a mutually beneficial solution at a first glance, in practice implies the subordination of the values of education to the logic of production, which marginalizes spaces primarily devoted to education. The author argues that conflicts like this are indicative of broader changes in the cultural norms of science and academic citizenship, rendering community-oriented tasks such as education work less attractive to academic scientists. Since education and supervision work are central cornerstones of any functioning higher education and research system, this could have negative repercussions for the long-term development of academic institutions.},
  langid = {english}
}

@article{Muller2017,
  title = {Thinking with Indicators. {{Exploring}} the Epistemic Impacts of Academic Performance Indicators in the Life Sciences},
  author = {M{\"u}ller, Ruth and {de Rijcke}, Sarah},
  year = {2017},
  month = jul,
  journal = {Research Evaluation},
  volume = {26},
  number = {3},
  pages = {157--168},
  issn = {0958-2029},
  doi = {10.1093/reseval/rvx023},
  abstract = {While quantitative performance indicators are widely used by organizations and individuals for evaluative purposes, little is known about their impacts on the epistemic processes of academic knowledge production. In this article we bring together three qualitative research projects undertaken in the Netherlands and Austria to contribute to filling this gap. The projects explored the role of performance metrics in the life sciences, and the interactions between institutional and disciplinary cultures of evaluating research in these fields. Our analytic perspective is focused on understanding how researchers themselves give value to research, and in how far these practices are related to performance metrics. The article zooms in on three key moments in research processes to show how `thinking with indicators' is becoming a central aspect of research activities themselves: (1) the planning and conception of research projects, (2) the social organization of research processes, and (3) determining the endpoints of research processes. Our findings demonstrate how the worth of research activities becomes increasingly assessed and defined by their potential to yield high value in quantitative terms. The analysis makes visible how certain norms and values related to performance metrics are stabilized as they become integrated into routine practices of knowledge production. Other norms and criteria for scientific quality, e.g. epistemic originality, long-term scientific progress, societal relevance, and social responsibility, receive less attention or become redefined through their relations to quantitative indicators. We understand this trend to be in tension with policy goals that seek to encourage innovative, societally relevant, and responsible research.}
}

@article{Munafo2015,
  title = {Using Prediction Markets to Forecast Research Evaluations},
  author = {Munafo, Marcus R. and Pfeiffer, Thomas and Altmejd, Adam and Heikensten, Emma and Almenberg, Johan and Bird, Alexander and Chen, Yiling and Wilson, Brad and Johannesson, Magnus and Dreber, Anna},
  year = {2015},
  month = oct,
  journal = {Royal Society Open Science},
  volume = {2},
  number = {10},
  pages = {150287},
  issn = {2054-5703},
  doi = {10.1098/rsos.150287},
  langid = {english},
  keywords = {meta-science,prediction markets,prior elicitation,prior probability}
}

@article{Nissen2016,
  title = {Publication Bias and the Canonization of False Facts},
  author = {Nissen, Silas Boye and Magidson, Tali and Gross, Kevin and Bergstrom, Carl T},
  editor = {Rodgers, Peter},
  year = {2016},
  month = dec,
  journal = {eLife},
  volume = {5},
  pages = {e21451},
  issn = {2050-084X},
  doi = {10.7554/eLife.21451},
  abstract = {Science is facing a ``replication crisis'' in which many experimental findings cannot be replicated and are likely to be false. Does this imply that many scientific facts are false as well? To find out, we explore the process by which a claim becomes fact. We model the community's confidence in a claim as a Markov process with successive published results shifting the degree of belief. Publication bias in favor of positive findings influences the distribution of published results. We find that unless a sufficient fraction of negative results are published, false claims frequently can become canonized as fact. Data-dredging, p-hacking, and similar behaviors exacerbate the problem. Should negative results become easier to publish as a claim approaches acceptance as a fact, however, true and false claims would be more readily distinguished. To the degree that the model reflects the real world, there may be serious concerns about the validity of purported facts in some disciplines.},
  keywords = {false positive,hypothesis testing,meta-science,phil sci,publication bias,replication crisis}
}

@article{OConnor2019,
  title = {The Natural Selection of Conservative Science},
  author = {O'Connor, Cailin},
  year = {2019},
  month = aug,
  journal = {Studies in History and Philosophy of Science Part A},
  volume = {76},
  pages = {24--29},
  issn = {0039-3681},
  doi = {10.1016/j.shpsa.2018.09.007},
  abstract = {Social epistemologists have argued that high risk, high reward science has an important role to play in scientific communities. Recently, though, it has also been argued that various scientific fields seem to be trending towards conservatism\textemdash the increasing production of what Kuhn (1962) might have called `normal science'. This paper will explore a possible explanation for this sort of trend: that the process by which scientific research groups form, grow, and dissolve might be inherently hostile to such science. In particular, I employ a paradigm developed by Smaldino and McElreath (2016) that treats a scientific community as a population undergoing selection. As will become clear, perhaps counter-intuitively this sort of process in some ways promotes high risk, high reward science. But, as I will point out, risky science is, in general, the sort of thing that is hard to repeat. While more conservative scientists will be able to train students capable of continuing their successful projects, and so create thriving lineages, successful risky science may not be the sort of thing one can easily pass on. In such cases, the structure of scientific communities selects against high risk, high rewards projects. More generally, this project makes clear that there are at least two processes to consider in thinking about how incentives shape scientific communities\textemdash the process by which individual scientists make choices about their careers and research, and the selective process governing the formation of new research groups.},
  langid = {english}
}

@article{Schotter2014,
  title = {Belief {{Elicitation}} in the {{Laboratory}}},
  author = {Schotter, Andrew and Trevino, Isabel},
  year = {2014},
  month = aug,
  journal = {Annual Review of Economics},
  volume = {6},
  number = {1},
  pages = {103--128},
  issn = {1941-1383, 1941-1391},
  doi = {10.1146/annurev-economics-080213-040927},
  langid = {english}
}

@article{Silberzahn2018,
  title = {Many {{Analysts}}, {{One Data Set}}: {{Making Transparent How Variations}} in {{Analytic Choices Affect Results}}},
  shorttitle = {Many {{Analysts}}, {{One Data Set}}},
  author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahn{\'i}k, {\v S}. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and {Gamez-Djokic}, M. and Glenz, A. and {Gordon-McKeon}, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and H{\"o}gden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schl{\"u}ter, E. and Sch{\"o}nbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Sp{\"o}rlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {337--356},
  issn = {2515-2459},
  doi = {10.1177/2515245917747646},
  abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts' prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
  langid = {english},
  keywords = {inference,meta-science,p-hacking,prior elicitation,prior probability,replication crisis,stats}
}

@article{Smaldino2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, R.},
  year = {2016},
  journal = {Royal Society Open Science},
  volume = {3},
  pages = {160384},
  doi = {10.1098/rsos.160384}
}

@article{Stewart2021,
  title = {The Natural Selection of Good Science},
  author = {Stewart, Alexander J. and Plotkin, Joshua B.},
  year = {2021},
  month = may,
  journal = {Nature Human Behaviour},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01111-x},
  abstract = {Scientists in some fields are concerned that many published results are false. Recent models predict selection for false positives as the inevitable result of pressure to publish, even when scientists are penalized for publications that fail to replicate. We model the cultural evolution of research practices when laboratories are allowed to expend effort on theory, enabling them, at a cost, to identify hypotheses that are more likely to be true, before empirical testing. Theory can restore high effort in research practice and suppress false positives to a technical minimum, even without replication. The mere ability to choose between two sets of hypotheses, one with greater prior chance of being correct, promotes better science than can be achieved with effortless access to the set of stronger hypotheses. Combining theory and replication can have synergistic effects. On the basis of our analysis, we propose four simple recommendations to promote good science.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english}
}

@article{Tiokhin2021a,
  title = {Competition for Priority Harms the Reliability of Science, but Reforms Can Help},
  author = {Tiokhin, Leonid and Yan, Minhua and Morgan, Thomas J. H.},
  year = {2021},
  month = jul,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {7},
  pages = {857--867},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-01040-1},
  abstract = {Incentives for priority of discovery are hypothesized to harm scientific reliability. Here, we evaluate this hypothesis by developing an evolutionary agent-based model of a competitive scientific process. We find that rewarding priority of discovery causes populations to culturally evolve towards conducting research with smaller samples. This reduces research reliability and the information value of the average study. Increased start-up costs for setting up single studies and increased payoffs for secondary results (also known as scoop protection) attenuate the negative effects of competition. Furthermore, large rewards for negative results promote the evolution of smaller sample sizes. Our results confirm the logical coherence of scoop protection reforms at several journals. Our results also imply that reforms to increase scientific efficiency, such as rapid journal turnaround times, may produce collateral damage by incentivizing lower-quality research; in contrast, reforms that increase start-up costs, such as pre-registration and registered reports, may generate incentives for higher-quality research.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Economics;Human behaviour Subject\_term\_id: economics;human-behaviour}
}

@article{vanDalen2012,
  title = {Intended and Unintended Consequences of a Publish-or-Perish Culture: {{A}} Worldwide Survey},
  shorttitle = {Intended and Unintended Consequences of a Publish-or-Perish Culture},
  author = {{van Dalen}, Hendrik P. and Henkens, K{\`e}ne},
  year = {2012},
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {63},
  number = {7},
  pages = {1282--1293},
  issn = {1532-2890},
  doi = {10.1002/asi.22636},
  abstract = {How does publication pressure in modern-day universities affect the intrinsic and extrinsic rewards in science? By using a worldwide survey among demographers in developed and developing countries, the authors show that the large majority perceive the publication pressure as high, but more so in Anglo-Saxon countries and to a lesser extent in Western Europe. However, scholars see both the pros (upward mobility) and cons (excessive publication and uncitedness, neglect of policy issues, etc.) of the so-called publish-or-perish culture. By measuring behavior in terms of reading and publishing, and perceived extrinsic rewards and stated intrinsic rewards of practicing science, it turns out that publication pressure negatively affects the orientation of demographers towards policy and knowledge sharing. There are no signs that the pressure affects reading and publishing outside the core discipline.},
  langid = {english},
  keywords = {bibliometrics,scientists,surveys},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.22636}
}

@article{Wacholder2004,
  title = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}: {{An Approach}} for {{Molecular Epidemiology Studies}}},
  shorttitle = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}},
  author = {Wacholder, S. and Chanock, S. and {Garcia-Closas}, M. and {El ghormli}, L. and Rothman, N.},
  year = {2004},
  month = mar,
  journal = {JNCI Journal of the National Cancer Institute},
  volume = {96},
  number = {6},
  pages = {434--442},
  issn = {0027-8874, 1460-2105},
  doi = {10.1093/jnci/djh075},
  langid = {english}
}

@article{Winterhalder1999,
  title = {Risk-Senstive Adaptive Tactics: {{Models}} and Evidence from Subsistence Studies in Biology and Anthropology},
  shorttitle = {Risk-Senstive Adaptive Tactics},
  author = {Winterhalder, Bruce and Lu, Flora and Tucker, Bram},
  year = {1999},
  month = dec,
  journal = {Journal of Archaeological Research},
  volume = {7},
  number = {4},
  pages = {301--348},
  issn = {1573-7756},
  doi = {10.1007/BF02446047},
  abstract = {Risk-sensitive analysis of subsistence adaptations is warranted when (i) outcomes are to some degree unpredictable and (ii) they have nonlinear consequences for fitness and/or utility. Both conditions are likely to be common among peoples studied by ecologicll anthropologists and archaeologists. We develop a general conceptual model of risk. We then review and summarize the extensive empirical literatures from biology and anthropology for methodological insights and for their comparative potential. Risk-sensitive adaptive tactics are diverse and they are taxonomically widespread. However, the anthropological literature rarely makes use of formal models of risk-sensitive adaptation, while the biological literature lacks naturalistic observations of risk-sensitive behavior. Both anthropology and biology could benefit from greater interdisciplinary exchange.},
  langid = {english}
}

@article{Yang2020a,
  title = {Estimating the Deep Replicability of Scientific Findings Using Human and Artificial Intelligence},
  author = {Yang, Yang and Youyou, Wu and Uzzi, Brian},
  year = {2020},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {20},
  pages = {10762--10768},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1909046117},
  langid = {english}
}

@article{Zollman2009,
  title = {Optimal {{Publishing Strategies}}},
  author = {Zollman, Kevin J. S.},
  year = {2009},
  month = jun,
  journal = {Episteme},
  volume = {6},
  number = {2},
  pages = {185--199},
  issn = {1742-3600, 1750-0117},
  doi = {10.3366/E174236000900063X},
  abstract = {Journals regulate a significant portion of the communication between scientists. This paper devises an agent-based model of scientific practice and uses it to compare various strategies for selecting publications by journals. Surprisingly, it appears that the best selection method for journals is to publish relatively few papers and to select those papers it publishes at random from the available ``above threshold'' papers it receives. This strategy is most effective at maintaining an appropriate type of diversity that is needed to solve a particular type of scientific problem. This problem and the limitation of the model is discussed in detail.},
  langid = {english}
}

@article{Zondervan-Zwijnenburg2017,
  title = {Application and {{Evaluation}} of an {{Expert Judgment Elicitation Procedure}} for {{Correlations}}},
  author = {{Zondervan-Zwijnenburg}, Mari{\"e}lle and {van de Schoot-Hubeek}, Wenneke and Lek, Kimberley and Hoijtink, Herbert and {van de Schoot}, Rens},
  year = {2017},
  journal = {Frontiers in Psychology},
  volume = {8},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00090},
  abstract = {The purpose of the current study was to apply and evaluate a procedure to elicit expert judgments about correlations, and to update this information with empirical data. The result is a face-to-face group elicitation procedure with as its central element a trial roulette question that elicits experts' judgments expressed as distributions. During the elicitation procedure, a concordance probability question was used to provide feedback to the experts on their judgments. We evaluated the elicitation procedure in terms of validity and reliability by means of an application with a small sample of experts. Validity means that the elicited distributions accurately represent the experts' judgments. Reliability concerns the consistency of the elicited judgments over time. Four behavioral scientists provided their judgments with respect to the correlation between cognitive potential and academic performance for two separate populations enrolled at a specific school in the Netherlands that provides special education to youth with severe behavioral problems: youth with autism spectrum disorder (ASD), and youth with diagnoses other than ASD. Measures of face-validity, feasibility, convergent validity, coherence, and intra-rater reliability showed promising results. Furthermore, the current study illustrates the use of the elicitation procedure and elicited distributions in a social science application. The elicited distributions were used as a prior for the correlation, and updated with data for both populations collected at the school of interest. The current study shows that the newly developed elicitation procedure combining the trial roulette method with the elicitation of correlations is a promising tool, and that the results of the procedure are useful as prior information in a Bayesian analysis.},
  langid = {english},
  keywords = {Bayes,Bayesian Analysis,correlation,Correlation,Elicitation procedure,expert elicitation,Expert judgment,informative priors,prior elicitation,prior probability}
}


