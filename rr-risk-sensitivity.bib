@article{Zhang2017,
  title = {Toward a Model of Risky Decisions: {{Synergistic}} Effect of Affect Intensity and Affective Processing on Risk-Seeking as a Function of Decision Domain},
  shorttitle = {Toward a Model of Risky Decisions},
  author = {Zhang, Yufeng and Chen, Zhuo Job and Li, Hong},
  year = {2017},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  volume = {73},
  pages = {235--242},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.06.006},
  urldate = {2018-04-06},
  abstract = {Four studies investigated the causal link of affect intensity with risky decisions, and showed a striking contrast of life-saving and valuables-saving domains. When social distance is small people are more risk-seeking in the life-saving domain but less risk-seeking in the valuables-saving domain (Study 1), and the results remain robust under different framings (Study 2). Relatedly, people rely more on affective processing when social distance is small in the life-saving domain, but not in the valuables-saving domain (Study 3). Furthermore, in the life-saving domain social distance exerts an effect on risk preference under affective processing but not under deliberate processing, whereas, in the valuables-saving domain, social distance influences risk preference under deliberate processing but not under affective processing (Study 4). A unified, causal model of risky decisions is proposed to account for these findings and the fundamental differences among decision domains in light of their relationships with the affective processing. The model has a potential to generalize into other decision domains.},
  keywords = {Affect intensity,Decision making,Domain,Framing,OSBadgeOpenData,OSBadgeOpenMaterials,Risk}
}

@article{Nosek2014,
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  shorttitle = {Registered {{Reports}}},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {137--141},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000192},
  urldate = {2018-05-23},
  langid = {english}
}

@article{Motyl2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  year = {2017},
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  number = {1},
  pages = {34--58},
  issn = {1939-1315, 0022-3514},
  doi = {10.1037/pspa0000084},
  urldate = {2018-06-14},
  langid = {english}
}

@article{Wagenmakers2012,
  title = {An {{Agenda}} for {{Purely Confirmatory Research}}},
  author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and {van der Maas}, Han L. J. and Kievit, Rogier A.},
  year = {2012},
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {632--638},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612463078},
  urldate = {2018-06-14},
  langid = {english}
}

@article{Smaldino2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, R.},
  year = {2016},
  journal = {Royal Society Open Science},
  volume = {3},
  pages = {160384},
  doi = {10.1098/rsos.160384}
}

@article{Mahoney1977,
  title = {Publication {{Prejudices}}: {{An Experimental Study}} of {{Confirmatory Bias}} in the {{Peer Review System}}},
  author = {Mahoney, Michael J.},
  year = {1977},
  journal = {Cognitive Therapy and Research},
  volume = {1},
  number = {2},
  pages = {161--175},
  doi = {10.1007/BF01173636}
}

@article{Sterling1959,
  title = {Publication {{Decisions}} and Their {{Possible Effects}} on {{Inferences Drawn}} from {{Tests}} of {{Significance---or Vice Versa}}},
  author = {Sterling, Theodore D.},
  year = {1959},
  journal = {Journal of the American Statistical Association},
  volume = {54},
  number = {285},
  eprint = {http://dx.doi.org/10.1080/01621459.1959.10501497},
  pages = {30--34},
  doi = {10.1080/01621459.1959.10501497}
}

@article{Agnoli2017,
  title = {Questionable Research Practices among Italian Research Psychologists},
  author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
  year = {2017},
  journal = {PLOS ONE},
  volume = {12},
  number = {3},
  pages = {e0172792},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0172792},
  abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
  bdsk-url-2 = {http://dx.doi.org/10.1371/journal.pone.0172792}
}

@article{Chambers2013,
  title = {Registered Reports: {{A}} New Publishing Initiative at {{Cortex}}},
  author = {Chambers, C. D.},
  year = {2013},
  journal = {Cortex},
  volume = {49},
  pages = {606--610},
  doi = {10.1016/j.cortex.2012.12.016}
}

@article{Franco2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, Annie and Malhotra, N. and Simonovits, G.},
  year = {2014},
  journal = {Science},
  volume = {345},
  number = {6203},
  pages = {1502--1505},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255484},
  urldate = {2018-06-19},
  langid = {english},
  keywords = {publication bias,replication crisis}
}

@article{Franco2016,
  title = {Underreporting in {{Psychology Experiments}}: {{Evidence From}} a {{Study Registry}}},
  shorttitle = {Underreporting in {{Psychology Experiments}}},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  year = {2016},
  journal = {Social Psychological and Personality Science},
  volume = {7},
  number = {1},
  pages = {8--12},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615598377},
  urldate = {2018-06-19},
  langid = {english},
  keywords = {publication bias,replication crisis}
}

@article{Johnson2010,
  title = {A Valid and Reliable Belief Elicitation Method for {{Bayesian}} Priors},
  author = {Johnson, Sindhu R. and Tomlinson, George A. and Hawker, Gillian A. and Granton, John T. and Grosbein, Haddas A. and Feldman, Brian M.},
  year = {2010},
  month = apr,
  journal = {Journal of Clinical Epidemiology},
  volume = {63},
  number = {4},
  pages = {370--383},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2009.08.005},
  urldate = {2019-02-25},
  abstract = {Objective Bayesian inference has the advantage of formally incorporating prior beliefs about the effect of an intervention into analyses of treatment effect through the use of prior probability distributions or ``priors.'' Multiple methods to elicit beliefs from experts for inclusion in a Bayesian study have been used; however, the measurement properties of these methods have been infrequently evaluated. The objectives of this study were to evaluate the feasibility, validity, and reliability of a belief elicitation method for Bayesian priors. Study Design and Setting A single-center, cross-sectional study using a sample of academic specialists who treat pulmonary hypertension patients was conducted to test the feasibility, face and construct validity, and reliability of a belief elicitation method. Using this method, participants expressed the probability of 3-year survival with and without warfarin. Applying adhesive dots or ``chips,'' each representing 5\% probability, in ``bins'' on a line, participants expressed their uncertainty and weight of belief about the effect of warfarin on 3-year survival. Results Of the 12 participants, 11 (92\%) reported that the belief elicitation method had face validity, 10 (83\%) found the questions clear, and 11 (92\%) found the response option easy to use. The median time to completion was 10 minutes (5--15 minutes). Internal validity testing found moderate agreement (weighted kappa=0.54--0.57). The intraclass correlation coefficient for test--retest reliability was 0.93. Conclusion This method of belief elicitation for Bayesian priors is feasible, valid, and reliable. It can be considered for application in Bayesian clinical studies.},
  keywords = {Bayes,Bayesian,Belief elicitation,expert elicitation,prior elicitation,prior probability,Priors,Pulmonary hypertension,Reliability,stats,Validity}
}

@article{Zondervan-Zwijnenburg2017,
  title = {Application and {{Evaluation}} of an {{Expert Judgment Elicitation Procedure}} for {{Correlations}}},
  author = {{Zondervan-Zwijnenburg}, Mari{\"e}lle and {van de Schoot-Hubeek}, Wenneke and Lek, Kimberley and Hoijtink, Herbert and {van de Schoot}, Rens},
  year = {2017},
  journal = {Frontiers in Psychology},
  volume = {8},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00090},
  urldate = {2019-02-25},
  abstract = {The purpose of the current study was to apply and evaluate a procedure to elicit expert judgments about correlations, and to update this information with empirical data. The result is a face-to-face group elicitation procedure with as its central element a trial roulette question that elicits experts' judgments expressed as distributions. During the elicitation procedure, a concordance probability question was used to provide feedback to the experts on their judgments. We evaluated the elicitation procedure in terms of validity and reliability by means of an application with a small sample of experts. Validity means that the elicited distributions accurately represent the experts' judgments. Reliability concerns the consistency of the elicited judgments over time. Four behavioral scientists provided their judgments with respect to the correlation between cognitive potential and academic performance for two separate populations enrolled at a specific school in the Netherlands that provides special education to youth with severe behavioral problems: youth with autism spectrum disorder (ASD), and youth with diagnoses other than ASD. Measures of face-validity, feasibility, convergent validity, coherence, and intra-rater reliability showed promising results. Furthermore, the current study illustrates the use of the elicitation procedure and elicited distributions in a social science application. The elicited distributions were used as a prior for the correlation, and updated with data for both populations collected at the school of interest. The current study shows that the newly developed elicitation procedure combining the trial roulette method with the elicitation of correlations is a promising tool, and that the results of the procedure are useful as prior information in a Bayesian analysis.},
  langid = {english},
  keywords = {Bayes,Bayesian Analysis,correlation,Correlation,Elicitation procedure,expert elicitation,Expert judgment,informative priors,prior elicitation,prior probability}
}

@article{Clemen2000,
  title = {Assessing {{Dependence}}: {{Some Experimental Results}}},
  shorttitle = {Assessing {{Dependence}}},
  author = {Clemen, Robert T. and Fischer, Gregory W. and Winkler, Robert L.},
  year = {2000},
  month = aug,
  journal = {Management Science},
  volume = {46},
  number = {8},
  pages = {1100--1115},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.46.8.1100.12023},
  urldate = {2019-02-25},
  langid = {english},
  keywords = {correlation,expert elicitation,prior elicitation,prior probability,stats}
}

@article{Hemming2018,
  title = {A Practical Guide to Structured Expert Elicitation Using the {{IDEA}} Protocol},
  author = {Hemming, Victoria and Burgman, Mark A. and Hanea, Anca M. and McBride, Marissa F. and Wintle, Bonnie C.},
  year = {2018},
  journal = {Methods in Ecology and Evolution},
  volume = {9},
  number = {1},
  pages = {169--180},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12857},
  urldate = {2019-02-25},
  abstract = {Expert judgement informs a variety of important applications in conservation and natural resource management, including threatened species management, environmental impact assessment and structured decision-making. However, expert judgements can be prone to contextual biases. Structured elicitation protocols mitigate these biases, and improve the accuracy and transparency of the resulting judgements. Despite this, the elicitation of expert judgement within conservation and natural resource management remains largely informal. We suggest this may be attributed to financial and practical constraints, which are not addressed by many existing structured elicitation protocols. In this paper, we advocate that structured elicitation protocols must be adopted when expert judgements are used to inform science. In order to motivate a wider adoption of structured elicitation protocols, we outline the IDEA protocol. The protocol improves the accuracy of expert judgements and includes several key steps which may be familiar to many conservation researchers, such as the four-step elicitation, and a modified Delphi procedure (``Investigate,'' ``Discuss,'' ``Estimate'' and ``Aggregate''). It can also incorporate remote elicitation, making structured expert judgement accessible on a modest budget. The IDEA protocol has recently been outlined in the scientific literature; however, a detailed description has been missing. This paper fills that important gap by clearly outlining each of the steps required to prepare for and undertake an elicitation. While this paper focuses on the need for the IDEA protocol within conservation and natural resource management, the protocol (and the advice contained in this paper) is applicable to a broad range of scientific domains, as evidenced by its application to biosecurity, engineering and political forecasting. By clearly outlining the IDEA protocol, we hope that structured protocols will be more widely understood and adopted, resulting in improved judgements and increased transparency when expert judgement is required.},
  copyright = {{\copyright} 2017 The Authors. Methods in Ecology and Evolution {\copyright} 2017 British Ecological Society},
  langid = {english},
  keywords = {Delphi,expert elicitation,forecasting,four-step elicitation,IDEA protocol,prior elicitation,prior probability,quantitative estimates,structured expert judgement}
}

@article{Forsell2018,
  title = {Predicting Replication Outcomes in the {{Many Labs}} 2 Study},
  author = {Forsell, Eskil and Viganola, Domenico and Pfeiffer, Thomas and Almenberg, Johan and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus and Dreber, Anna},
  year = {2018},
  month = oct,
  journal = {Journal of Economic Psychology},
  issn = {0167-4870},
  doi = {10.1016/j.joep.2018.10.009},
  urldate = {2019-02-19},
  abstract = {Understanding and improving reproducibility is crucial for scientific progress. Prediction markets and related methods of eliciting peer beliefs are promising tools to predict replication outcomes. We invited researchers in the field of psychology to judge the replicability of 24 studies replicated in the large scale Many Labs 2 project. We elicited peer beliefs in prediction markets and surveys about two replication success metrics: the probability that the replication yields a statistically significant effect in the original direction (p\,{$<$}\,0.001), and the relative effect size of the replication. The prediction markets correctly predicted 75\% of the replication outcomes, and were highly correlated with the replication outcomes. Survey beliefs were also significantly correlated with replication outcomes, but had larger prediction errors. The prediction markets for relative effect sizes attracted little trading and thus did not work well. The survey beliefs about relative effect sizes performed better and were significantly correlated with observed relative effect sizes. The results suggest that replication outcomes can be predicted and that the elicitation of peer beliefs can increase our knowledge about scientific reproducibility and the dynamics of hypothesis testing.},
  keywords = {Beliefs,ManyLabs,meta-science,prediction markets,Prediction markets,prior elicitation,prior probability,replicability,replication crisis,Replications,Reproducibility}
}

@article{Dreber2015,
  title = {Using Prediction Markets to Estimate the Reproducibility of Scientific Research},
  author = {Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus},
  year = {2015},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {50},
  pages = {15343--15347},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1516179112},
  urldate = {2019-02-19},
  langid = {english},
  keywords = {meta-science,prediction markets,prior elicitation,prior probability,replicability,replication crisis}
}

@article{Camerer2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  urldate = {2019-02-19},
  abstract = {Camerer et al. carried out replications of 21 Science and Nature social science experiments, successfully replicating 13 out of 21 (62\%). Effect sizes of replications were about half of the size of the originals.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {meta-science,prediction markets,prior elicitation,prior probability,replicability,replication crisis}
}

@article{Munafo2015,
  title = {Using Prediction Markets to Forecast Research Evaluations},
  author = {Munafo, Marcus R. and Pfeiffer, Thomas and Altmejd, Adam and Heikensten, Emma and Almenberg, Johan and Bird, Alexander and Chen, Yiling and Wilson, Brad and Johannesson, Magnus and Dreber, Anna},
  year = {2015},
  month = oct,
  journal = {Royal Society Open Science},
  volume = {2},
  number = {10},
  pages = {150287},
  issn = {2054-5703},
  doi = {10.1098/rsos.150287},
  urldate = {2019-02-19},
  langid = {english},
  keywords = {meta-science,prediction markets,prior elicitation,prior probability}
}

@article{Dickersin1990,
  title = {The Existence of Publication Bias and Risk Factors for Its Occurrence},
  author = {Dickersin, K.},
  year = {1990},
  month = mar,
  journal = {JAMA: The Journal of the American Medical Association},
  volume = {263},
  number = {10},
  pages = {1385--1389},
  issn = {00987484, 15383598},
  doi = {10.1001/jama.263.10.1385},
  urldate = {2019-02-19},
  langid = {english},
  keywords = {meta-science,preregistration,publication bias}
}

@article{Silberzahn2018,
  title = {Many {{Analysts}}, {{One Data Set}}: {{Making Transparent How Variations}} in {{Analytic Choices Affect Results}}},
  shorttitle = {Many {{Analysts}}, {{One Data Set}}},
  author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahn{\'i}k, {\v S}. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and {Gamez-Djokic}, M. and Glenz, A. and {Gordon-McKeon}, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and H{\"o}gden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schl{\"u}ter, E. and Sch{\"o}nbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Sp{\"o}rlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {337--356},
  issn = {2515-2459},
  doi = {10.1177/2515245917747646},
  urldate = {2019-02-19},
  abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts' prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
  langid = {english},
  keywords = {inference,meta-science,p-hacking,prior elicitation,prior probability,replication crisis,stats}
}

@article{Schotter2014,
  title = {Belief {{Elicitation}} in the {{Laboratory}}},
  author = {Schotter, Andrew and Trevino, Isabel},
  year = {2014},
  month = aug,
  journal = {Annual Review of Economics},
  volume = {6},
  number = {1},
  pages = {103--128},
  issn = {1941-1383, 1941-1391},
  doi = {10.1146/annurev-economics-080213-040927},
  urldate = {2019-03-14},
  langid = {english}
}

@article{Schafer2019,
  title = {The {{Meaningfulness}} of {{Effect Sizes}} in {{Psychological Research}}: {{Differences Between Sub-Disciplines}} and the {{Impact}} of {{Potential Biases}}},
  shorttitle = {The {{Meaningfulness}} of {{Effect Sizes}} in {{Psychological Research}}},
  author = {Sch{\"a}fer, Thomas and Schwarz, Marcus A.},
  year = {2019},
  journal = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.00813},
  urldate = {2019-04-15},
  abstract = {Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes---when is an effect small, medium, or large?---has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = .36) were much larger than effects from the latter (median r = .16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.},
  langid = {english},
  keywords = {Cohen,effect size,effect sizes,meta-science,power,preregistration,Publication Bias,replicability,Replication,RRs,Sample Size}
}

@article{Garthwaite2005,
  title = {Statistical {{Methods}} for {{Eliciting Probability Distributions}}},
  author = {Garthwaite, Paul H and Kadane, Joseph B and O'Hagan, Anthony},
  year = {2005},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {470},
  pages = {680--701},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214505000000105},
  urldate = {2019-04-24},
  langid = {english}
}

@article{Sterling1995,
  title = {Publication {{Decisions Revisited}}: {{The Effect}} of the {{Outcome}} of {{Statistical Tests}} on the {{Decision}} to {{Publish}} and {{Vice Versa}}},
  shorttitle = {Publication {{Decisions Revisited}}},
  author = {Sterling, Theodore D. and Rosenbaum, W. L. and Weinkam, J. J.},
  year = {1995},
  month = feb,
  journal = {The American Statistician},
  volume = {49},
  number = {1},
  eprint = {2684823},
  eprinttype = {jstor},
  pages = {108},
  issn = {00031305},
  doi = {10.2307/2684823},
  urldate = {2019-08-15},
  keywords = {meta-science,NHST,publication bias}
}

@article{Wacholder2004,
  title = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}: {{An Approach}} for {{Molecular Epidemiology Studies}}},
  shorttitle = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}},
  author = {Wacholder, S. and Chanock, S. and {Garcia-Closas}, M. and {El ghormli}, L. and Rothman, N.},
  year = {2004},
  month = mar,
  journal = {JNCI Journal of the National Cancer Institute},
  volume = {96},
  number = {6},
  pages = {434--442},
  issn = {0027-8874, 1460-2105},
  doi = {10.1093/jnci/djh075},
  urldate = {2019-08-28},
  langid = {english}
}

@article{Allen2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  year = {2019},
  month = may,
  journal = {PLOS Biology},
  volume = {17},
  number = {5},
  pages = {e3000246},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  urldate = {2019-08-28},
  langid = {english}
}

@article{Kaplan2015,
  title = {Likelihood of {{Null Effects}} of {{Large NHLBI Clinical Trials Has Increased}} over {{Time}}},
  author = {Kaplan, Robert M. and Irvin, Veronica L.},
  editor = {Garattini, Silvio},
  year = {2015},
  month = aug,
  journal = {PLOS ONE},
  volume = {10},
  number = {8},
  pages = {e0132382},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0132382},
  urldate = {2019-08-28},
  langid = {english}
}

@article{Coburn2015,
  title = {Publication Bias as a Function of Study Characteristics.},
  author = {Coburn, Kathleen M. and Vevea, Jack L.},
  year = {2015},
  journal = {Psychological Methods},
  volume = {20},
  number = {3},
  pages = {310--330},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000046},
  urldate = {2019-08-28},
  langid = {english}
}

@article{Ingre2018,
  title = {Estimating Statistical Power, Posterior Probability and Publication Bias of Psychological Research Using the Observed Replication Rate},
  author = {Ingre, Michael and Nilsonne, Gustav},
  year = {2018},
  month = sep,
  journal = {Royal Society Open Science},
  volume = {5},
  number = {9},
  pages = {181190},
  issn = {2054-5703, 2054-5703},
  doi = {10.1098/rsos.181190},
  urldate = {2019-08-28},
  langid = {english}
}

@article{Nissen2016,
  title = {Publication Bias and the Canonization of False Facts},
  author = {Nissen, Silas Boye and Magidson, Tali and Gross, Kevin and Bergstrom, Carl T},
  editor = {Rodgers, Peter},
  year = {2016},
  month = dec,
  journal = {eLife},
  volume = {5},
  pages = {e21451},
  issn = {2050-084X},
  doi = {10.7554/eLife.21451},
  urldate = {2019-09-11},
  abstract = {Science is facing a ``replication crisis'' in which many experimental findings cannot be replicated and are likely to be false. Does this imply that many scientific facts are false as well? To find out, we explore the process by which a claim becomes fact. We model the community's confidence in a claim as a Markov process with successive published results shifting the degree of belief. Publication bias in favor of positive findings influences the distribution of published results. We find that unless a sufficient fraction of negative results are published, false claims frequently can become canonized as fact. Data-dredging, p-hacking, and similar behaviors exacerbate the problem. Should negative results become easier to publish as a claim approaches acceptance as a fact, however, true and false claims would be more readily distinguished. To the degree that the model reflects the real world, there may be serious concerns about the validity of purported facts in some disciplines.},
  keywords = {false positive,hypothesis testing,meta-science,phil sci,publication bias,replication crisis}
}

@article{Fiedler2016,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  year = {2016},
  month = jan,
  journal = {Social Psychological and Personality Science},
  volume = {7},
  number = {1},
  pages = {45--52},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615612150},
  urldate = {2019-09-23},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  langid = {english}
}

@article{Cristea2018,
  title = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant: {{A}} Survey of Top Science Journals},
  shorttitle = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant},
  author = {Cristea, Ioana Alina and Ioannidis, John P. A.},
  year = {2018},
  month = may,
  journal = {PLOS ONE},
  volume = {13},
  number = {5},
  pages = {e0197440},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0197440},
  urldate = {2019-10-01},
  abstract = {P values represent a widely used, but pervasively misunderstood and fiercely contested method of scientific inference. Display items, such as figures and tables, often containing the main results, are an important source of P values. We conducted a survey comparing the overall use of P values and the occurrence of significant P values in display items of a sample of articles in the three top multidisciplinary journals (Nature, Science, PNAS) in 2017 and, respectively, in 1997. We also examined the reporting of multiplicity corrections and its potential influence on the proportion of statistically significant P values. Our findings demonstrated substantial and growing reliance on P values in display items, with increases of 2.5 to 14.5 times in 2017 compared to 1997. The overwhelming majority of P values (94\%, 95\% confidence interval [CI] 92\% to 96\%) were statistically significant. Methods to adjust for multiplicity were almost non-existent in 1997, but reported in many articles relying on P values in 2017 (Nature 68\%, Science 48\%, PNAS 38\%). In their absence, almost all reported P values were statistically significant (98\%, 95\% CI 96\% to 99\%). Conversely, when any multiplicity corrections were described, 88\% (95\% CI 82\% to 93\%) of reported P values were statistically significant. Use of Bayesian methods was scant (2.5\%) and rarely (0.7\%) articles relied exclusively on Bayesian statistics. Overall, wider appreciation of the need for multiplicity corrections is a welcome evolution, but the rapid growth of reliance on P values and implausibly high rates of reported statistical significance are worrisome.},
  langid = {english},
  keywords = {Analysis of variance,Bayesian method,Bayesian statistics,Computer software,Meta-analysis,Scientific publishing,Software tools,Statistical data}
}

@article{OSC2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  urldate = {2019-12-19},
  abstract = {Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 Structured Abstract INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  copyright = {Copyright {\copyright} 2015, American Association for the Advancement of Science},
  langid = {english},
  pmid = {26315443}
}

@article{Wiseman2019,
  title = {Registered Reports: An Early Example and Analysis},
  shorttitle = {Registered Reports},
  author = {Wiseman, Richard and Watt, Caroline and Kornbrot, Diana},
  year = {2019},
  month = jan,
  journal = {PeerJ},
  volume = {7},
  pages = {e6232},
  issn = {2167-8359},
  doi = {10.7717/peerj.6232},
  urldate = {2019-12-28},
  abstract = {The recent `replication crisis' in psychology has focused attention on ways of increasing methodological rigor within the behavioral sciences. Part of this work has involved promoting `Registered Reports', wherein journals peer review papers prior to data collection and publication. Although this approach is usually seen as a relatively recent development, we note that a prototype of this publishing model was initiated in the mid-1970s by parapsychologist Martin Johnson in the European Journal of Parapsychology (EJP). A retrospective and observational comparison of Registered and non-Registered Reports published in the EJP during a seventeen-year period provides circumstantial evidence to suggest that the approach helped to reduce questionable research practices. This paper aims both to bring Johnson's pioneering work to a wider audience, and to investigate the positive role that Registered Reports may play in helping to promote higher methodological and statistical standards.},
  langid = {english},
  keywords = {history of science,meta-science,parapsychology,RRs}
}

@article{Landy2020,
  title = {Crowdsourcing Hypothesis Tests: {{Making}} Transparent How Design Choices Shape Research Results.},
  shorttitle = {Crowdsourcing Hypothesis Tests},
  author = {Landy, Justin F. and Jia, Miaolei (Liam) and Ding, Isabel L. and Viganola, Domenico and Tierney, Warren and Dreber, Anna and Johannesson, Magnus and Pfeiffer, Thomas and Ebersole, Charles R. and Gronau, Quentin F. and Ly, Alexander and {van den Bergh}, Don and Marsman, Maarten and Derks, Koen and Wagenmakers, Eric-Jan and Proctor, Andrew and Bartels, Daniel M. and Bauman, Christopher W. and Brady, William J. and Cheung, Felix and Cimpian, Andrei and Dohle, Simone and Donnellan, M. Brent and Hahn, Adam and Hall, Michael P. and {Jim{\'e}nez-Leal}, William and Johnson, David J. and Lucas, Richard E. and Monin, Beno{\^i}t and Montealegre, Andres and Mullen, Elizabeth and Pang, Jun and Ray, Jennifer and Reinero, Diego A. and Reynolds, Jesse and Sowden, Walter and Storage, Daniel and Su, Runkun and Tworek, Christina M. and Van Bavel, Jay J. and Walco, Daniel and Wills, Julian and Xu, Xiaobing and Yam, Kai Chi and Yang, Xiaoyu and Cunningham, William A. and Schweinsberg, Martin and Urwitz, Molly and Uhlmann, Eric L. and {The Crowdsourcing Hypothesis Tests Collaboration}},
  year = {2020},
  month = jan,
  journal = {Psychological Bulletin},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/bul0000220},
  urldate = {2020-02-24},
  langid = {english}
}

@article{Chambers2015,
  title = {Registered {{Reports}}: {{Realigning}} Incentives in Scientific Publishing},
  author = {Chambers, C. D. and Dienes, Z. and McIntosh, R.D. and Rotshtein, P. and Willmes, K.},
  year = {2015},
  journal = {Cortex},
  volume = {66},
  pages = {1--2},
  issn = {19738102},
  doi = {10.1016/j.cortex.2015.03.022},
  abstract = {This editorial present views on realigning incentives in scientific publishing. As editors recognize this important moment for Cortex, they also take the opportunity to reiterate our view that Registered Reports should not be seen as a one-shot cure for reproducibility problems in science. The applicability of Registered Reports to different sub-fields within neuropsychology and cognitive neuroscience remains to be established; for instance, studies that rely exclusively on exploration rather than deductive hypothesis testing may not be compatible. Registered Reports present no threat to exploratory science in cases where studies include a mixture of both hypothesis testing and exploratory analyses, authors are welcome to report the outcomes of the unregistered analyses, as Sassenhagen and Bornkessel-Schlesewsky do in the current issue. Pre-registration simply allows readers to distinguish the outcomes based on a priori hypothesis testing from post hoc exploration. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  pmid = {25892410}
}

@article{Yang2020a,
  title = {Estimating the Deep Replicability of Scientific Findings Using Human and Artificial Intelligence},
  author = {Yang, Yang and Youyou, Wu and Uzzi, Brian},
  year = {2020},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {20},
  pages = {10762--10768},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1909046117},
  urldate = {2020-09-03},
  langid = {english}
}

@article{Hoogeveen2020,
  title = {Laypeople {{Can Predict Which Social-Science Studies Will Be Replicated Successfully}}:},
  shorttitle = {Laypeople {{Can Predict Which Social-Science Studies Will Be Replicated Successfully}}},
  author = {Hoogeveen, Suzanne and Sarafoglou, Alexandra and Wagenmakers, Eric-Jan},
  year = {2020},
  month = aug,
  journal = {Advances in Methods and Practices in Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  doi = {10.1177/2515245920919667},
  urldate = {2020-09-11},
  abstract = {Large-scale collaborative projects recently demonstrated that several key findings from the social-science literature could not be replicated successfully. Here...},
  copyright = {{\copyright} The Author(s) 2020},
  langid = {english}
}

@article{Altmejd2019,
  title = {Predicting the Replicability of Social Science Lab Experiments},
  author = {Altmejd, Adam and Dreber, Anna and Forsell, Eskil and Huber, Juergen and Imai, Taisuke and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Camerer, Colin},
  editor = {Wicherts, Jelte M.},
  year = {2019},
  month = dec,
  journal = {PLOS ONE},
  volume = {14},
  number = {12},
  pages = {e0225826},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0225826},
  urldate = {2020-09-11},
  langid = {english}
}

@article{Fraser2018,
  title = {Questionable Research Practices in Ecology and Evolution},
  author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
  year = {2018},
  month = jul,
  journal = {PLOS ONE},
  volume = {13},
  number = {7},
  pages = {e0200303},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0200303},
  urldate = {2021-01-21},
  abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
  langid = {english},
  keywords = {Behavioral ecology,Community ecology,Evolutionary biology,Evolutionary ecology,Evolutionary rate,Psychology,Publication ethics,Statistical data}
}

@article{Csada1996,
  title = {The "{{File Drawer Problem}}" of {{Non-Significant Results}}: {{Does It Apply}} to {{Biological Research}}?},
  shorttitle = {The "{{File Drawer Problem}}" of {{Non-Significant Results}}},
  author = {Csada, Ryan D. and James, Paul C. and Espie, Richard H. M.},
  year = {1996},
  journal = {Oikos},
  volume = {76},
  number = {3},
  eprint = {3546355},
  eprinttype = {jstor},
  pages = {591--593},
  publisher = {[Nordic Society Oikos, Wiley]},
  issn = {0030-1299},
  doi = {10.2307/3546355},
  urldate = {2021-03-01},
  abstract = {We show that there appears to be a publication bias against non-significant results in the biological literature. We suggest reasons why non-significant results are not published, the implications of not publishing non-significant results, and why we need to correct the problem.}
}

@article{Makel2021,
  title = {Both {{Questionable}} and {{Open Research Practices Are Prevalent}} in {{Education Research}}:},
  shorttitle = {Both {{Questionable}} and {{Open Research Practices Are Prevalent}} in {{Education Research}}},
  author = {Makel, Matthew C. and Hodges, Jaret and Cook, Bryan G. and Plucker, Jonathan A.},
  year = {2021},
  month = mar,
  journal = {Educational Researcher},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  doi = {10.3102/0013189X211001356},
  urldate = {2021-04-10},
  abstract = {Concerns about the conduct of research are pervasive in many fields, including education. In this preregistered study, we replicated and extended previous studi...},
  copyright = {{\copyright} 2021 AERA},
  langid = {english}
}

@article{Driessen2015,
  title = {Does {{Publication Bias Inflate}} the {{Apparent Efficacy}} of {{Psychological Treatment}} for {{Major Depressive Disorder}}? {{A Systematic Review}} and {{Meta-Analysis}} of {{US National Institutes}} of {{Health-Funded Trials}}},
  shorttitle = {Does {{Publication Bias Inflate}} the {{Apparent Efficacy}} of {{Psychological Treatment}} for {{Major Depressive Disorder}}?},
  author = {Driessen, Ellen and Hollon, Steven D. and Bockting, Claudi L. H. and Cuijpers, Pim and Turner, Erick H.},
  year = {2015},
  month = sep,
  journal = {PLOS ONE},
  volume = {10},
  number = {9},
  pages = {e0137864},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0137864},
  urldate = {2021-04-14},
  abstract = {Background The efficacy of antidepressant medication has been shown empirically to be overestimated due to publication bias, but this has only been inferred statistically with regard to psychological treatment for depression. We assessed directly the extent of study publication bias in trials examining the efficacy of psychological treatment for depression. Methods and Findings We identified US National Institutes of Health grants awarded to fund randomized clinical trials comparing psychological treatment to control conditions or other treatments in patients diagnosed with major depressive disorder for the period 1972--2008, and we determined whether those grants led to publications. For studies that were not published, data were requested from investigators and included in the meta-analyses. Thirteen (23.6\%) of the 55 funded grants that began trials did not result in publications, and two others never started. Among comparisons to control conditions, adding unpublished studies (Hedges' g = 0.20; CI95\% -0.11{\textasciitilde}0.51; k = 6) to published studies (g = 0.52; 0.37{\textasciitilde}0.68; k = 20) reduced the psychotherapy effect size point estimate (g = 0.39; 0.08{\textasciitilde}0.70) by 25\%. Moreover, these findings may overestimate the "true" effect of psychological treatment for depression as outcome reporting bias could not be examined quantitatively. Conclusion The efficacy of psychological interventions for depression has been overestimated in the published literature, just as it has been for pharmacotherapy. Both are efficacious but not to the extent that the published literature would suggest. Funding agencies and journals should archive both original protocols and raw data from treatment trials to allow the detection and correction of outcome reporting bias. Clinicians, guidelines developers, and decision makers should be aware that the published literature overestimates the effects of the predominant treatments for depression.},
  langid = {english},
  keywords = {Antidepressant drug therapy,Antidepressants,Clinical psychology,Depression,Drug therapy,Mental health therapies,Psychotherapy,Publication ethics}
}

@article{Lakens2019b,
  title = {{The value of preregistration for psychological science: A conceptual analysis}},
  shorttitle = {{The value of preregistration for psychological science}},
  author = {Lakens, Dani{\"e}l},
  year = {2019},
  journal = {Japanese Psychological Review},
  volume = {62},
  number = {3},
  pages = {221--230},
  publisher = {},
  issn = {0386-1058, 2433-4650},
  doi = {10.24602/sjpr.62.3_221},
  urldate = {2021-04-18},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish  {\dots}},
  langid = {japanese}
}

@article{Scheel2021a,
  title = {An {{Excess}} of {{Positive Results}}: {{Comparing}} the {{Standard Psychology Literature With Registered Reports}}},
  shorttitle = {An {{Excess}} of {{Positive Results}}},
  author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Dani{\"e}l},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {251524592110074},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/25152459211007467},
  urldate = {2021-04-20},
  abstract = {Selectively publishing results that support the tested hypotheses (``positive'' results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs ( N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature ( N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
  copyright = {All rights reserved},
  langid = {english}
}

@article{Nosek2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2600--2606},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  urldate = {2021-04-25},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes---a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  chapter = {Colloquium Paper},
  copyright = {{\copyright} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {29531091},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration}
}

@article{Kacelnik1997,
  title = {Risk-Sensitivity: Crossroads for Theories of Decision-Making.},
  shorttitle = {Risk-Sensitivity},
  author = {Kacelnik, Alex and Bateson, M.},
  year = {1997},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {1},
  number = {8},
  pages = {304--309},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/s1364-6613(97)01093-0},
  urldate = {2021-04-28},
  abstract = {Europe PMC is an archive of life sciences journal literature., Risk-sensitivity: crossroads for theories of decision-making.},
  langid = {english},
  pmid = {21223933}
}

@article{Maki2006,
  title = {Models Are Experiments, Experiments Are Models},
  author = {M{\"a}ki, Uskali},
  year = {2006},
  month = aug,
  journal = {Journal of Economic Methodology},
  publisher = {Taylor \& Francis Group},
  doi = {10.1080/13501780500086255},
  urldate = {2021-04-29},
  abstract = {(2005). Models are experiments, experiments are models. Journal of Economic Methodology: Vol. 12, No. 2, pp. 303-315.},
  copyright = {Copyright Taylor and Francis Group, LLC},
  langid = {english}
}

@article{Winterhalder1999,
  title = {Risk-Senstive Adaptive Tactics: {{Models}} and Evidence from Subsistence Studies in Biology and Anthropology},
  shorttitle = {Risk-Senstive Adaptive Tactics},
  author = {Winterhalder, Bruce and Lu, Flora and Tucker, Bram},
  year = {1999},
  month = dec,
  journal = {Journal of Archaeological Research},
  volume = {7},
  number = {4},
  pages = {301--348},
  issn = {1573-7756},
  doi = {10.1007/BF02446047},
  urldate = {2021-04-29},
  abstract = {Risk-sensitive analysis of subsistence adaptations is warranted when (i) outcomes are to some degree unpredictable and (ii) they have nonlinear consequences for fitness and/or utility. Both conditions are likely to be common among peoples studied by ecologicll anthropologists and archaeologists. We develop a general conceptual model of risk. We then review and summarize the extensive empirical literatures from biology and anthropology for methodological insights and for their comparative potential. Risk-sensitive adaptive tactics are diverse and they are taxonomically widespread. However, the anthropological literature rarely makes use of formal models of risk-sensitive adaptation, while the biological literature lacks naturalistic observations of risk-sensitive behavior. Both anthropology and biology could benefit from greater interdisciplinary exchange.},
  langid = {english}
}

@article{Stewart2021,
  title = {The Natural Selection of Good Science},
  author = {Stewart, Alexander J. and Plotkin, Joshua B.},
  year = {2021},
  month = may,
  journal = {Nature Human Behaviour},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01111-x},
  urldate = {2021-05-19},
  abstract = {Scientists in some fields are concerned that many published results are false. Recent models predict selection for false positives as the inevitable result of pressure to publish, even when scientists are penalized for publications that fail to replicate. We model the cultural evolution of research practices when laboratories are allowed to expend effort on theory, enabling them, at a cost, to identify hypotheses that are more likely to be true, before empirical testing. Theory can restore high effort in research practice and suppress false positives to a technical minimum, even without replication. The mere ability to choose between two sets of hypotheses, one with greater prior chance of being correct, promotes better science than can be achieved with effortless access to the set of stronger hypotheses. Combining theory and replication can have synergistic effects. On the basis of our analysis, we propose four simple recommendations to promote good science.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english}
}

@article{Zollman2009,
  title = {Optimal {{Publishing Strategies}}},
  author = {Zollman, Kevin J. S.},
  year = {2009},
  month = jun,
  journal = {Episteme},
  volume = {6},
  number = {2},
  pages = {185--199},
  issn = {1742-3600, 1750-0117},
  doi = {10.3366/E174236000900063X},
  urldate = {2021-06-14},
  abstract = {Journals regulate a significant portion of the communication between scientists. This paper devises an agent-based model of scientific practice and uses it to compare various strategies for selecting publications by journals. Surprisingly, it appears that the best selection method for journals is to publish relatively few papers and to select those papers it publishes at random from the available ``above threshold'' papers it receives. This strategy is most effective at maintaining an appropriate type of diversity that is needed to solve a particular type of scientific problem. This problem and the limitation of the model is discussed in detail.},
  langid = {english}
}

@article{Tiokhin2021,
  title = {Competition for Priority Harms the Reliability of Science, but Reforms Can Help},
  author = {Tiokhin, Leonid and Yan, Minhua and Morgan, Thomas J. H.},
  year = {2021},
  month = jul,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {7},
  pages = {857--867},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-01040-1},
  urldate = {2021-07-28},
  abstract = {Incentives for priority of discovery are hypothesized to harm scientific reliability. Here, we evaluate this hypothesis by developing an evolutionary agent-based model of a competitive scientific process. We find that rewarding priority of discovery causes populations to culturally evolve towards conducting research with smaller samples. This reduces research reliability and the information value of the average study. Increased start-up costs for setting up single studies and increased payoffs for secondary results (also known as scoop protection) attenuate the negative effects of competition. Furthermore, large rewards for negative results promote the evolution of smaller sample sizes. Our results confirm the logical coherence of scoop protection reforms at several journals. Our results also imply that reforms to increase scientific efficiency, such as rapid journal turnaround times, may produce collateral damage by incentivizing lower-quality research; in contrast, reforms that increase start-up costs, such as pre-registration and registered reports, may generate incentives for higher-quality research.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Economics;Human behaviour\\
Subject\_term\_id: economics;human-behaviour}
}

@article{Roos2010,
  title = {Decision Making under Risk in {{Deal}} or {{No Deal}}},
  author = {de Roos, Nicolas and Sarafidis, Yianis},
  year = {2010},
  journal = {Journal of Applied Econometrics},
  volume = {25},
  number = {6},
  pages = {987--1027},
  issn = {1099-1255},
  doi = {10.1002/jae.1110},
  urldate = {2021-08-07},
  abstract = {We analyse the choices of 399 contestants in the Australian version of the television game show Deal or No Deal. We calculate risk aversion bounds for each contestant, revealing considerable heterogeneity. We then estimate a structural stochastic choice model that captures the dynamic decision problem faced by contestants. To address individual heterogeneity, we nest the dynamic problem within the settings of both a random effects and a random coefficients probit model. Our structural model produces plausible estimates of risk aversion, confirms the role of individual heterogeneity and suggests that a model of stochastic choice is indeed appropriate. We find mixed evidence of greater risk aversion by females. We also examine generalizations to expected utility theory, finding that the rank-dependent utility model adds non-negligible explanatory power and indicates optimism in probability weighting. Finally, we test, but are unable to confirm, the existence of an endowment effect for lotteries. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  langid = {english}
}

@article{Mishra2014,
  title = {Decision-{{Making Under Risk}}: {{Integrating Perspectives From Biology}}, {{Economics}}, and {{Psychology}}},
  shorttitle = {Decision-{{Making Under Risk}}},
  author = {Mishra, Sandeep},
  year = {2014},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {18},
  number = {3},
  pages = {280--307},
  issn = {1088-8683, 1532-7957},
  doi = {10.1177/1088868314530517},
  urldate = {2021-08-25},
  abstract = {Decision-making under risk has been variably characterized and examined in many different disciplines. However, interdisciplinary integration has not been forthcoming. Classic theories of decision-making have not been amply revised in light of greater empirical data on actual patterns of decision-making behavior. Furthermore, the meta-theoretical framework of evolution by natural selection has been largely ignored in theories of decision-making under risk in the human behavioral sciences. In this review, I critically examine four of the most influential theories of decision-making from economics, psychology, and biology: expected utility theory, prospect theory, risk-sensitivity theory, and heuristic approaches. I focus especially on risk-sensitivity theory, which offers a framework for understanding decision-making under risk that explicitly involves evolutionary considerations. I also review robust empirical evidence for individual differences and environmental/ situational factors that predict actual risky decision-making that any general theory must account for. Finally, I offer steps toward integrating various theoretical perspectives and empirical findings on risky decision-making.},
  langid = {english}
}

@article{McElreath2015,
  title = {Replication, {{Communication}}, and the {{Population Dynamics}} of {{Scientific Discovery}}},
  author = {McElreath, Richard and Smaldino, Paul E.},
  year = {2015},
  month = aug,
  journal = {PLOS ONE},
  volume = {10},
  number = {8},
  pages = {e0136088},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0136088},
  urldate = {2021-09-20},
  abstract = {Many published research results are false (Ioannidis, 2005), and controversy continues over the roles of replication and publication policy in improving the reliability of research. Addressing these problems is frustrated by the lack of a formal framework that jointly represents hypothesis formation, replication, publication bias, and variation in research quality. We develop a mathematical model of scientific discovery that combines all of these elements. This model provides both a dynamic model of research as well as a formal framework for reasoning about the normative structure of science. We show that replication may serve as a ratchet that gradually separates true hypotheses from false, but the same factors that make initial findings unreliable also make replications unreliable. The most important factors in improving the reliability of research are the rate of false positives and the base rate of true hypotheses, and we offer suggestions for addressing each. Our results also bring clarity to verbal debates about the communication of research. Surprisingly, publication bias is not always an obstacle, but instead may have positive impacts---suppression of negative novel findings is often beneficial. We also find that communication of negative replications may aid true discovery even when attempts to replicate have diminished power. The model speaks constructively to ongoing debates about the design and conduct of science, focusing analysis and discussion on precise, internally consistent models, as well as highlighting the importance of population dynamics.},
  langid = {english},
  keywords = {Chemical elements,Mathematical models,Pigments,Population dynamics,Publication ethics,Research quality assessment,Research validity,Scientists}
}

@article{Haaland2019,
  title = {Bet-Hedging across Generations Can Affect the Evolution of Variance-Sensitive Strategies within Generations},
  author = {Haaland, Thomas R. and Wright, Jonathan and Ratikainen, Irja I.},
  year = {2019},
  month = dec,
  journal = {Proceedings of the Royal Society B},
  publisher = {The Royal Society},
  doi = {10.1098/rspb.2019.2070},
  urldate = {2021-10-08},
  abstract = {In order to understand how organisms cope with ongoing changes in environmental variability, it is necessary to consider multiple adaptations to environmental uncertainty on different time scales. Conservative bet-hedging (CBH) represents a long-term ...},
  copyright = {{\copyright} 2019 The Authors.},
  langid = {english}
}

@article{Liner2009,
  title = {Research Requirements for Promotion and Tenure at {{PhD}} Granting Departments of Economics},
  author = {Liner, Gaines H. and Sewell, Ellen},
  year = {2009},
  month = may,
  journal = {Applied Economics Letters},
  publisher = {Taylor \& Francis},
  doi = {10.1080/13504850701221998},
  urldate = {2021-10-11},
  abstract = {(2009). Research requirements for promotion and tenure at PhD granting departments of economics. Applied Economics Letters: Vol. 16, No. 8, pp. 765-768.},
  copyright = {Copyright Taylor and Francis Group, LLC},
  langid = {english}
}

@article{Yoshimura1991,
  title = {Individual Adaptations in Stochastic Environments},
  author = {Yoshimura, Jin and Clark, Colin W.},
  year = {1991},
  month = apr,
  journal = {Evolutionary Ecology},
  volume = {5},
  number = {2},
  pages = {173--192},
  issn = {1573-8477},
  doi = {10.1007/BF02270833},
  urldate = {2021-10-16},
  abstract = {Many natural populations undergo radical and unpredictable fluctuations, associated with stochastic environmental conditions. Under such circumstances, fitness of a genotype (or `strategy') is defined as the geometric mean of the intergenerational genotypic population growth ratel(t). Unfortunately, this population-level criterion has proved difficult to apply at the level of individual organisms.},
  langid = {english}
}

@article{Soderberg2021,
  title = {Initial Evidence of Research Quality of Registered Reports Compared with the Standard Publishing Model},
  author = {Soderberg, Courtney K. and Errington, Timothy M. and Schiavone, Sarah R. and Bottesini, Julia and Thorn, Felix Singleton and Vazire, Simine and Esterling, Kevin M. and Nosek, Brian A.},
  year = {2021},
  month = aug,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {8},
  pages = {990--997},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01142-4},
  urldate = {2021-10-27},
  abstract = {In registered reports (RRs), initial peer review and in-principle acceptance occur before knowing the research outcomes. This combats publication bias and distinguishes planned from unplanned research. How RRs could improve the credibility of research findings is straightforward, but there is little empirical evidence. Also, there could be unintended costs such as reducing novelty. Here, 353 researchers peer reviewed a pair of papers from 29 published RRs from psychology and neuroscience and 57 non-RR comparison papers. RRs numerically outperformed comparison papers on all 19 criteria (mean difference 0.46, scale range -4 to +4) with effects ranging from RRs being statistically indistinguishable from comparison papers in novelty (0.13, 95\% credible interval [-0.24, 0.49]) and creativity (0.22, [-0.14, 0.58]) to sizeable improvements in rigour of methodology (0.99, [0.62, 1.35]) and analysis (0.97, [0.60, 1.34]) and overall paper quality (0.66, [0.30, 1.02]). RRs could improve research quality while reducing publication bias and ultimately improve the credibility of the published literature.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Psychology,Publishing},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Psychology;Publishing\\
Subject\_term\_id: psychology;publishing}
}

@article{Young2020,
  title = {Theory and Measurement of Environmental Unpredictability},
  author = {Young, Ethan S. and Frankenhuis, Willem E. and Ellis, Bruce J.},
  year = {2020},
  month = nov,
  journal = {Evolution and Human Behavior},
  series = {Current Debates in Human Life History Research},
  volume = {41},
  number = {6},
  pages = {550--556},
  issn = {1090-5138},
  doi = {10.1016/j.evolhumbehav.2020.08.006},
  urldate = {2021-11-28},
  abstract = {Over the past decade, there is increasing interest in the ways in which environmental unpredictability shapes human life history development. However, progress is hindered by two theoretical ambiguities. The first is that conceptual definitions of environmental unpredictability are not precise enough to be able to express them in statistical terms. The second is that there are different implicit hypotheses about the proximate mechanisms that detect unpredictability, which have not been explicitly described and compared. The first is the ancestral cue perspective, which proposes that humans evolved to detect cues (e.g., loss of a parent, residential changes) that indicated high environmental unpredictability across evolutionary history. The second is the statistical learning perspective, which proposes that organisms estimate the level of unpredictability from lived experiences across development (e.g., prediction errors encountered through time). In this paper, we address both sources of ambiguity. First, we describe the possible statistical properties of unpredictability. Second, we outline the ancestral cue and statistical learning perspectives and their implications for the measurement of environmental unpredictability. Our goal is to provide concrete steps toward better conceptualization and measurement of environmental unpredictability from both approaches. Doing so will refine our understanding of environmental unpredictability and its connection to life history development.},
  langid = {english},
  keywords = {Ancestral cues,Environmental unpredictability,Life history development,Statistical learning}
}

@article{Chambers2021,
  title = {The Past, Present and Future of {{Registered Reports}}},
  author = {Chambers, C. D. and Tzavella, Loukia},
  year = {2021},
  month = nov,
  journal = {Nature Human Behaviour},
  pages = {1--14},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01193-7},
  urldate = {2021-11-29},
  abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Culture,Publishing},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Reviews\\
Subject\_term: Culture;Publishing\\
Subject\_term\_id: culture;publishing}
}

@article{Nosek2022,
  title = {Replicability, {{Robustness}}, and {{Reproducibility}} in {{Psychological Science}}},
  author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aur{\'e}lien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Struhl, Melissa Kline and Nuijten, Mich{\`e}le B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Sch{\"o}nbrodt, Felix D. and Vazire, Simine},
  year = {2022},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {73},
  number = {1},
  pages = {annurev-psych-020821-114157},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-020821-114157},
  urldate = {2021-11-30},
  abstract = {Replication---an important, uncommon, and misunderstood practice---is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.             Expected final online publication date for the Annual Review of Psychology, Volume 73 is January 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  copyright = {All rights reserved},
  langid = {english}
}

@article{Dickersin1993,
  title = {Publication Bias: The Problem That Won't Go Away},
  shorttitle = {Publication Bias},
  author = {Dickersin, K. and Min, Y. I.},
  year = {1993},
  month = dec,
  journal = {Annals of the New York Academy of Sciences},
  volume = {703},
  pages = {135-146; discussion 146-148},
  issn = {0077-8923},
  doi = {10.1111/j.1749-6632.1993.tb26343.x},
  abstract = {Conclusions about the efficacy and safety of medical interventions are based on data presented in the scientific literature. The validity of these conclusions is threatened if publication bias results from investigators or editors making decisions about publishing study results on the basis of the direction or strength of the study findings. This paper reports meta-analyses performed using data from four prospective investigations in which a total of 997 initiated studies were followed to learn of study results, publication status, and reasons for nonpublication. The analysis indicates that there is a positive association between "significant" study results and publication (OR = 2.88; 95\% confidence interval [CI] 2.13 to 3.90). When the analysis was restricted to controlled trials (n = 280), an even stronger relationship between "significant" results and publication was observed (OR = 6.15; 95\% CI 2.24 to 16.92), with randomized trials (n = 200) apparently no less susceptible to publication bias than controlled trials in general (OR = 8.72; 95\% CI 1.91 to 39.81). In every case, failure to publish was investigator-based, and not due to editorial decisions. The results of clinical trials should not be suppressed in this way. Development of registration systems for randomized trials is essential if this problem is to be minimized in future.},
  langid = {english},
  pmid = {8192291},
  keywords = {Bias,Clinical Trials as Topic,Confidence Intervals,Female,Humans,Logistic Models,Male,Odds Ratio,Professional Staff Committees,Prospective Studies,Publishing,Registries,Reproducibility of Results,Research Design,Research Support as Topic}
}

@article{Gross2021,
  title = {Why Ex Post Peer Review Encourages High-Risk Research While Ex Ante Review Discourages It},
  author = {Gross, Kevin and Bergstrom, Carl T.},
  year = {2021},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {51},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2111615118},
  urldate = {2022-01-18},
  abstract = {Peer review is an integral component of contemporary science. While peer review focuses attention on promising and interesting science, it also encourages scientists to pursue some questions at the expense of others. Here, we use ideas from forecasting assessment to examine how two modes of peer review---ex ante review of proposals for future work and ex post review of completed science---motivate scientists to favor some questions instead of others. Our main result is that ex ante and ex post peer review push investigators toward distinct sets of scientific questions. This tension arises because ex post review allows investigators to leverage their own scientific beliefs to generate results that others will find surprising, whereas ex ante review does not. Moreover, ex ante review will favor different research questions depending on whether reviewers rank proposals in anticipation of changes to their own personal beliefs or to the beliefs of their peers. The tension between ex ante and ex post review puts investigators in a bind because most researchers need to find projects that will survive both. By unpacking the tension between these two modes of review, we can understand how they shape the landscape of science and how changes to peer review might shift scientific activity in unforeseen directions.},
  chapter = {Social Sciences},
  copyright = {{\copyright} 2021 . https://www-pnas-org.vu-nl.idm.oclc.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {34921115},
  keywords = {Bayesian reasoning,decision theory,information theory,peer review,philosophy of science}
}

@article{Goldman1991,
  title = {An Economic Model of Scientific Activity and Truth Acquisition},
  author = {Goldman, Alvin I. and Shaked, Moshe},
  year = {1991},
  month = jul,
  journal = {Philosophical Studies},
  volume = {63},
  number = {1},
  pages = {31--55},
  issn = {0031-8116, 1573-0883},
  doi = {10.1007/BF00375996},
  urldate = {2022-01-26},
  langid = {english}
}

@article{Muller2017,
  title = {Thinking with Indicators. {{Exploring}} the Epistemic Impacts of Academic Performance Indicators in the Life Sciences},
  author = {M{\"u}ller, Ruth and {de Rijcke}, Sarah},
  year = {2017},
  month = jul,
  journal = {Research Evaluation},
  volume = {26},
  number = {3},
  pages = {157--168},
  issn = {0958-2029},
  doi = {10.1093/reseval/rvx023},
  urldate = {2022-02-14},
  abstract = {While quantitative performance indicators are widely used by organizations and individuals for evaluative purposes, little is known about their impacts on the epistemic processes of academic knowledge production. In this article we bring together three qualitative research projects undertaken in the Netherlands and Austria to contribute to filling this gap. The projects explored the role of performance metrics in the life sciences, and the interactions between institutional and disciplinary cultures of evaluating research in these fields. Our analytic perspective is focused on understanding how researchers themselves give value to research, and in how far these practices are related to performance metrics. The article zooms in on three key moments in research processes to show how `thinking with indicators' is becoming a central aspect of research activities themselves: (1) the planning and conception of research projects, (2) the social organization of research processes, and (3) determining the endpoints of research processes. Our findings demonstrate how the worth of research activities becomes increasingly assessed and defined by their potential to yield high value in quantitative terms. The analysis makes visible how certain norms and values related to performance metrics are stabilized as they become integrated into routine practices of knowledge production. Other norms and criteria for scientific quality, e.g. epistemic originality, long-term scientific progress, societal relevance, and social responsibility, receive less attention or become redefined through their relations to quantitative indicators. We understand this trend to be in tension with policy goals that seek to encourage innovative, societally relevant, and responsible research.}
}

@article{Butler2007,
  title = {Assessing University Research: {{A}} Plea for a Balanced Approach},
  shorttitle = {Assessing University Research},
  author = {Butler, Linda},
  year = {2007},
  month = oct,
  journal = {Science and Public Policy},
  volume = {34},
  number = {8},
  pages = {565--574},
  publisher = {Oxford Academic},
  issn = {0302-3427},
  doi = {10.3152/030234207X254404},
  urldate = {2022-02-14},
  abstract = {Abstract. The use of quantitative performance measures to assess the quality of university research is being introduced in Australia and the UK. This paper pres},
  langid = {english}
}

@article{Laudel2014,
  title = {Beyond Breakthrough Research: {{Epistemic}} Properties of Research and Their Consequences for Research Funding},
  shorttitle = {Beyond Breakthrough Research},
  author = {Laudel, Grit and Gl{\"a}ser, Jochen},
  year = {2014},
  month = sep,
  journal = {Research Policy},
  volume = {43},
  number = {7},
  pages = {1204--1216},
  issn = {0048-7333},
  doi = {10.1016/j.respol.2014.02.006},
  urldate = {2022-02-14},
  abstract = {The aim of this paper is to initiate a discussion about links between epistemic properties and institutional conditions for research by providing an exploratory analysis of such links featured by projects funded by the European Research Council (ERC). Our analysis identifies epistemic properties of research processes and links them to necessary and favourable conditions for research, and through these to institutional conditions provided by grants. Our findings enable the conclusion that there is research that is important for the progress of a field but is difficult to fund with common project grants. The predominance and standardisation of grant funding, which can be observed about many European countries, appears to reduce the chances of unconventional projects across all disciplines. Funding programmes of the `ERC-type' (featuring large and flexible budgets, long time horizons, and risk-tolerant selection processes) constitute an institutional innovation because they enable such research. However, while the ERC funding and other new funding schemes for exceptional research attempt to cover these requirements, they are unlikely to suffice.},
  langid = {english},
  keywords = {Epistemic properties of research,High risk -- high reward research,Intellectual innovation,Research funding}
}

@article{vanDalen2012,
  title = {Intended and Unintended Consequences of a Publish-or-Perish Culture: {{A}} Worldwide Survey},
  shorttitle = {Intended and Unintended Consequences of a Publish-or-Perish Culture},
  author = {{van Dalen}, Hendrik P. and Henkens, K{\`e}ne},
  year = {2012},
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {63},
  number = {7},
  pages = {1282--1293},
  issn = {1532-2890},
  doi = {10.1002/asi.22636},
  urldate = {2022-02-14},
  abstract = {How does publication pressure in modern-day universities affect the intrinsic and extrinsic rewards in science? By using a worldwide survey among demographers in developed and developing countries, the authors show that the large majority perceive the publication pressure as high, but more so in Anglo-Saxon countries and to a lesser extent in Western Europe. However, scholars see both the pros (upward mobility) and cons (excessive publication and uncitedness, neglect of policy issues, etc.) of the so-called publish-or-perish culture. By measuring behavior in terms of reading and publishing, and perceived extrinsic rewards and stated intrinsic rewards of practicing science, it turns out that publication pressure negatively affects the orientation of demographers towards policy and knowledge sharing. There are no signs that the pressure affects reading and publishing outside the core discipline.},
  langid = {english},
  keywords = {bibliometrics,scientists,surveys}
}

@article{Muller2014,
  title = {Postdoctoral {{Life Scientists}} and {{Supervision Work}} in the {{Contemporary University}}: {{A Case Study}} of {{Changes}} in the {{Cultural Norms}} of {{Science}}},
  shorttitle = {Postdoctoral {{Life Scientists}} and {{Supervision Work}} in the {{Contemporary University}}},
  author = {M{\"u}ller, Ruth},
  year = {2014},
  month = sep,
  journal = {Minerva},
  volume = {52},
  number = {3},
  pages = {329--349},
  issn = {1573-1871},
  doi = {10.1007/s11024-014-9257-y},
  urldate = {2022-02-14},
  abstract = {This paper explores the ways in which postdoctoral life scientists engage in supervision work in academic institutions in Austria. Reward systems and career conditions in academic institutions in most European and other OECD countries have changed significantly during the last two decades. While an increasing focus is put on evaluating research performances, little reward is attached to excellent performances in mentoring and advising students. Postdoctoral scientists mostly inhabit fragile institutional positions and experience harsh competition, as the number of available senior positions is small compared to that of young scientists striving for an academic career. To prevail in this competition, publications and mobility are key. Educational work is rarely rewarded. Nevertheless, postdocs play a key role in educating PhD students, as overburdened senior scientists often pass on practical supervision duties to their postdoctoral fellows. This paper shows how under these conditions, postdocs reframe the students they supervise as potential resources for co-authored publications. What might look like a mutually beneficial solution at a first glance, in practice implies the subordination of the values of education to the logic of production, which marginalizes spaces primarily devoted to education. The author argues that conflicts like this are indicative of broader changes in the cultural norms of science and academic citizenship, rendering community-oriented tasks such as education work less attractive to academic scientists. Since education and supervision work are central cornerstones of any functioning higher education and research system, this could have negative repercussions for the long-term development of academic institutions.},
  langid = {english}
}

@incollection{Dickersin2005,
  title = {Publication {{Bias}}: {{Recognizing}} the {{Problem}}, {{Understanding Its Origins}} and {{Scope}}, and {{Preventing Harm}}},
  shorttitle = {Publication {{Bias}}},
  booktitle = {Publication {{Bias}} in {{Meta-Analysis}}},
  author = {Dickersin, Kay},
  year = {2005},
  pages = {9--33},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/0470870168.ch2},
  urldate = {2022-02-27},
  abstract = {This chapter contains sections titled: Background Early Evidence of Publication Bias Is Publication Bias Important? Improved Understanding of the Process of Research Dissemination and Publication Other Factors Associated with Failure to Publish and Direction of Results Biased Reporting of Outcomes Selective Citation of Positive Results Solutions to Publication Bias Conclusions Acknowledgements References},
  chapter = {2},
  isbn = {978-0-470-87016-7},
  langid = {english},
  keywords = {biased reporting of outcomes,from study start to dissemination of results,JAMA - Negative Results,publication bias and recognition,publication bias in meta-analysis,publication bias significance,systematic review problem method,systematic reviews and meta-analysis}
}

@article{Hurly2003,
  title = {The Twin Threshold Model: Risk-Intermediate Foraging by Rufous Hummingbirds, {{Selasphorus}} Rufus},
  shorttitle = {The Twin Threshold Model},
  author = {Hurly, Andrew T.},
  year = {2003},
  month = oct,
  journal = {Animal Behaviour},
  volume = {66},
  number = {4},
  pages = {751--761},
  issn = {0003-3472},
  doi = {10.1006/anbe.2003.2278},
  urldate = {2022-03-05},
  abstract = {I developed two versions of the twin threshold model (TTM) to assess risk-sensitive foraging decisions by rufous hummingbirds. The model incorporates energy thresholds for both starvation and reproduction and assesses how three reward distributions with a common mean but different levels of variance interact with these critical thresholds to determine fitness. Fitness, a combination of survival and reproduction, is influenced by both the amount of variance in the distributions and the relative position of the common mean between the thresholds. The model predicts that risk-intermediate foraging is often the optimal policy, and that risk aversion is favoured as the common mean of the distributions approaches the starvation threshold, whereas risk preference is favoured as the common mean approaches the reproduction threshold. Tests with free-living hummingbirds supported these predictions. Hummingbirds were presented with three distributions of nectar rewards that had a common mean but Nil, Moderate or High levels of variance. Birds preferred intermediate levels of variance (Moderate) when presented with all three rewards simultaneously, and became more risk-averse as the mean of the distributions was decreased but more risk-prone as the mean was increased. Birds preferred Nil when it was paired with Moderate or with High, but preferred Moderate in the presence of Nil and High together. This reversal of preference is a violation of regularity, conventionally interpreted as irrational choice behaviour. I provide an alternative version of the TTM demonstrating that violations of regularity can occur when relative instead of absolute evaluation mechanisms are used.},
  langid = {english}
}

@article{Tiokhin2021a,
  title = {Honest Signaling in Academic Publishing},
  author = {Tiokhin, Leonid and Panchanathan, Karthik and Lakens, Daniel and Vazire, Simine and Morgan, Thomas and Zollman, Kevin},
  year = {2021},
  month = feb,
  journal = {PLOS ONE},
  volume = {16},
  number = {2},
  pages = {e0246675},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0246675},
  urldate = {2022-03-07},
  abstract = {Academic journals provide a key quality-control mechanism in science. Yet, information asymmetries and conflicts of interests incentivize scientists to deceive journals about the quality of their research. How can honesty be ensured, despite incentives for deception? Here, we address this question by applying the theory of honest signaling to the publication process. Our models demonstrate that several mechanisms can ensure honest journal submission, including differential benefits, differential costs, and costs to resubmitting rejected papers. Without submission costs, scientists benefit from submitting all papers to high-ranking journals, unless papers can only be submitted a limited number of times. Counterintuitively, our analysis implies that inefficiencies in academic publishing (e.g., arbitrary formatting requirements, long review times) can serve a function by disincentivizing scientists from submitting low-quality work to high-ranking journals. Our models provide simple, powerful tools for understanding how to promote honest paper submission in academic publishing.},
  langid = {english},
  keywords = {Asymmetric information,Communications,Conflicts of interest,Deception,Peer review,Research quality assessment,Scientific publishing,Scientists}
}

@article{Mishra2010,
  title = {You Can't Always Get What You Want: {{The}} Motivational Effect of Need on Risk-Sensitive Decision-Making},
  shorttitle = {You Can't Always Get What You Want},
  author = {Mishra, Sandeep and Lalumi{\`e}re, Martin L.},
  year = {2010},
  month = jul,
  journal = {Journal of Experimental Social Psychology},
  volume = {46},
  number = {4},
  pages = {605--611},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2009.12.009},
  urldate = {2022-03-09},
  abstract = {Risky behavior in humans is typically considered irrational, reckless, and maladaptive. Risk-sensitivity theory, however, suggests that risky behavior may be adaptive in some circumstances: decision-makers should prefer high-risk options in situations of high need, when lower risk options are unlikely to meet those needs. This pattern of decision-making has been well established in the non-human animal literature, but little research has been conducted on humans. We demonstrate in a two-part experimental study that young men and women (n=115) behave as predicted by risk-sensitivity theory, shifting from risk-aversion to risk-proneness in situations of high need. This shift occurred whether decisions were made from description or from experience, and was observed controlling for sex and individual differences in general risk-taking propensity. This study is the first ecologically-relevant demonstration of risk-sensitive decision-making in humans.},
  langid = {english},
  keywords = {Decision-making,Ecological rationality,Individual differences,Need,Personality,Risk,Risk-sensitivity,Sex differences}
}

@article{OConnor2019,
  title = {The Natural Selection of Conservative Science},
  author = {O'Connor, Cailin},
  year = {2019},
  month = aug,
  journal = {Studies in History and Philosophy of Science Part A},
  volume = {76},
  pages = {24--29},
  issn = {0039-3681},
  doi = {10.1016/j.shpsa.2018.09.007},
  urldate = {2022-03-11},
  abstract = {Social epistemologists have argued that high risk, high reward science has an important role to play in scientific communities. Recently, though, it has also been argued that various scientific fields seem to be trending towards conservatism---the increasing production of what Kuhn (1962) might have called `normal science'. This paper will explore a possible explanation for this sort of trend: that the process by which scientific research groups form, grow, and dissolve might be inherently hostile to such science. In particular, I employ a paradigm developed by Smaldino and McElreath (2016) that treats a scientific community as a population undergoing selection. As will become clear, perhaps counter-intuitively this sort of process in some ways promotes high risk, high reward science. But, as I will point out, risky science is, in general, the sort of thing that is hard to repeat. While more conservative scientists will be able to train students capable of continuing their successful projects, and so create thriving lineages, successful risky science may not be the sort of thing one can easily pass on. In such cases, the structure of scientific communities selects against high risk, high rewards projects. More generally, this project makes clear that there are at least two processes to consider in thinking about how incentives shape scientific communities---the process by which individual scientists make choices about their careers and research, and the selective process governing the formation of new research groups.},
  langid = {english}
}

@book{R-bookdown,
  title = {Bookdown: {{Authoring}} Books and Technical Documents with {{R}} Markdown},
  author = {Xie, Yihui},
  year = {2016},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, Florida}
}

@book{R-base,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2019},
  address = {Vienna, Austria},
  organization = {R Foundation for Statistical Computing}
}

@book{R-papaja,
  title = {{{papaja}}: {{Create APA}} Manuscripts with {{R Markdown}}},
  author = {Aust, Frederik and Barth, Marius},
  year = {2018}
}

@book{R-ggplot2,
  title = {Ggplot2: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {Springer-Verlag New York},
  isbn = {978-3-319-24277-4}
}

@book{R-rmarkdown,
  title = {R Markdown: {{The}} Definitive Guide},
  author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, Florida}
}

@book{R-knitr,
  title = {Dynamic Documents with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2015},
  edition = {2nd},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, Florida}
}

@book{R-here,
  title = {Here: {{A}} Simpler Way to Find Your Files},
  author = {M{\"u}ller, Kirill},
  year = {2017}
}

@book{RStudioTeam2019,
  title = {{{RStudio}}: {{Integrated}} Development Environment for r},
  author = {{RStudio Team}},
  year = {2019},
  address = {Boston, MA},
  organization = {RStudio, Inc.}
}

@article{Barclay2018,
  title = {State-Dependent Risk-Taking},
  author = {Barclay, Pat and Mishra, Sandeep and Sparks, Adam Maxwell},
  year = {2018},
  month = jun,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {285},
  number = {1881},
  pages = {20180180},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2018.0180},
  urldate = {2022-03-23},
  abstract = {Who takes risks, and when? The               relative state model               proposes two non-independent selection pressures governing risk-taking: need-based and ability-based. The need-based account suggests that actors take risks when they cannot reach target states with low-risk options (consistent with risk-sensitivity theory). The ability-based account suggests that actors engage in risk-taking when they possess traits or abilities that increase the expected value of risk-taking (by increasing the probability of success, enhancing payoffs for success or buffering against failure). Adaptive risk-taking involves integrating both considerations. Risk-takers compute the expected value of risk-taking based on their               state               ---the interaction of embodied capital relative to one's situation, to the same individual in other circumstances or to other individuals. We provide mathematical support for this dual pathway model, and show that it can predict who will take the most risks and when (e.g. when risk-taking will be performed by those in good, poor, intermediate or extreme state only). Results confirm and elaborate on the initial verbal model of state-dependent risk-taking: selection favours agents who calibrate risk-taking based on implicit computations of condition and/or competitive (dis)advantage, which in turn drives patterned individual differences in risk-taking behaviour.},
  langid = {english}
}

@article{John2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  journal = {Psychological Science},
  volume = {23},
  number = {5},
  pages = {524--532},
  issn = {14679280},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  pmid = {22508865},
  keywords = {disclosure,judgment,methodology,professional standards}
}

@article{Simmons2011,
  title = {False-Positive Psychology: {{Undisclosed}} Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {14679280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  pmid = {22006061},
  keywords = {disclosure,methodology,motivated reasoning,publication}
}

@article{Ferguson2012,
  title = {A {{Vast Graveyard}} of {{Undead Theories}}: {{Publication Bias}} and {{Psychological Science}}'s {{Aversion}} to the {{Null}}},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  year = {2012},
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {555--561},
  issn = {17456916},
  doi = {10.1177/1745691612459059},
  abstract = {Publication bias remains a controversial issue in psychological science. The tendency of psychological science to avoid publishing null results produces a situation that limits the replicability assumption of science, as replication cannot be meaningful without the potential acknowledgment of failed replications. We argue that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science's capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous ``undead'' theories that are ideologically popular but have little basis in fact.},
  pmid = {26168112},
  keywords = {fail-safe number,falsification,meta-analyses,null hypothesis significance testing,publication bias}
}

@article{Kerr1998,
  title = {{{HARKing}}: {{Hypothesizing}} after the Results Are Known},
  author = {Kerr, Norbert L.},
  year = {1998},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  issn = {10888683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  pmid = {15647155}
}

@article{Rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  issn = {00332909},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any gien research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type 1 errors, while the file drawers are filled with the 95\% of the studies that show non-significant resluts. Quantitative procedures for computing the tolerance for filed and future results are reported and illustrated, and the implications are discussed.},
  pmid = {53},
  keywords = {tolerance for null results bias in publication of}
}

@article{Fanelli2010,
  title = {"{{Positive}}" Results Increase down the Hierarchy of the Sciences},
  author = {Fanelli, Daniele},
  editor = {Scalas, Enrico},
  year = {2010},
  month = apr,
  journal = {PLoS ONE},
  volume = {5},
  number = {4},
  pages = {e10068},
  issn = {19326203},
  doi = {10.1371/journal.pone.0010068},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the "hardness" of scientific research--i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors--is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a "positive" (full or partial) or "negative" support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in "softer" sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  pmid = {20383332}
}

@article{Hardwicke2018,
  title = {Mapping the Universe of Registered Reports},
  author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
  year = {2018},
  month = oct,
  journal = {Nature Human Behaviour},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0444-y},
  keywords = {journal policy,meta-research,open science,pre-registration,Registered Reports}
}

@article{Fanelli2012,
  title = {Negative Results Are Disappearing from Most Disciplines and Countries},
  author = {Fanelli, Daniele},
  year = {2012},
  month = mar,
  journal = {Scientometrics},
  volume = {90},
  number = {3},
  pages = {891--904},
  issn = {01389130},
  doi = {10.1007/s11192-011-0494-7},
  abstract = {Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have ``tested'' a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22\% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.},
  keywords = {Bias,Competition,Misconduct,Publication,Publish or perish,Research evaluation}
}

@article{Greenwald1975,
  title = {Consequences of {{Prejudice Against}} the {{Null Hypothesis}}},
  author = {Greenwald, Anthony G.},
  year = {1975},
  journal = {Psychological Bulletin},
  volume = {82},
  number = {1},
  pages = {1--20}
}

@article{Jonas2016,
  title = {How Can Preregistration Contribute to Research in Our Field?},
  author = {Jonas, Kai J. and Cesario, Joseph},
  year = {2016},
  journal = {Comprehensive Results in Social Psychology},
  volume = {1},
  number = {1-3},
  pages = {1--7},
  issn = {2374-3603, 2374-3611},
  doi = {10.1080/23743603.2015.1070611},
  urldate = {2018-05-23},
  langid = {english}
}

@article{Atkinson1982,
  title = {Statistical Significance, Reviewer Evaluations, and the Scientific Process: {{Is}} There a (Statistically) Significant Relationship?},
  shorttitle = {Statistical Significance, Reviewer Evaluations, and the Scientific Process},
  author = {Atkinson, Donald R. and Furlong, Michael J. and Wampold, Bruce E.},
  year = {1982},
  journal = {Journal of Counseling Psychology},
  volume = {29},
  number = {2},
  pages = {189--194},
  issn = {0022-0167},
  doi = {10.1037/0022-0167.29.2.189},
  urldate = {2020-01-05},
  langid = {english}
}

@article{Kacelnik1996,
  title = {Risky {{Theories}}---{{The Effects}} of {{Variance}} on {{Foraging Decisions}}},
  author = {Kacelnik, Alex and Bateson, Melissa},
  year = {1996},
  month = sep,
  journal = {Integrative and Comparative Biology},
  volume = {36},
  number = {4},
  pages = {402--434},
  publisher = {Oxford Academic},
  issn = {1540-7063},
  doi = {10.1093/icb/36.4.402},
  urldate = {2022-03-23},
  abstract = {Abstract. This paper concerns the response of foraging animals to variability in rate of gain, or risk. Both the empirical and theoretical literatures relevant},
  langid = {english}
}

@article{Pennycook2018,
  title = {An Analysis of the {{Canadian}} Cognitive Psychology Job Market (2006-2016)},
  author = {Pennycook, Gordon and Thompson, Valerie A.},
  year = {2018},
  month = jun,
  journal = {Canadian Journal of Experimental Psychology = Revue Canadienne De Psychologie Experimentale},
  volume = {72},
  number = {2},
  pages = {71--80},
  issn = {1878-7290},
  doi = {10.1037/cep0000149},
  abstract = {How accomplished does one need to be to compete in the Canadian cognitive psychology job market? We looked at the publication record of everyone who was hired as an assistant professor in Canadian cognitive psychology divisions with PhD programs between 2006 and 2016 (N = 64). Individuals who were hired from 2006 to 2011 averaged 10 journal-article publications up to and including the year they were hired. However, this number increased by 57\% to 18 publications between 2012 and 2016. Notably, this increase (a) occurred despite an increase in the number of positions since 2010, (b) was not restricted to top-ranked institutions, (c) did not come at the cost of decreasing quality in research (based on citations), and (d) was not driven by longer postdoctoral fellowships. To supply context, we obtained data on the publication records of 98 eminent and early-career award-winning cognitive psychologists when they obtained their first faculty positions. The correlation between year of hire and publication number in the full sample was strongly positive (r = .47) and driven primarily by a substantial increase in recent years, which suggests that the increasingly competitive job market is not specific to Canada. Finally, we found that behaviour (as opposed to neuroscience) researchers and those who obtained their PhDs from Canadian universities may be at particular risk in the job market. At a time when increasing numbers of PhDs are graduating from cognitive psychology programs, it has likely never been more difficult to obtain a faculty position. (PsycINFO Database Record},
  langid = {english},
  pmid = {29902028},
  keywords = {Behavioral Sciences,Canada,Cognition,Employment,Health Workforce,Humans,Psychology}
}

@article{Begley2012,
  title = {Raise Standards for Preclinical Cancer Research},
  author = {Begley, C. Glenn and Ellis, Lee M.},
  year = {2012},
  month = mar,
  journal = {Nature},
  volume = {483},
  number = {7391},
  pages = {531--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/483531a},
  urldate = {2022-06-25},
  abstract = {C. Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit.},
  copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Cancer,Drug development}
}

@article{deVries2018,
  title = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments: The Case of Depression},
  shorttitle = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments},
  author = {{de Vries}, Y. A. and Roest, A. M. and de Jonge, P. and Cuijpers, P. and Munaf{\`o}, M. R. and Bastiaansen, J. A.},
  year = {2018},
  month = nov,
  journal = {Psychological Medicine},
  volume = {48},
  number = {15},
  pages = {2453--2455},
  publisher = {Cambridge University Press},
  issn = {0033-2917, 1469-8978},
  doi = {10.1017/S0033291718001873},
  urldate = {2022-06-27},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0033291718001873/resource/name/firstPage-S0033291718001873a.jpg},
  langid = {english},
  keywords = {Antidepressants,bias,citation bias,depression,psychotherapy,reporting bias}
}

@article{Chalmers2009,
  title = {Avoidable Waste in the Production and Reporting of Research Evidence},
  author = {Chalmers, Iain and Glasziou, Paul},
  year = {2009},
  month = jul,
  journal = {The Lancet},
  volume = {374},
  number = {9683},
  pages = {86--89},
  publisher = {Elsevier},
  issn = {0140-6736, 1474-547X},
  doi = {10.1016/S0140-6736(09)60329-9},
  urldate = {2022-07-24},
  langid = {english},
  pmid = {19525005}
}

@article{Brand2022,
  title = {{{THE MAINTENANCE RACE}}},
  author = {Brand, Stewart},
  year = {2022},
  month = jul,
  journal = {Works in Progress},
  number = {8},
  urldate = {2022-07-25},
  abstract = {The world's first round-the-world solo yacht race was a thrilling and, for some, deadly contest. How its participants maintained their vessels can help us understand just how fundamental maintenance is.},
  langid = {american}
}

@article{Traag2021,
  title = {Inferring the Causal Effect of Journals on Citations},
  author = {Traag, V. A.},
  year = {2021},
  month = jul,
  journal = {Quantitative Science Studies},
  volume = {2},
  number = {2},
  pages = {496--504},
  issn = {2641-3337},
  doi = {10.1162/qss_a_00128},
  urldate = {2022-07-25},
  abstract = {Articles in high-impact journals are, on average, more frequently cited. But are they cited more often because those articles are somehow more ``citable''? Or are they cited more often simply because they are published in a high-impact journal? Although some evidence suggests the latter, the causal relationship is not clear. We here compare citations of preprints to citations of the published version to uncover the causal mechanism. We build on an earlier model of citation dynamics to infer the causal effect of journals on citations. We find that high-impact journals select articles that tend to attract more citations. At the same time, we find that high-impact journals augment the citation rate of published articles. Our results yield a deeper understanding of the role of journals in the research system. The use of journal metrics in research evaluation has been increasingly criticized in recent years and article-level citations are sometimes suggested as an alternative. Our results show that removing impact factors from evaluation does not negate the influence of journals. This insight has important implications for changing practices of research evaluation.}
}

@article{Shafir2005,
  title = {Caste-Specific Differences in Risk Sensitivity in Honeybees, {{Apis}} Mellifera},
  author = {Shafir, Sharoni and Menda, Gil and Smith, Brian H.},
  year = {2005},
  month = apr,
  journal = {Animal Behaviour},
  volume = {69},
  number = {4},
  pages = {859--868},
  issn = {0003-3472},
  doi = {10.1016/j.anbehav.2004.07.011},
  urldate = {2022-10-16},
  abstract = {Honeybee workers (foragers) are risk averse to variability in volume of reward when measured by conditioning of the proboscis extension response, and the level of risk aversion depends on the coefficient of variation of the variable distribution. Since drones do not forage on flowers, they may not have been under selection for risk-sensitive choice behaviour. We compared risk sensitivity of workers and drones and their ability to discriminate between the reward volumes used in the risk sensitivity experiments. Both castes discriminated better between 0 and 0.4{$\mu$}l than between 0.4 and 1.2{$\mu$}l, consistent with Weber's law of relative discrimination. Workers discriminated between both volume pairs better than drones, and workers showed greater risk aversion than drones. This is the first demonstration of caste-specific differences in risk sensitivity. These differences do not appear to be the result of differences in energy budgets, since both castes were on positive energy budgets. Levels of risk aversion were consistent with the coefficient of variation model. We calculated the relative associative strengths of subjects to the reward volumes from their choice proportions in the discrimination tests. The relative associative strengths of workers were greater than those of drones, and in both castes the relative associative strength of 0.4{$\mu$}l relative to 0{$\mu$}l was greater than that of 1.2{$\mu$}l relative to 0.4{$\mu$}l. Owing to Jensen's inequality, the decreasing functions of differences in relative associative strengths could explain differences in degree of risk aversion between the castes. Our findings are consistent with both mechanistic and functional explanations.},
  langid = {english}
}

@article{OBoyle2017,
  title = {The {{Chrysalis Effect}}: {{How Ugly Initial Results Metamorphosize Into Beautiful Articles}}},
  shorttitle = {The {{Chrysalis Effect}}},
  author = {O'Boyle, Ernest Hugh and Banks, George Christopher and {Gonzalez-Mul{\'e}}, Erik},
  year = {2017},
  month = feb,
  journal = {Journal of Management},
  volume = {43},
  number = {2},
  pages = {376--399},
  publisher = {SAGE Publications Inc},
  issn = {0149-2063},
  doi = {10.1177/0149206314527133},
  urldate = {2022-11-25},
  abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the ?Chrysalis Effect.?},
  langid = {english}
}

@article{Gopalakrishna2022,
  title = {Prevalence of Responsible Research Practices among Academics in {{The Netherlands}}},
  author = {Gopalakrishna, Gowri and Wicherts, Jelte M. and Vink, Gerko and Stoop, Ineke and van den Akker, Olmo R. and ter Riet, Gerben and Bouter, Lex M.},
  year = {2022},
  month = aug,
  volume = {11},
  number = {471},
  doi = {10.12688/f1000research.110664.2},
  urldate = {2023-01-10},
  abstract = {Background: Traditionally, research integrity studies have focused on research misbehaviors and their explanations. Over time, attention has shifted towards preventing questionable research practices and promoting responsible ones. However, data on the prevalence of responsible research practices, especially open methods, open codes and open data and their underlying associative factors, remains scarce. Methods: We conducted a web-based anonymized questionnaire, targeting all academic researchers working at or affiliated to a university or university medical center in The Netherlands, to investigate the prevalence and potential explanatory factors of 11 responsible research practices. Results: A total of 6,813 academics completed the survey, the results of which show that prevalence of responsible practices differs substantially across disciplines and ranks, with 99 percent avoiding plagiarism in their work but less than 50 percent pre-registering a research protocol. Arts and humanities scholars as well as PhD candidates and junior researchers engaged less often in responsible research practices. Publication pressure negatively affected responsible practices, while mentoring, scientific norms subscription and funding pressure stimulated them. Conclusions: Understanding the prevalence of responsible research practices across disciplines and ranks, as well as their associated explanatory factors, can help to systematically address disciplinary- and academic rank-specific obstacles, and thereby facilitate responsible conduct of research.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Open science,Research integrity,Responsible conduct of research,Responsible research practices}
}

@article{Gopalakrishna2022a,
  title = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors: {{A}} Survey among Academic Researchers in {{The Netherlands}}},
  shorttitle = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors},
  author = {Gopalakrishna, Gowri and ter Riet, Gerben and Vink, Gerko and Stoop, Ineke and Wicherts, Jelte M. and Bouter, Lex M.},
  year = {2022},
  month = feb,
  journal = {PLOS ONE},
  volume = {17},
  number = {2},
  pages = {e0263023},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0263023},
  urldate = {2023-03-14},
  abstract = {Prevalence of research misconduct, questionable research practices (QRPs) and their associations with a range of explanatory factors has not been studied sufficiently among academic researchers. The National Survey on Research Integrity targeted all disciplinary fields and academic ranks in the Netherlands. It included questions about engagement in fabrication, falsification and 11 QRPs over the previous three years, and 12 explanatory factor scales. We ensured strict identity protection and used the randomized response method for questions on research misconduct. 6,813 respondents completed the survey. Prevalence of fabrication was 4.3\% (95\% CI: 2.9, 5.7) and of falsification 4.2\% (95\% CI: 2.8, 5.6). Prevalence of QRPs ranged from 0.6\% (95\% CI: 0.5, 0.9) to 17.5\% (95\% CI: 16.4, 18.7) with 51.3\% (95\% CI: 50.1, 52.5) of respondents engaging frequently in at least one QRP. Being a PhD candidate or junior researcher increased the odds of frequently engaging in at least one QRP, as did being male. Scientific norm subscription (odds ratio (OR) 0.79; 95\% CI: 0.63, 1.00) and perceived likelihood of detection by reviewers (OR 0.62, 95\% CI: 0.44, 0.88) were associated with engaging in less research misconduct. Publication pressure was associated with more often engaging in one or more QRPs frequently (OR 1.22, 95\% CI: 1.14, 1.30). We found higher prevalence of misconduct than earlier surveys. Our results suggest that greater emphasis on scientific norm subscription, strengthening reviewers in their role as gatekeepers of research quality and curbing the ``publish or perish'' incentive system promotes research integrity.},
  langid = {english},
  keywords = {Deception,Linear regression analysis,Medical humanities,Medicine and health sciences,Open science,Research integrity,Scientific misconduct,Surveys}
}

@article{Boschen2023,
  title = {Changes in Methodological Study Characteristics in Psychology between 2010-2021},
  author = {B{\"o}schen, Ingmar},
  year = {2023},
  month = may,
  journal = {PLOS ONE},
  volume = {18},
  number = {5},
  pages = {e0283353},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0283353},
  urldate = {2023-10-01},
  abstract = {In 2015, the Open Science Collaboration repeated a series of 100 psychological experiments. Since a considerable part of these replications could not confirm the original effects and some of them pointed in the opposite direction, psychological research is said to lack reproducibility. Several general criticisms can explain this finding, such as the standardized use of undirected nil-null hypothesis tests, samples being too small and selective, lack of corrections for multiple testing, but also some widespread questionable research practices and incentives to publish positive results only. A selection of 57,909 articles from 12 renowned journals is processed with the JATSdecoder software to analyze the extent to which several empirical research practices in psychology have changed over the past 12 years. To identify journal- and time-specific changes, the relative use of statistics based on p-values, the number of reported p-values per paper, the relative use of confidence intervals, directed tests, power analysis, Bayesian procedures, non-standard {$\alpha$} levels, correction procedures for multiple testing, and median sample sizes are analyzed for articles published between 2010 and 2015 and after 2015, and in more detail for every included journal and year of publication. In addition, the origin of authorships is analyzed over time. Compared to articles that were published in and before 2015, the median number of reported p-values per article has decreased from 14 to 12, whereas the median proportion of significant p-values per article remained constant at 69\%. While reports of effect sizes and confidence intervals have increased, the {$\alpha$} level is usually set to the default value of .05. The use of corrections for multiple testing has decreased. Although uncommon in each case (4\% in total), directed testing is used less frequently, while Bayesian inference has become more common after 2015. The overall median estimated sample size has increased from 105 to 190.},
  langid = {english},
  keywords = {Anxiety,Behavioral neuroscience,Cognitive neuroscience,Depression,Experimental psychology,Mental health and psychiatry,Psychology,Social psychology}
}

@article{Gerber2001,
  title = {Testing for {{Publication Bias}} in {{Political Science}}},
  author = {Gerber, Alan S. and Green, Donald P. and Nickerson, David},
  year = {2001},
  month = jan,
  journal = {Political Analysis},
  volume = {9},
  number = {4},
  pages = {385--392},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/oxfordjournals.pan.a004877},
  urldate = {2023-11-20},
  abstract = {If the publication decisions of journals are a function of the statistical significance of research findings, the published literature may suffer from ``publication bias.'' This paper describes a method for detecting publication bias. We point out that to achieve statistical significance, the effect size must be larger in small samples. If publications tend to be biased against statistically insignificant results, we should observe that the effect size diminishes as sample sizes increase. This proposition is tested and confirmed using the experimental literature on voter mobilization.},
  langid = {english}
}

@misc{McElreath2021,
  title = {Science {{Is Like A Chicken Coop}}},
  author = {McElreath, Richard},
  year = {2021},
  month = jul,
  address = {online},
  urldate = {2024-07-04},
  abstract = {Creative Commons Attribution license (reuse allowed)},
  annotation = {Abstract: The quality and transparency of scholarship is influenced by\\
professional incentives. So say we all. Naturally much discussion\\
focuses on reforming incentives. But reforming scholarly incentives is\\
not easy, and incentives may matter less than structural and\\
demographic forces. Using analogies from population biology, I sketch\\
some problems and opportunities for effective science reform. First,\\
incentives arise from structure as much as from explicit reward.\\
Second, incentives are not all---demography and development\\
matter as well. Third, there are fundamental limits on the power of\\
incentives when the fates of individuals are largely up to chance.\\
There are reasons to think reform can succeed, especially if we adopt\\
a dynamic and structured view of the cultural evolution of scholarly\\
communities.}
}

@article{Snyder2021,
  title = {Time and {{Chance}}: {{Using Age Partitioning}} to {{Understand How Luck Drives Variation}} in {{Reproductive Success}}},
  shorttitle = {Time and {{Chance}}},
  author = {Snyder, Robin E. and Ellner, Stephen P. and Hooker, Giles},
  year = {2021},
  month = apr,
  journal = {The American Naturalist},
  volume = {197},
  number = {4},
  pages = {E110-E128},
  publisher = {The University of Chicago Press},
  issn = {0003-0147},
  doi = {10.1086/712874},
  urldate = {2024-07-04},
  abstract = {Over the course of individual lifetimes, luck usually explains a large fraction of the between-individual variation in life span or lifetime reproductive output (LRO) within a population, while variation in individual traits or ``quality'' explains much less. To understand how, where in the life cycle, and through which demographic processes luck trumps trait variation, we show how to partition by age the contributions of luck and trait variation to LRO variance and how to quantify three distinct components of luck. We apply these tools to several empirical case studies. We find that luck swamps effects of trait variation at all ages, primarily because of randomness in individual state dynamics (``state trajectory luck''). Luck early in life is most important. Very early state trajectory luck generally determines whether an individual ever breeds, likely by ensuring that they are not dead or doomed quickly. Less early luck drives variation in success among those breeding at least once. Consequently, the importance of luck often has a sharp peak early in life or it has two peaks. We suggest that ages or stages where the importance of luck peaks are potential targets for interventions to benefit a population of concern, different from those identified by eigenvalue elasticity analysis.},
  keywords = {Artemisia tridentata,individual stochasticity,lifetime reproductive success,reproductive skew,Rissa tridactyla,trait variation}
}

@article{Sanbonmatsu2015,
  title = {Why a {{Confirmation Strategy Dominates Psychological Science}}},
  author = {Sanbonmatsu, David M. and Posavac, Steven S. and Behrends, Arwen A. and Moore, Shannon M. and Uchino, Bert N.},
  year = {2015},
  month = sep,
  journal = {PLOS ONE},
  volume = {10},
  number = {9},
  pages = {e0138197},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0138197},
  urldate = {2024-07-10},
  abstract = {Our research explored the incidence and appropriateness of the much-maligned confirmatory approach to testing scientific hypotheses. Psychological scientists completed a survey about their research goals and strategies. The most frequently reported goal is to test the non-absolute hypothesis that a particular relation exists in some conditions. As expected, few scientists reported testing universal hypotheses. Most indicated an inclination to use a confirmation strategy to test the non-absolute hypotheses that a particular relation sometimes occurs or sometimes does not occur, and a disconfirmation strategy to test the absolute hypotheses that a particular relation always occurs or never occurs. The confirmatory search that dominates the field was found to be associated with the testing of non-absolute hypotheses. Our analysis indicates that a confirmatory approach is the normatively correct test of the non-absolute hypotheses that are the starting point of most studies. It also suggests that the strategy of falsification that was once proposed by Popper is generally incorrect given the infrequency of tests of universal hypotheses.},
  langid = {english},
  keywords = {Clinical psychology,Developmental psychology,Psychologists,Psychology,Scientists,Social psychology,Survey research,Surveys}
}

@article{Duc2020,
  title = {Impacts of {{Novel Vietnamese Government Regulations}} on {{Radiological PhD}} and {{Professorship Candidates}}: An {{Initial Report}}},
  shorttitle = {Impacts of {{Novel Vietnamese Government Regulations}} on {{Radiological PhD}} and {{Professorship Candidates}}},
  author = {Duc, Nguyen Minh and Ha, Hoang Duc and Khoa, Mai Trong and Thong, Pham Minh},
  year = {2020},
  month = jun,
  journal = {Acta Informatica Medica},
  volume = {28},
  number = {2},
  pages = {152--156},
  issn = {0353-8109},
  doi = {10.5455/aim.2020.28.152-156},
  urldate = {2024-07-17},
  abstract = {Introduction: In Vietnam, the successful publication of research in indexed journals is mandatory to obtain academic appointments and promotions in medical colleges and institutions, according to the current guidelines established by the State Council for Professorship and Ministry of Education and Training. Aim: This study aimed to investigate the impacts of novel Vietnamese government regulations on radiological PhD and professorship candidates. Methods: This study evaluated freely accessible data, available online, and, therefore, did not require institutional review board approval. We assessed the numbers of radiological PhD candidates at Hanoi Medical University and the numbers of published Vietnamese radiological papers, from 2012 to 2019, indexed in the SCImago database. In addition, we evaluated the numbers of qualified radiological professors and associate professors employed at universities during the same period. We did not include nuclear medicine PhD and professorship candidates, in this study. The data are presented as bar and line charts. Results: Following the enactment of 08/2017/TT-BGD{\DJ}T and 37/2018/Q{\DJ}-TTg, we observed that the numbers of radiological PhD and professorship candidates were significantly reduced. From 2012 to 2019, only one candidate qualified for appointment as a radiological professor. However, the number of radiological papers rose dramatically during the same time period. Conclusion: The enactment of 08/2017/TT-BGD{\DJ}T and 37/2018/Q{\DJ}-TTg had strong impacts on the numbers of PhD and professorship candidates. Owing to these new regulations, the number of published, international, peer-reviewed radiological papers has increased; however, some undesired consequences may have occurred, such as papers being published in predatory or suspected predatory journals, double or triple submissions, and plagiarism.},
  pmcid = {PMC7382773},
  pmid = {32742070}
}

@misc{Schiekiera2024,
  title = {Meta-{{Research}}: {{Does Scientific Productivity Increase}} the {{Publication}} of {{Positive Results}}? {{Examining Research Groups}}' {{Scientific Productivity}} and {{Positive Results}} in a {{German Clinical Psychology Sample}}},
  shorttitle = {Meta-{{Research}}},
  author = {Schiekiera, Louis and Niemeyer, Helen},
  year = {2024},
  month = jul,
  publisher = {OSF},
  doi = {10.31234/osf.io/8ncbx},
  urldate = {2024-08-08},
  abstract = {Background: The overrepresentation of positive results in psychology is often attributed in part to publication bias. However, the impact of research group output on the prevalence of positive results has not yet been investigated. The present study examines whether German clinical psychology research groups with high versus low publication outputs differ in the prevalence of positive outcomes in their publications. Methods: Scientific productivity was defined as the ratio of quantitative-empirical publications to the number of academic staff per chair. We analyzed publications authored by clinical psychology researchers at German universities from 2013 to 2022, sourced from PubMed and OpenAlex. After excluding meta-analyses, reviews, and non-empirical studies, 2,280 empirical studies from 99 research groups were identified. We then randomly sampled and coded 300 papers, evenly split between the highest and lowest output quartiles, and examined the first hypothesis. Results: There was no statistically significant difference between the highest and the lowest output quartiles, with both reporting approximately 90\% positive results. Higher group paper counts were not associated with more positive results. However, results with partial support were significantly more prevalent in the highest output quartile than in the lowest output quartile. Conclusion: Our results suggest a general excess of positive results in clinical psychology. Contrary to our hypothesis, German clinical psychology research groups with high and low publication outputs do not differ in the prevalence of positive outcomes in their publications.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {clinical psychology,meta-science,negative results,open science,positive results,publication bias,scientific productivity}
}

@phdthesis{OMahony2023,
  title = {Comparative Analysis of {{Registered Reports}} and the Standard Research Literature},
  author = {O'Mahony, Aoife},
  year = {2023},
  month = oct,
  urldate = {2024-08-08},
  abstract = {The replication crisis revealed high levels of bias and questionable research practices (QRPs) in psychology research. Registered Reports (RRs) have been increasingly adopted as a possible solution, but there has been relatively little evidence of whether this novel publishing format appears to be working as intended to reduce bias and QRPs. This project sought to build a detailed database of RRs and closely matched standard reports (SRs), to investigate whether RRs perform better than SRs on indicators of quality, rigour, and transparency. 170 RRs were gathered, representing psychology, health, and related disciplines. Each RR was matched to 2 SRs and their characteristics were coded and compared between the two article types. Some brief descriptive analyses were also undertaken on a larger total sample of RRs (n = 359) that did not have a comparison sample, and a smaller sample of 12 RRs and 12 SRs was examined for signs of HARKing. Six key findings were observed. First, RRs exhibit lower rates of supported hypotheses and higher rates of unsupported hypotheses compared with SRs. Second, rates of open practices are higher among RRs than SRs. Third, RRs also appear to be more strongly associated with some methodological practices indicative of greater rigour and transparency. Fourth, author demographics and article citation rates revealed few differences between the article types. Fifth, higher citation rates were associated with more positive findings and fewer negative findings within both the SRs and RRs, but there was no statistically significant relationship between the journal impact factor and whether hypotheses were supported, for either article type. Finally, HARKing appeared to be non-existent in RRs, while some evidence of HARKing was observed in SRs, however, replication is needed. Overall, the evidence presented in this work demonstrates that, while there are still areas for improvement, RRs do appear to be working as intended in being associated with improved research practices, although further research is needed to determine causal impacts.},
  langid = {english},
  school = {Cardiff University}
}

@article{Stefan2023,
  title = {Big Little Lies: A Compendium and Simulation of p-Hacking Strategies},
  shorttitle = {Big Little Lies},
  author = {Stefan, Angelika M. and Sch{\"o}nbrodt, Felix D.},
  year = {2023},
  month = feb,
  journal = {Royal Society Open Science},
  volume = {10},
  number = {2},
  pages = {220346},
  publisher = {Royal Society},
  doi = {10.1098/rsos.220346},
  urldate = {2024-08-09},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of 12 p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
  keywords = {error rates,false-positive rate,p-curve,questionable research practices,Shiny app,significance,simulation}
}

@article{Kepes2022,
  title = {Questionable Research Practices among Researchers in the Most Research-Productive Management Programs},
  author = {Kepes, Sven and Keener, Sheila K. and McDaniel, Michael A. and Hartman, Nathan S.},
  year = {2022},
  journal = {Journal of Organizational Behavior},
  volume = {43},
  number = {7},
  pages = {1190--1208},
  issn = {1099-1379},
  doi = {10.1002/job.2623},
  urldate = {2024-08-09},
  abstract = {Questionable research practices (QRPs) among researchers have been a source of concern in many fields of study. QRPs are often used to enhance the probability of achieving statistical significance which affects the likelihood of a paper being published. Using a sample of researchers from 10 top research-productive management programs, we compared hypotheses tested in dissertations to those tested in journal articles derived from those dissertations to draw inferences concerning the extent of engagement in QRPs. Results indicated that QRPs related to changes in sample size and covariates were associated with unsupported dissertation hypotheses becoming supported in journal articles. Researchers also tended to exclude unsupported dissertation hypotheses from journal articles. Likewise, results suggested that many article hypotheses may have been created after the results were known (i.e., HARKed). Articles from prestigious journals contained a higher percentage of potentially HARKed hypotheses than those from less well-regarded journals. Finally, articles published in prestigious journals were associated with more QRP usage than less prestigious journals. QRPs increase in the percentage of supported hypotheses and result in effect sizes that likely overestimate population parameters. As such, results reported in articles published in our most prestigious journals may be less credible than previously believed.},
  copyright = {{\copyright} 2022 The Authors. Journal of Organizational Behavior published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {Chrysalis Effect,HARKing,questionable research practices,research integrity}
}

@article{Tijdink2014,
  title = {Publication {{Pressure}} and {{Scientific Misconduct}} in {{Medical Scientists}}},
  author = {Tijdink, Joeri K. and Verbeke, Reinout and Smulders, Yvo M.},
  year = {2014},
  month = dec,
  journal = {Journal of Empirical Research on Human Research Ethics},
  volume = {9},
  number = {5},
  pages = {64--71},
  publisher = {SAGE Publications Inc},
  issn = {1556-2646},
  doi = {10.1177/1556264614552421},
  urldate = {2024-08-10},
  abstract = {There is increasing evidence that scientific misconduct is more common than previously thought. Strong emphasis on scientific productivity may increase the sense of publication pressure. We administered a nationwide survey to Flemish biomedical scientists on whether they had engaged in scientific misconduct and whether they had experienced publication pressure. A total of 315 scientists participated in the survey; 15\% of the respondents admitted they had fabricated, falsified, plagiarized, or manipulated data in the past 3 years. Fraud was more common among younger scientists working in a university hospital. Furthermore, 72\% rated publication pressure as ``too high.'' Publication pressure was strongly and significantly associated with a composite scientific misconduct severity score.},
  langid = {english}
}

@article{Tijdink2013,
  title = {Publication {{Pressure}} and {{Burn Out}} among {{Dutch Medical Professors}}: {{A Nationwide Survey}}},
  shorttitle = {Publication {{Pressure}} and {{Burn Out}} among {{Dutch Medical Professors}}},
  author = {Tijdink, Joeri K. and Vergouwen, Anton C. M. and Smulders, Yvo M.},
  year = {2013},
  month = sep,
  journal = {PLOS ONE},
  volume = {8},
  number = {9},
  pages = {e73381},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0073381},
  urldate = {2024-08-10},
  abstract = {Background Publication of scientific research papers is important for professionals working in academic medical centres. Quantitative measures of scientific output determine status and prestige, and serve to rank universities as well as individuals. The pressure to generate maximum scientific output is high, and quantitative aspects may tend to dominate over qualitative ones. How this pressure influences professionals' perception of science and their personal well-being is unknown. Methods and Findings We performed an online survey inviting all medical professors (n = 1206) of the 8 academic medical centres in The Netherlands to participate. They were asked to fill out 2 questionnaires; a validated Publication Pressure Questionnaire and the Maslach Burnout Inventory. In total, 437 professors completed the questionnaires. among them, 54\% judge that publication pressure `has become excessive', 39\% believe that publication pressure `affects the credibility of medical research' and 26\% judge that publication pressure has a `sickening effect on medical science'. The burn out questionnaire indicates that 24\% of medical professors have signs of burn out. The number of years of professorship was significantly related with experiencing less publication pressure. Significant and strong associations between burn out symptoms and the level of perceived publication pressure were found. The main limitation is the possibility of response bias. Conclusion A substantial proportion of medical professors believe that publication pressure has become excessive, and have a cynical view on the validity of medical science. These perceptions are statistically correlated to burn out symptoms. Further research should address the effects of publication pressure in more detail and identify alternative ways to stimulate the quality of medical science.},
  langid = {english},
  keywords = {Careers,Emotions,Medical personnel,Medicine and health sciences,Psychological stress,Psychometrics,Questionnaires,Surveys}
}

@article{Waaijer2018,
  title = {Competition in {{Science}}: {{Links Between Publication Pressure}}, {{Grant Pressure}} and the {{Academic Job Market}}},
  shorttitle = {Competition in {{Science}}},
  author = {Waaijer, Cathelijn J. F. and Teelken, Christine and Wouters, Paul F. and {van der Weijden}, Inge C. M.},
  year = {2018},
  month = jun,
  journal = {Higher Education Policy},
  volume = {31},
  number = {2},
  pages = {225--243},
  issn = {1740-3863},
  doi = {10.1057/s41307-017-0051-y},
  urldate = {2024-08-10},
  abstract = {In the current discussions concerning the pressure for publication and to obtain grants, the questions about what publication and grant pressure actually involve and how they are linked to the academic job market, are often neglected. In this study, we show that publication and grand pressure are not just external forces but internal ones as scientists apply pressure to themselves in the process of competition. Through two surveys, one of 1,133 recent PhDs at five Dutch universities and one of 225 postdoctoral researchers at two Dutch universities, we found that publication and grant pressure have to be considered in relation with competition for academic jobs. While publication and grant pressure are perceived to be too high by a majority of these early career researchers, the effects of publication and grant pressure by themselves are limited.},
  langid = {english},
  keywords = {academic careers,competition,early career researchers,grant pressure,publication pressure}
}

@article{Fanelli2010b,
  title = {Do {{Pressures}} to {{Publish Increase Scientists}}' {{Bias}}? {{An Empirical Support}} from {{US States Data}}},
  shorttitle = {Do {{Pressures}} to {{Publish Increase Scientists}}' {{Bias}}?},
  author = {Fanelli, Daniele},
  year = {2010},
  month = apr,
  journal = {PLOS ONE},
  volume = {5},
  number = {4},
  pages = {e10271},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0010271},
  urldate = {2024-08-10},
  abstract = {The growing competition and ``publish or perish'' culture in academia might conflict with the objectivity and integrity of research, because it forces scientists to produce ``publishable'' results at all costs. Papers are less likely to be published and to be cited if they report ``negative'' results (results that fail to support the tested hypothesis). Therefore, if publication pressures increase scientific bias, the frequency of ``positive'' results in the literature should be higher in the more competitive and ``productive'' academic environments. This study verified this hypothesis by measuring the frequency of positive results in a large random sample of papers with a corresponding author based in the US. Across all disciplines, papers were more likely to support a tested hypothesis if their corresponding authors were working in states that, according to NSF data, produced more academic papers per capita. The size of this effect increased when controlling for state's per capita R\&D expenditure and for study characteristics that previous research showed to correlate with the frequency of positive results, including discipline and methodology. Although the confounding effect of institutions' prestige could not be excluded (researchers in the more productive universities could be the most clever and successful in their experiments), these results support the hypothesis that competitive academic environments increase not only scientists' productivity but also their bias. The same phenomenon might be observed in other countries where academic competition and pressures to publish are high.},
  langid = {english},
  keywords = {Behavioral neuroscience,Bibliometrics,Careers,Drug interactions,Scientific misconduct,Scientists,Toxicology,United States}
}

@article{vanDalen2021,
  title = {How the Publish-or-Perish Principle Divides a Science: The Case of Economists},
  shorttitle = {How the Publish-or-Perish Principle Divides a Science},
  author = {{van Dalen}, Hendrik P.},
  year = {2021},
  month = feb,
  journal = {Scientometrics},
  volume = {126},
  number = {2},
  pages = {1675--1694},
  issn = {1588-2861},
  doi = {10.1007/s11192-020-03786-x},
  urldate = {2024-08-10},
  abstract = {The publish-or-perish principle has become a fact of academic life in gaining a position or being promoted. Evidence is mounting that benefits of this pressure is being countered by the downsides, like forms of goal displacement by scientists or unethical practices. In this paper we evaluate whether perceived work pressure (publishing, acquisition funds, teaching, administration) is associated with different attitudes towards science and the workplace among economists working at Dutch universities. Publication pressure is high and is related to faculty position and university ranking position. Based on a latent class analysis we can detect a clear divide among economists. Around two third of the economists perceives that this pressure has upsides as well as serious downsides and one third only perceives upsides and no downsides. Full professors see more than other faculty members the positive sides of the publish-or-perish principle and virtually no downsides. These different perceptions are also reflected in their appreciation of the academic work environment.},
  langid = {english},
  keywords = {Economists,Incentives,Publication pressure,Science metrics,Universities}
}

@article{vanDalen2012a,
  title = {Intended and Unintended Consequences of a Publish-or-Perish Culture: {{A}} Worldwide Survey},
  shorttitle = {Intended and Unintended Consequences of a Publish-or-Perish Culture},
  author = {{van Dalen}, Hendrik P. and Henkens, K{\`e}ne},
  year = {2012},
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {63},
  number = {7},
  pages = {1282--1293},
  issn = {1532-2890},
  doi = {10.1002/asi.22636},
  urldate = {2024-08-10},
  abstract = {How does publication pressure in modern-day universities affect the intrinsic and extrinsic rewards in science? By using a worldwide survey among demographers in developed and developing countries, the authors show that the large majority perceive the publication pressure as high, but more so in Anglo-Saxon countries and to a lesser extent in Western Europe. However, scholars see both the pros (upward mobility) and cons (excessive publication and uncitedness, neglect of policy issues, etc.) of the so-called publish-or-perish culture. By measuring behavior in terms of reading and publishing, and perceived extrinsic rewards and stated intrinsic rewards of practicing science, it turns out that publication pressure negatively affects the orientation of demographers towards policy and knowledge sharing. There are no signs that the pressure affects reading and publishing outside the core discipline.},
  copyright = {{\copyright} 2012 ASIS\&T},
  langid = {english},
  keywords = {bibliometrics,scientists,surveys}
}

@article{Miller2011a,
  title = {Publish or Perish: Academic Life as Management Faculty Live It},
  shorttitle = {Publish or Perish},
  author = {Miller, Alan N. and Taylor, Shannon G. and Bedeian, Arthur G.},
  year = {2011},
  month = sep,
  journal = {Career Development International},
  volume = {16},
  number = {5},
  pages = {422--445},
  publisher = {Emerald Group Publishing Limited},
  issn = {1362-0436},
  doi = {10.1108/13620431111167751},
  urldate = {2024-08-10},
  abstract = {-- Although many in academe have speculated about the effects of pressure to publish on the management discipline -- often referred to as ``publish or perish'' -- prevailing knowledge has been based on anecdotal rather than empirical evidence. The aim of the present paper is to shed light on the perceptions of management faculty regarding the pressure to publish imperative., -- The authors surveyed faculty in 104 management departments of AACSB accredited, research-oriented US business schools to explore the prevalence, sources, and effects of pressure to publish., -- Results indicate that pressure to publish affects both tenured and tenure-track management faculty, although the latter, as a group, feel significantly more pressure than those who are tenured. The primary source of this pressure is faculty themselves who are motivated by the prospects of enhancing their professional reputation, leaving a permanent mark on their profession, and increasing their salary and job mobility. The effects of pressure to publish include heightened stress levels; the marginalization of teaching; and research that may lack relevance, creativity, and innovation., -- The sample was intentionally restricted to faculty from management departments affiliated with research-oriented US business schools and does not include faculty from departments that are less research-oriented and, therefore, would be expected to put less pressure on their faculty to publish., -- Although the effects of pressure to publish are not necessarily always negative, the paper offers some fundamental suggestions to management (and other) faculty who wish to mitigate the deleterious effects of pressure to publish., -- Although the findings may not be surprising to more seasoned faculty, to the authors' knowledge this is the first time they have been documented in the published literature. As such, they advance discussions of ``publish or perish'' beyond mere conjecture and ``shared myths'' allowing management faculty to more rationally debate its consequences and their implications for academic life.},
  langid = {english}
}

@article{Grimes2018,
  title = {Modelling Science Trustworthiness under Publish or Perish Pressure},
  author = {Grimes, David Robert and Bauch, Chris T. and Ioannidis, John P. A.},
  year = {2018},
  month = jan,
  journal = {Royal Society Open Science},
  volume = {5},
  number = {1},
  pages = {171511},
  publisher = {Royal Society},
  doi = {10.1098/rsos.171511},
  urldate = {2024-08-10},
  abstract = {Scientific publication is immensely important to the scientific endeavour. There is, however, concern that rewarding scientists chiefly on publication creates a perverse incentive, allowing careless and fraudulent conduct to thrive, compounded by the predisposition of top-tier journals towards novel, positive findings rather than investigations confirming null hypothesis. This potentially compounds a reproducibility crisis in several fields, and risks undermining science and public trust in scientific findings. To date, there has been comparatively little modelling on factors that influence science trustworthiness, despite the importance of quantifying the problem. We present a simple phenomenological model with cohorts of diligent, careless and unethical scientists, with funding allocated by published outputs. This analysis suggests that trustworthiness of published science in a given field is influenced by false positive rate, and pressures for positive results. We find decreasing available funding has negative consequences for resulting trustworthiness, and examine strategies to combat propagation of irreproducible science.},
  keywords = {public trust in science,publish or perish,research ethics,research fraud,science trustworthiness}
}

@article{Qiu2010,
  title = {Publish or Perish in {{China}}: The Pressure to Rack up Publications in High-Impact Journals Could Encourage Misconduct, Some Say},
  shorttitle = {Publish or Perish in {{China}}},
  author = {Qiu, Jane},
  year = {2010},
  month = jan,
  journal = {Nature},
  volume = {463},
  number = {7278},
  pages = {142--144},
  publisher = {Nature Publishing Group},
  issn = {00280836},
  urldate = {2024-08-10},
  abstract = {{$<$}em{$>$}Gale{$<$}/em{$>$} OneFile includes Publish or perish in China: the pressure to rack up pub by Jane Qiu. Click to explore.},
  langid = {english}
}

@article{Paruzel-Czachura2021,
  title = {Publish or Be Ethical? {{Publishing}} Pressure and Scientific Misconduct in Research},
  shorttitle = {Publish or Be Ethical?},
  author = {{Paruzel-Czachura}, Mariola and Baran, Lidia and Spendel, Zbigniew},
  year = {2021},
  month = jul,
  journal = {Research Ethics},
  volume = {17},
  number = {3},
  pages = {375--397},
  publisher = {SAGE Publications Ltd},
  issn = {1747-0161},
  doi = {10.1177/1747016120980562},
  urldate = {2024-08-10},
  abstract = {The paper reports two studies exploring the relationship between scholars' self-reported publication pressure and their self-reported scientific misconduct in research. In Study 1 the participants (N\,=\,423) were scholars representing various disciplines from one big university in Poland. In Study 2 the participants (N\,=\,31) were exclusively members of the management, such as dean, director, etc. from the same university. In Study 1 the most common reported form of scientific misconduct was honorary authorship. The majority of researchers (71\%) reported that they had not violated ethical standards in the past; 3\% admitted to scientific misconduct; 51\% reported being were aware of colleagues' scientific misconduct. A small positive correlation between perceived publication pressure and intention to engage in scientific misconduct in the future was found. In Study 2 more than half of the management (52\%) reported being aware of researchers' dishonest practices, the most frequent one of these being honorary authorship. As many as 71\% of the participants report observing publication pressure in their subordinates. The primary conclusions are: (1) most scholars are convinced of their morality and predict that they will behave morally in the future; (2) scientific misconduct, particularly minor offenses such as honorary authorship, is frequently observed both by researchers (particularly in their colleagues) and by their managers; (3) researchers experiencing publication pressure report a willingness to engage in scientific misconduct in the future.},
  langid = {english}
}

@article{Mutongoza2023,
  title = {Pressured to Perform: {{The}} Negative Consequences of the `Publish or Perish' Phenomenon among Junior Academics},
  shorttitle = {Pressured to Perform},
  author = {Mutongoza, Bonginkosi Hardy},
  year = {2023},
  month = aug,
  journal = {Scholarship of Teaching and Learning in the South},
  volume = {7},
  number = {2},
  pages = {46--62},
  issn = {2523-1154},
  doi = {10.36615/sotls.v7i2.301},
  urldate = {2024-08-10},
  abstract = {A growing body of work suggests that junior researchers in universities are often confronted by an unfathomable pressure to conduct research and get published in order to scale up the academic ladder. These pressures are often loaded with little to no regard for the welfare of the junior academics and no concern for the career paths to be taken. Against this background, this study explored the negative consequences associated with the pressure to publish from the unique perspective of junior academics at a rural university in South Africa. The study was underpinned by a qualitative research approach which enabled the utilisation of qualitative interviews with twelve junior academics from four faculties at the university. The findings demonstrated the often-salient bullying and abuse of junior academics that happens under the guise of mentorship from their senior colleagues. The study also revealed the cost at which the pressure to perform comes, namely the cost to mental well-being, the temptation to publish in predator journals, the rise of unethical publishing, and the sacrifice of quality research. Based on these findings, the study recommended that more considered efforts be made to secure the welfare of emerging academics and that more concerted efforts be instituted in universities to guard against the rise of academic bullying at the hands of senior academics.},
  copyright = {Copyright (c) 2023 Scholarship of Teaching and Learning in the South},
  langid = {english},
  keywords = {academia,bullying,emerging researchers,mentorship,psychosocial wellness,publishing}
}

@article{Higginson2016,
  title = {Current {{Incentives}} for {{Scientists Lead}} to {{Underpowered Studies}} with {{Erroneous Conclusions}}},
  author = {Higginson, Andrew D. and Munaf{\`o}, Marcus R.},
  year = {2016},
  month = nov,
  journal = {PLOS Biology},
  volume = {14},
  number = {11},
  pages = {e2000995},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2000995},
  urldate = {2024-08-10},
  abstract = {We can regard the wider incentive structures that operate across science, such as the priority given to novel findings, as an ecosystem within which scientists strive to maximise their fitness (i.e., publication record and career success). Here, we develop an optimality model that predicts the most rational research strategy, in terms of the proportion of research effort spent on seeking novel results rather than on confirmatory studies, and the amount of research effort per exploratory study. We show that, for parameter values derived from the scientific literature, researchers acting to maximise their fitness should spend most of their effort seeking novel results and conduct small studies that have only 10\%--40\% statistical power. As a result, half of the studies they publish will report erroneous conclusions. Current incentive structures are in conflict with maximising the scientific value of research; we suggest ways that the scientific ecosystem could be improved.},
  langid = {english},
  keywords = {Careers,Careers in research,Drug discovery,Ecosystems,Peer review,Research assessment,Research errors,Scientists}
}
