@article{Agnoli2017,
  title = {Questionable Research Practices among Italian Research Psychologists},
  author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
  year = {2017},
  journal = {PLOS ONE},
  volume = {12},
  number = {3},
  pages = {e0172792},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0172792},
  abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
  bdsk-url-2 = {http://dx.doi.org/10.1371/journal.pone.0172792}
}

@article{Allen2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  year = {2019},
  month = may,
  journal = {PLOS Biology},
  volume = {17},
  number = {5},
  pages = {e3000246},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  urldate = {2019-08-28},
  langid = {english}
}

@article{Altmejd2019,
  title = {Predicting the Replicability of Social Science Lab Experiments},
  author = {Altmejd, Adam and Dreber, Anna and Forsell, Eskil and Huber, Juergen and Imai, Taisuke and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Camerer, Colin},
  editor = {Wicherts, Jelte M.},
  year = {2019},
  month = dec,
  journal = {PLOS ONE},
  volume = {14},
  number = {12},
  pages = {e0225826},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0225826},
  urldate = {2020-09-11},
  langid = {english}
}

@article{Atkinson1982,
  title = {Statistical Significance, Reviewer Evaluations, and the Scientific Process: {{Is}} There a (Statistically) Significant Relationship?},
  shorttitle = {Statistical Significance, Reviewer Evaluations, and the Scientific Process},
  author = {Atkinson, Donald R. and Furlong, Michael J. and Wampold, Bruce E.},
  year = {1982},
  journal = {Journal of Counseling Psychology},
  volume = {29},
  number = {2},
  pages = {189--194},
  issn = {0022-0167},
  doi = {10.1037/0022-0167.29.2.189},
  urldate = {2020-01-05},
  langid = {english}
}

@article{Barclay2018,
  title = {State-Dependent Risk-Taking},
  author = {Barclay, Pat and Mishra, Sandeep and Sparks, Adam Maxwell},
  year = {2018},
  month = jun,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {285},
  number = {1881},
  pages = {20180180},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2018.0180},
  urldate = {2022-03-23},
  abstract = {Who takes risks, and when? The               relative state model               proposes two non-independent selection pressures governing risk-taking: need-based and ability-based. The need-based account suggests that actors take risks when they cannot reach target states with low-risk options (consistent with risk-sensitivity theory). The ability-based account suggests that actors engage in risk-taking when they possess traits or abilities that increase the expected value of risk-taking (by increasing the probability of success, enhancing payoffs for success or buffering against failure). Adaptive risk-taking involves integrating both considerations. Risk-takers compute the expected value of risk-taking based on their               state               ---the interaction of embodied capital relative to one's situation, to the same individual in other circumstances or to other individuals. We provide mathematical support for this dual pathway model, and show that it can predict who will take the most risks and when (e.g. when risk-taking will be performed by those in good, poor, intermediate or extreme state only). Results confirm and elaborate on the initial verbal model of state-dependent risk-taking: selection favours agents who calibrate risk-taking based on implicit computations of condition and/or competitive (dis)advantage, which in turn drives patterned individual differences in risk-taking behaviour.},
  langid = {english}
}

@article{Begley2012,
  title = {Raise Standards for Preclinical Cancer Research},
  author = {Begley, C. Glenn and Ellis, Lee M.},
  year = {2012},
  month = mar,
  journal = {Nature},
  volume = {483},
  number = {7391},
  pages = {531--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/483531a},
  urldate = {2022-06-25},
  abstract = {C. Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit.},
  copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Cancer,Drug development}
}

@article{Boschen2023,
  title = {Changes in Methodological Study Characteristics in Psychology between 2010-2021},
  author = {B{\"o}schen, Ingmar},
  year = {2023},
  month = may,
  journal = {PLOS ONE},
  volume = {18},
  number = {5},
  pages = {e0283353},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0283353},
  urldate = {2023-10-01},
  abstract = {In 2015, the Open Science Collaboration repeated a series of 100 psychological experiments. Since a considerable part of these replications could not confirm the original effects and some of them pointed in the opposite direction, psychological research is said to lack reproducibility. Several general criticisms can explain this finding, such as the standardized use of undirected nil-null hypothesis tests, samples being too small and selective, lack of corrections for multiple testing, but also some widespread questionable research practices and incentives to publish positive results only. A selection of 57,909 articles from 12 renowned journals is processed with the JATSdecoder software to analyze the extent to which several empirical research practices in psychology have changed over the past 12 years. To identify journal- and time-specific changes, the relative use of statistics based on p-values, the number of reported p-values per paper, the relative use of confidence intervals, directed tests, power analysis, Bayesian procedures, non-standard {$\alpha$} levels, correction procedures for multiple testing, and median sample sizes are analyzed for articles published between 2010 and 2015 and after 2015, and in more detail for every included journal and year of publication. In addition, the origin of authorships is analyzed over time. Compared to articles that were published in and before 2015, the median number of reported p-values per article has decreased from 14 to 12, whereas the median proportion of significant p-values per article remained constant at 69\%. While reports of effect sizes and confidence intervals have increased, the {$\alpha$} level is usually set to the default value of .05. The use of corrections for multiple testing has decreased. Although uncommon in each case (4\% in total), directed testing is used less frequently, while Bayesian inference has become more common after 2015. The overall median estimated sample size has increased from 105 to 190.},
  langid = {english},
  keywords = {Anxiety,Behavioral neuroscience,Cognitive neuroscience,Depression,Experimental psychology,Mental health and psychiatry,Psychology,Social psychology}
}

@article{Brand2022,
  title = {{{THE MAINTENANCE RACE}}},
  author = {Brand, Stewart},
  year = {2022},
  month = jul,
  journal = {Works in Progress},
  number = {8},
  urldate = {2022-07-25},
  abstract = {The world's first round-the-world solo yacht race was a thrilling and, for some, deadly contest. How its participants maintained their vessels can help us understand just how fundamental maintenance is.},
  langid = {american}
}

@article{Butler2007,
  title = {Assessing University Research: {{A}} Plea for a Balanced Approach},
  shorttitle = {Assessing University Research},
  author = {Butler, Linda},
  year = {2007},
  month = oct,
  journal = {Science and Public Policy},
  volume = {34},
  number = {8},
  pages = {565--574},
  publisher = {Oxford Academic},
  issn = {0302-3427},
  doi = {10.3152/030234207X254404},
  urldate = {2022-02-14},
  abstract = {Abstract. The use of quantitative performance measures to assess the quality of university research is being introduced in Australia and the UK. This paper pres},
  langid = {english}
}

@article{Camerer2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  urldate = {2019-02-19},
  abstract = {Camerer et al. carried out replications of 21 Science and Nature social science experiments, successfully replicating 13 out of 21 (62\%). Effect sizes of replications were about half of the size of the originals.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {meta-science,prediction markets,prior elicitation,prior probability,replicability,replication crisis}
}

@article{Chalmers2009,
  title = {Avoidable Waste in the Production and Reporting of Research Evidence},
  author = {Chalmers, Iain and Glasziou, Paul},
  year = {2009},
  month = jul,
  journal = {The Lancet},
  volume = {374},
  number = {9683},
  pages = {86--89},
  publisher = {Elsevier},
  issn = {0140-6736, 1474-547X},
  doi = {10.1016/S0140-6736(09)60329-9},
  urldate = {2022-07-24},
  langid = {english},
  pmid = {19525005}
}

@article{Chambers2013,
  title = {Registered Reports: {{A}} New Publishing Initiative at {{Cortex}}},
  author = {Chambers, C. D.},
  year = {2013},
  journal = {Cortex},
  volume = {49},
  pages = {606--610},
  doi = {10.1016/j.cortex.2012.12.016}
}

@article{Chambers2015,
  title = {Registered {{Reports}}: {{Realigning}} Incentives in Scientific Publishing},
  author = {Chambers, C. D. and Dienes, Z. and McIntosh, R.D. and Rotshtein, P. and Willmes, K.},
  year = {2015},
  journal = {Cortex},
  volume = {66},
  pages = {1--2},
  issn = {19738102},
  doi = {10.1016/j.cortex.2015.03.022},
  abstract = {This editorial present views on realigning incentives in scientific publishing. As editors recognize this important moment for Cortex, they also take the opportunity to reiterate our view that Registered Reports should not be seen as a one-shot cure for reproducibility problems in science. The applicability of Registered Reports to different sub-fields within neuropsychology and cognitive neuroscience remains to be established; for instance, studies that rely exclusively on exploration rather than deductive hypothesis testing may not be compatible. Registered Reports present no threat to exploratory science in cases where studies include a mixture of both hypothesis testing and exploratory analyses, authors are welcome to report the outcomes of the unregistered analyses, as Sassenhagen and Bornkessel-Schlesewsky do in the current issue. Pre-registration simply allows readers to distinguish the outcomes based on a priori hypothesis testing from post hoc exploration. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  pmid = {25892410}
}

@article{Chambers2021,
  title = {The Past, Present and Future of {{Registered Reports}}},
  author = {Chambers, C. D. and Tzavella, Loukia},
  year = {2021},
  month = nov,
  journal = {Nature Human Behaviour},
  pages = {1--14},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01193-7},
  urldate = {2021-11-29},
  abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Culture,Publishing},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Reviews\\
Subject\_term: Culture;Publishing\\
Subject\_term\_id: culture;publishing}
}

@article{Clemen2000,
  title = {Assessing {{Dependence}}: {{Some Experimental Results}}},
  shorttitle = {Assessing {{Dependence}}},
  author = {Clemen, Robert T. and Fischer, Gregory W. and Winkler, Robert L.},
  year = {2000},
  month = aug,
  journal = {Management Science},
  volume = {46},
  number = {8},
  pages = {1100--1115},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.46.8.1100.12023},
  urldate = {2019-02-25},
  langid = {english},
  keywords = {correlation,expert elicitation,prior elicitation,prior probability,stats}
}

@article{Cristea2018,
  title = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant: {{A}} Survey of Top Science Journals},
  shorttitle = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant},
  author = {Cristea, Ioana Alina and Ioannidis, John P. A.},
  year = {2018},
  month = may,
  journal = {PLOS ONE},
  volume = {13},
  number = {5},
  pages = {e0197440},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0197440},
  urldate = {2019-10-01},
  abstract = {P values represent a widely used, but pervasively misunderstood and fiercely contested method of scientific inference. Display items, such as figures and tables, often containing the main results, are an important source of P values. We conducted a survey comparing the overall use of P values and the occurrence of significant P values in display items of a sample of articles in the three top multidisciplinary journals (Nature, Science, PNAS) in 2017 and, respectively, in 1997. We also examined the reporting of multiplicity corrections and its potential influence on the proportion of statistically significant P values. Our findings demonstrated substantial and growing reliance on P values in display items, with increases of 2.5 to 14.5 times in 2017 compared to 1997. The overwhelming majority of P values (94\%, 95\% confidence interval [CI] 92\% to 96\%) were statistically significant. Methods to adjust for multiplicity were almost non-existent in 1997, but reported in many articles relying on P values in 2017 (Nature 68\%, Science 48\%, PNAS 38\%). In their absence, almost all reported P values were statistically significant (98\%, 95\% CI 96\% to 99\%). Conversely, when any multiplicity corrections were described, 88\% (95\% CI 82\% to 93\%) of reported P values were statistically significant. Use of Bayesian methods was scant (2.5\%) and rarely (0.7\%) articles relied exclusively on Bayesian statistics. Overall, wider appreciation of the need for multiplicity corrections is a welcome evolution, but the rapid growth of reliance on P values and implausibly high rates of reported statistical significance are worrisome.},
  langid = {english},
  keywords = {Analysis of variance,Bayesian method,Bayesian statistics,Computer software,Meta-analysis,Scientific publishing,Software tools,Statistical data}
}

@article{Csada1996,
  title = {The "{{File Drawer Problem}}" of {{Non-Significant Results}}: {{Does It Apply}} to {{Biological Research}}?},
  shorttitle = {The "{{File Drawer Problem}}" of {{Non-Significant Results}}},
  author = {Csada, Ryan D. and James, Paul C. and Espie, Richard H. M.},
  year = {1996},
  journal = {Oikos},
  volume = {76},
  number = {3},
  eprint = {3546355},
  eprinttype = {jstor},
  pages = {591--593},
  publisher = {[Nordic Society Oikos, Wiley]},
  issn = {0030-1299},
  doi = {10.2307/3546355},
  urldate = {2021-03-01},
  abstract = {We show that there appears to be a publication bias against non-significant results in the biological literature. We suggest reasons why non-significant results are not published, the implications of not publishing non-significant results, and why we need to correct the problem.}
}

@article{deVries2018,
  title = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments: The Case of Depression},
  shorttitle = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments},
  author = {{de Vries}, Y. A. and Roest, A. M. and de Jonge, P. and Cuijpers, P. and Munaf{\`o}, M. R. and Bastiaansen, J. A.},
  year = {2018},
  month = nov,
  journal = {Psychological Medicine},
  volume = {48},
  number = {15},
  pages = {2453--2455},
  publisher = {Cambridge University Press},
  issn = {0033-2917, 1469-8978},
  doi = {10.1017/S0033291718001873},
  urldate = {2022-06-27},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0033291718001873/resource/name/firstPage-S0033291718001873a.jpg},
  langid = {english},
  keywords = {Antidepressants,bias,citation bias,depression,psychotherapy,reporting bias}
}

@article{Dickersin1990,
  title = {The Existence of Publication Bias and Risk Factors for Its Occurrence},
  author = {Dickersin, K.},
  year = {1990},
  month = mar,
  journal = {JAMA: The Journal of the American Medical Association},
  volume = {263},
  number = {10},
  pages = {1385--1389},
  issn = {00987484, 15383598},
  doi = {10.1001/jama.263.10.1385},
  urldate = {2019-02-19},
  langid = {english},
  keywords = {meta-science,preregistration,publication bias}
}

@article{Dickersin1993,
  title = {Publication Bias: The Problem That Won't Go Away},
  shorttitle = {Publication Bias},
  author = {Dickersin, K. and Min, Y. I.},
  year = {1993},
  month = dec,
  journal = {Annals of the New York Academy of Sciences},
  volume = {703},
  pages = {135-146; discussion 146-148},
  issn = {0077-8923},
  doi = {10.1111/j.1749-6632.1993.tb26343.x},
  abstract = {Conclusions about the efficacy and safety of medical interventions are based on data presented in the scientific literature. The validity of these conclusions is threatened if publication bias results from investigators or editors making decisions about publishing study results on the basis of the direction or strength of the study findings. This paper reports meta-analyses performed using data from four prospective investigations in which a total of 997 initiated studies were followed to learn of study results, publication status, and reasons for nonpublication. The analysis indicates that there is a positive association between "significant" study results and publication (OR = 2.88; 95\% confidence interval [CI] 2.13 to 3.90). When the analysis was restricted to controlled trials (n = 280), an even stronger relationship between "significant" results and publication was observed (OR = 6.15; 95\% CI 2.24 to 16.92), with randomized trials (n = 200) apparently no less susceptible to publication bias than controlled trials in general (OR = 8.72; 95\% CI 1.91 to 39.81). In every case, failure to publish was investigator-based, and not due to editorial decisions. The results of clinical trials should not be suppressed in this way. Development of registration systems for randomized trials is essential if this problem is to be minimized in future.},
  langid = {english},
  pmid = {8192291},
  keywords = {Bias,Clinical Trials as Topic,Confidence Intervals,Female,Humans,Logistic Models,Male,Odds Ratio,Professional Staff Committees,Prospective Studies,Publishing,Registries,Reproducibility of Results,Research Design,Research Support as Topic}
}

@incollection{Dickersin2005,
  title = {Publication {{Bias}}: {{Recognizing}} the {{Problem}}, {{Understanding Its Origins}} and {{Scope}}, and {{Preventing Harm}}},
  shorttitle = {Publication {{Bias}}},
  booktitle = {Publication {{Bias}} in {{Meta-Analysis}}},
  author = {Dickersin, Kay},
  year = {2005},
  pages = {9--33},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/0470870168.ch2},
  urldate = {2022-02-27},
  abstract = {This chapter contains sections titled: Background Early Evidence of Publication Bias Is Publication Bias Important? Improved Understanding of the Process of Research Dissemination and Publication Other Factors Associated with Failure to Publish and Direction of Results Biased Reporting of Outcomes Selective Citation of Positive Results Solutions to Publication Bias Conclusions Acknowledgements References},
  chapter = {2},
  isbn = {978-0-470-87016-7},
  langid = {english},
  keywords = {biased reporting of outcomes,from study start to dissemination of results,JAMA - Negative Results,publication bias and recognition,publication bias in meta-analysis,publication bias significance,systematic review problem method,systematic reviews and meta-analysis}
}

@article{Dreber2015,
  title = {Using Prediction Markets to Estimate the Reproducibility of Scientific Research},
  author = {Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus},
  year = {2015},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {50},
  pages = {15343--15347},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1516179112},
  urldate = {2019-02-19},
  langid = {english},
  keywords = {meta-science,prediction markets,prior elicitation,prior probability,replicability,replication crisis}
}

@article{Duc2020,
  title = {Impacts of {{Novel Vietnamese Government Regulations}} on {{Radiological PhD}} and {{Professorship Candidates}}: An {{Initial Report}}},
  shorttitle = {Impacts of {{Novel Vietnamese Government Regulations}} on {{Radiological PhD}} and {{Professorship Candidates}}},
  author = {Duc, Nguyen Minh and Ha, Hoang Duc and Khoa, Mai Trong and Thong, Pham Minh},
  year = {2020},
  month = jun,
  journal = {Acta Informatica Medica},
  volume = {28},
  number = {2},
  pages = {152--156},
  issn = {0353-8109},
  doi = {10.5455/aim.2020.28.152-156},
  urldate = {2024-07-17},
  abstract = {Introduction: In Vietnam, the successful publication of research in indexed journals is mandatory to obtain academic appointments and promotions in medical colleges and institutions, according to the current guidelines established by the State Council for Professorship and Ministry of Education and Training. Aim: This study aimed to investigate the impacts of novel Vietnamese government regulations on radiological PhD and professorship candidates. Methods: This study evaluated freely accessible data, available online, and, therefore, did not require institutional review board approval. We assessed the numbers of radiological PhD candidates at Hanoi Medical University and the numbers of published Vietnamese radiological papers, from 2012 to 2019, indexed in the SCImago database. In addition, we evaluated the numbers of qualified radiological professors and associate professors employed at universities during the same period. We did not include nuclear medicine PhD and professorship candidates, in this study. The data are presented as bar and line charts. Results: Following the enactment of 08/2017/TT-BGD{\DJ}T and 37/2018/Q{\DJ}-TTg, we observed that the numbers of radiological PhD and professorship candidates were significantly reduced. From 2012 to 2019, only one candidate qualified for appointment as a radiological professor. However, the number of radiological papers rose dramatically during the same time period. Conclusion: The enactment of 08/2017/TT-BGD{\DJ}T and 37/2018/Q{\DJ}-TTg had strong impacts on the numbers of PhD and professorship candidates. Owing to these new regulations, the number of published, international, peer-reviewed radiological papers has increased; however, some undesired consequences may have occurred, such as papers being published in predatory or suspected predatory journals, double or triple submissions, and plagiarism.},
  pmcid = {PMC7382773},
  pmid = {32742070}
}

@article{Fanelli2010,
  title = {"{{Positive}}" Results Increase down the Hierarchy of the Sciences},
  author = {Fanelli, Daniele},
  editor = {Scalas, Enrico},
  year = {2010},
  month = apr,
  journal = {PLoS ONE},
  volume = {5},
  number = {4},
  pages = {e10068},
  issn = {19326203},
  doi = {10.1371/journal.pone.0010068},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the "hardness" of scientific research--i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors--is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a "positive" (full or partial) or "negative" support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in "softer" sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  pmid = {20383332}
}

@article{Fanelli2012,
  title = {Negative Results Are Disappearing from Most Disciplines and Countries},
  author = {Fanelli, Daniele},
  year = {2012},
  month = mar,
  journal = {Scientometrics},
  volume = {90},
  number = {3},
  pages = {891--904},
  issn = {01389130},
  doi = {10.1007/s11192-011-0494-7},
  abstract = {Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have ``tested'' a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22\% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.},
  keywords = {Bias,Competition,Misconduct,Publication,Publish or perish,Research evaluation}
}

@article{Ferguson2012,
  title = {A {{Vast Graveyard}} of {{Undead Theories}}: {{Publication Bias}} and {{Psychological Science}}'s {{Aversion}} to the {{Null}}},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  year = {2012},
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {555--561},
  issn = {17456916},
  doi = {10.1177/1745691612459059},
  abstract = {Publication bias remains a controversial issue in psychological science. The tendency of psychological science to avoid publishing null results produces a situation that limits the replicability assumption of science, as replication cannot be meaningful without the potential acknowledgment of failed replications. We argue that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science's capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous ``undead'' theories that are ideologically popular but have little basis in fact.},
  pmid = {26168112},
  keywords = {fail-safe number,falsification,meta-analyses,null hypothesis significance testing,publication bias}
}

@article{Fiedler2016,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  year = {2016},
  month = jan,
  journal = {Social Psychological and Personality Science},
  volume = {7},
  number = {1},
  pages = {45--52},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615612150},
  urldate = {2019-09-23},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  langid = {english}
}

@article{Forsell2018,
  title = {Predicting Replication Outcomes in the {{Many Labs}} 2 Study},
  author = {Forsell, Eskil and Viganola, Domenico and Pfeiffer, Thomas and Almenberg, Johan and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus and Dreber, Anna},
  year = {2018},
  month = oct,
  journal = {Journal of Economic Psychology},
  issn = {0167-4870},
  doi = {10.1016/j.joep.2018.10.009},
  urldate = {2019-02-19},
  abstract = {Understanding and improving reproducibility is crucial for scientific progress. Prediction markets and related methods of eliciting peer beliefs are promising tools to predict replication outcomes. We invited researchers in the field of psychology to judge the replicability of 24 studies replicated in the large scale Many Labs 2 project. We elicited peer beliefs in prediction markets and surveys about two replication success metrics: the probability that the replication yields a statistically significant effect in the original direction (p\,{$<$}\,0.001), and the relative effect size of the replication. The prediction markets correctly predicted 75\% of the replication outcomes, and were highly correlated with the replication outcomes. Survey beliefs were also significantly correlated with replication outcomes, but had larger prediction errors. The prediction markets for relative effect sizes attracted little trading and thus did not work well. The survey beliefs about relative effect sizes performed better and were significantly correlated with observed relative effect sizes. The results suggest that replication outcomes can be predicted and that the elicitation of peer beliefs can increase our knowledge about scientific reproducibility and the dynamics of hypothesis testing.},
  keywords = {Beliefs,ManyLabs,meta-science,prediction markets,Prediction markets,prior elicitation,prior probability,replicability,replication crisis,Replications,Reproducibility}
}

@article{Franco2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, Annie and Malhotra, N. and Simonovits, G.},
  year = {2014},
  journal = {Science},
  volume = {345},
  number = {6203},
  pages = {1502--1505},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255484},
  urldate = {2018-06-19},
  langid = {english},
  keywords = {publication bias,replication crisis}
}

@article{Franco2016,
  title = {Underreporting in {{Psychology Experiments}}: {{Evidence From}} a {{Study Registry}}},
  shorttitle = {Underreporting in {{Psychology Experiments}}},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  year = {2016},
  journal = {Social Psychological and Personality Science},
  volume = {7},
  number = {1},
  pages = {8--12},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615598377},
  urldate = {2018-06-19},
  langid = {english},
  keywords = {publication bias,replication crisis}
}

@article{Fraser2018,
  title = {Questionable Research Practices in Ecology and Evolution},
  author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
  year = {2018},
  month = jul,
  journal = {PLOS ONE},
  volume = {13},
  number = {7},
  pages = {e0200303},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0200303},
  urldate = {2021-01-21},
  abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
  langid = {english},
  keywords = {Behavioral ecology,Community ecology,Evolutionary biology,Evolutionary ecology,Evolutionary rate,Psychology,Publication ethics,Statistical data}
}

@article{Garthwaite2005,
  title = {Statistical {{Methods}} for {{Eliciting Probability Distributions}}},
  author = {Garthwaite, Paul H and Kadane, Joseph B and O'Hagan, Anthony},
  year = {2005},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {470},
  pages = {680--701},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214505000000105},
  urldate = {2019-04-24},
  langid = {english}
}

@article{Goldman1991,
  title = {An Economic Model of Scientific Activity and Truth Acquisition},
  author = {Goldman, Alvin I. and Shaked, Moshe},
  year = {1991},
  month = jul,
  journal = {Philosophical Studies},
  volume = {63},
  number = {1},
  pages = {31--55},
  issn = {0031-8116, 1573-0883},
  doi = {10.1007/BF00375996},
  urldate = {2022-01-26},
  langid = {english}
}

@article{Gopalakrishna2022,
  title = {Prevalence of Responsible Research Practices among Academics in {{The Netherlands}}},
  author = {Gopalakrishna, Gowri and Wicherts, Jelte M. and Vink, Gerko and Stoop, Ineke and van den Akker, Olmo R. and ter Riet, Gerben and Bouter, Lex M.},
  year = {2022},
  month = aug,
  volume = {11},
  number = {471},
  doi = {10.12688/f1000research.110664.2},
  urldate = {2023-01-10},
  abstract = {Background: Traditionally, research integrity studies have focused on research misbehaviors and their explanations. Over time, attention has shifted towards preventing questionable research practices and promoting responsible ones. However, data on the prevalence of responsible research practices, especially open methods, open codes and open data and their underlying associative factors, remains scarce. Methods: We conducted a web-based anonymized questionnaire, targeting all academic researchers working at or affiliated to a university or university medical center in The Netherlands, to investigate the prevalence and potential explanatory factors of 11 responsible research practices. Results: A total of 6,813 academics completed the survey, the results of which show that prevalence of responsible practices differs substantially across disciplines and ranks, with 99 percent avoiding plagiarism in their work but less than 50 percent pre-registering a research protocol. Arts and humanities scholars as well as PhD candidates and junior researchers engaged less often in responsible research practices. Publication pressure negatively affected responsible practices, while mentoring, scientific norms subscription and funding pressure stimulated them. Conclusions: Understanding the prevalence of responsible research practices across disciplines and ranks, as well as their associated explanatory factors, can help to systematically address disciplinary- and academic rank-specific obstacles, and thereby facilitate responsible conduct of research.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Open science,Research integrity,Responsible conduct of research,Responsible research practices}
}

@article{Greenwald1975,
  title = {Consequences of {{Prejudice Against}} the {{Null Hypothesis}}},
  author = {Greenwald, Anthony G.},
  year = {1975},
  journal = {Psychological Bulletin},
  volume = {82},
  number = {1},
  pages = {1--20}
}

@article{Gross2021,
  title = {Why Ex Post Peer Review Encourages High-Risk Research While Ex Ante Review Discourages It},
  author = {Gross, Kevin and Bergstrom, Carl T.},
  year = {2021},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {51},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2111615118},
  urldate = {2022-01-18},
  abstract = {Peer review is an integral component of contemporary science. While peer review focuses attention on promising and interesting science, it also encourages scientists to pursue some questions at the expense of others. Here, we use ideas from forecasting assessment to examine how two modes of peer review---ex ante review of proposals for future work and ex post review of completed science---motivate scientists to favor some questions instead of others. Our main result is that ex ante and ex post peer review push investigators toward distinct sets of scientific questions. This tension arises because ex post review allows investigators to leverage their own scientific beliefs to generate results that others will find surprising, whereas ex ante review does not. Moreover, ex ante review will favor different research questions depending on whether reviewers rank proposals in anticipation of changes to their own personal beliefs or to the beliefs of their peers. The tension between ex ante and ex post review puts investigators in a bind because most researchers need to find projects that will survive both. By unpacking the tension between these two modes of review, we can understand how they shape the landscape of science and how changes to peer review might shift scientific activity in unforeseen directions.},
  chapter = {Social Sciences},
  copyright = {{\copyright} 2021 . https://www-pnas-org.vu-nl.idm.oclc.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {34921115},
  keywords = {Bayesian reasoning,decision theory,information theory,peer review,philosophy of science}
}

@article{Haaland2019,
  title = {Bet-Hedging across Generations Can Affect the Evolution of Variance-Sensitive Strategies within Generations},
  author = {Haaland, Thomas R. and Wright, Jonathan and Ratikainen, Irja I.},
  year = {2019},
  month = dec,
  journal = {Proceedings of the Royal Society B},
  publisher = {The Royal Society},
  doi = {10.1098/rspb.2019.2070},
  urldate = {2021-10-08},
  abstract = {In order to understand how organisms cope with ongoing changes in environmental variability, it is necessary to consider multiple adaptations to environmental uncertainty on different time scales. Conservative bet-hedging (CBH) represents a long-term ...},
  copyright = {{\copyright} 2019 The Authors.},
  langid = {english}
}

@article{Hardwicke2018,
  title = {Mapping the Universe of Registered Reports},
  author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
  year = {2018},
  month = oct,
  journal = {Nature Human Behaviour},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0444-y},
  keywords = {journal policy,meta-research,open science,pre-registration,Registered Reports}
}

@article{Hemming2018,
  title = {A Practical Guide to Structured Expert Elicitation Using the {{IDEA}} Protocol},
  author = {Hemming, Victoria and Burgman, Mark A. and Hanea, Anca M. and McBride, Marissa F. and Wintle, Bonnie C.},
  year = {2018},
  journal = {Methods in Ecology and Evolution},
  volume = {9},
  number = {1},
  pages = {169--180},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12857},
  urldate = {2019-02-25},
  abstract = {Expert judgement informs a variety of important applications in conservation and natural resource management, including threatened species management, environmental impact assessment and structured decision-making. However, expert judgements can be prone to contextual biases. Structured elicitation protocols mitigate these biases, and improve the accuracy and transparency of the resulting judgements. Despite this, the elicitation of expert judgement within conservation and natural resource management remains largely informal. We suggest this may be attributed to financial and practical constraints, which are not addressed by many existing structured elicitation protocols. In this paper, we advocate that structured elicitation protocols must be adopted when expert judgements are used to inform science. In order to motivate a wider adoption of structured elicitation protocols, we outline the IDEA protocol. The protocol improves the accuracy of expert judgements and includes several key steps which may be familiar to many conservation researchers, such as the four-step elicitation, and a modified Delphi procedure (``Investigate,'' ``Discuss,'' ``Estimate'' and ``Aggregate''). It can also incorporate remote elicitation, making structured expert judgement accessible on a modest budget. The IDEA protocol has recently been outlined in the scientific literature; however, a detailed description has been missing. This paper fills that important gap by clearly outlining each of the steps required to prepare for and undertake an elicitation. While this paper focuses on the need for the IDEA protocol within conservation and natural resource management, the protocol (and the advice contained in this paper) is applicable to a broad range of scientific domains, as evidenced by its application to biosecurity, engineering and political forecasting. By clearly outlining the IDEA protocol, we hope that structured protocols will be more widely understood and adopted, resulting in improved judgements and increased transparency when expert judgement is required.},
  copyright = {{\copyright} 2017 The Authors. Methods in Ecology and Evolution {\copyright} 2017 British Ecological Society},
  langid = {english},
  keywords = {Delphi,expert elicitation,forecasting,four-step elicitation,IDEA protocol,prior elicitation,prior probability,quantitative estimates,structured expert judgement}
}

@article{Hoogeveen2020,
  title = {Laypeople {{Can Predict Which Social-Science Studies Will Be Replicated Successfully}}:},
  shorttitle = {Laypeople {{Can Predict Which Social-Science Studies Will Be Replicated Successfully}}},
  author = {Hoogeveen, Suzanne and Sarafoglou, Alexandra and Wagenmakers, Eric-Jan},
  year = {2020},
  month = aug,
  journal = {Advances in Methods and Practices in Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  doi = {10.1177/2515245920919667},
  urldate = {2020-09-11},
  abstract = {Large-scale collaborative projects recently demonstrated that several key findings from the social-science literature could not be replicated successfully. Here...},
  copyright = {{\copyright} The Author(s) 2020},
  langid = {english}
}

@article{Hurly2003,
  title = {The Twin Threshold Model: Risk-Intermediate Foraging by Rufous Hummingbirds, {{Selasphorus}} Rufus},
  shorttitle = {The Twin Threshold Model},
  author = {Hurly, Andrew T.},
  year = {2003},
  month = oct,
  journal = {Animal Behaviour},
  volume = {66},
  number = {4},
  pages = {751--761},
  issn = {0003-3472},
  doi = {10.1006/anbe.2003.2278},
  urldate = {2022-03-05},
  abstract = {I developed two versions of the twin threshold model (TTM) to assess risk-sensitive foraging decisions by rufous hummingbirds. The model incorporates energy thresholds for both starvation and reproduction and assesses how three reward distributions with a common mean but different levels of variance interact with these critical thresholds to determine fitness. Fitness, a combination of survival and reproduction, is influenced by both the amount of variance in the distributions and the relative position of the common mean between the thresholds. The model predicts that risk-intermediate foraging is often the optimal policy, and that risk aversion is favoured as the common mean of the distributions approaches the starvation threshold, whereas risk preference is favoured as the common mean approaches the reproduction threshold. Tests with free-living hummingbirds supported these predictions. Hummingbirds were presented with three distributions of nectar rewards that had a common mean but Nil, Moderate or High levels of variance. Birds preferred intermediate levels of variance (Moderate) when presented with all three rewards simultaneously, and became more risk-averse as the mean of the distributions was decreased but more risk-prone as the mean was increased. Birds preferred Nil when it was paired with Moderate or with High, but preferred Moderate in the presence of Nil and High together. This reversal of preference is a violation of regularity, conventionally interpreted as irrational choice behaviour. I provide an alternative version of the TTM demonstrating that violations of regularity can occur when relative instead of absolute evaluation mechanisms are used.},
  langid = {english}
}

@article{Ingre2018,
  title = {Estimating Statistical Power, Posterior Probability and Publication Bias of Psychological Research Using the Observed Replication Rate},
  author = {Ingre, Michael and Nilsonne, Gustav},
  year = {2018},
  month = sep,
  journal = {Royal Society Open Science},
  volume = {5},
  number = {9},
  pages = {181190},
  issn = {2054-5703, 2054-5703},
  doi = {10.1098/rsos.181190},
  urldate = {2019-08-28},
  langid = {english}
}

@article{John2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  journal = {Psychological Science},
  volume = {23},
  number = {5},
  pages = {524--532},
  issn = {14679280},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  pmid = {22508865},
  keywords = {disclosure,judgment,methodology,professional standards}
}

@article{Johnson2010,
  title = {A Valid and Reliable Belief Elicitation Method for {{Bayesian}} Priors},
  author = {Johnson, Sindhu R. and Tomlinson, George A. and Hawker, Gillian A. and Granton, John T. and Grosbein, Haddas A. and Feldman, Brian M.},
  year = {2010},
  month = apr,
  journal = {Journal of Clinical Epidemiology},
  volume = {63},
  number = {4},
  pages = {370--383},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2009.08.005},
  urldate = {2019-02-25},
  abstract = {Objective Bayesian inference has the advantage of formally incorporating prior beliefs about the effect of an intervention into analyses of treatment effect through the use of prior probability distributions or ``priors.'' Multiple methods to elicit beliefs from experts for inclusion in a Bayesian study have been used; however, the measurement properties of these methods have been infrequently evaluated. The objectives of this study were to evaluate the feasibility, validity, and reliability of a belief elicitation method for Bayesian priors. Study Design and Setting A single-center, cross-sectional study using a sample of academic specialists who treat pulmonary hypertension patients was conducted to test the feasibility, face and construct validity, and reliability of a belief elicitation method. Using this method, participants expressed the probability of 3-year survival with and without warfarin. Applying adhesive dots or ``chips,'' each representing 5\% probability, in ``bins'' on a line, participants expressed their uncertainty and weight of belief about the effect of warfarin on 3-year survival. Results Of the 12 participants, 11 (92\%) reported that the belief elicitation method had face validity, 10 (83\%) found the questions clear, and 11 (92\%) found the response option easy to use. The median time to completion was 10 minutes (5--15 minutes). Internal validity testing found moderate agreement (weighted kappa=0.54--0.57). The intraclass correlation coefficient for test--retest reliability was 0.93. Conclusion This method of belief elicitation for Bayesian priors is feasible, valid, and reliable. It can be considered for application in Bayesian clinical studies.},
  keywords = {Bayes,Bayesian,Belief elicitation,expert elicitation,prior elicitation,prior probability,Priors,Pulmonary hypertension,Reliability,stats,Validity}
}

@article{Jonas2016,
  title = {How Can Preregistration Contribute to Research in Our Field?},
  author = {Jonas, Kai J. and Cesario, Joseph},
  year = {2016},
  journal = {Comprehensive Results in Social Psychology},
  volume = {1},
  number = {1-3},
  pages = {1--7},
  issn = {2374-3603, 2374-3611},
  doi = {10.1080/23743603.2015.1070611},
  urldate = {2018-05-23},
  langid = {english}
}

@article{Kacelnik1996,
  title = {Risky {{Theories}}---{{The Effects}} of {{Variance}} on {{Foraging Decisions}}},
  author = {Kacelnik, Alex and Bateson, Melissa},
  year = {1996},
  month = sep,
  journal = {Integrative and Comparative Biology},
  volume = {36},
  number = {4},
  pages = {402--434},
  publisher = {Oxford Academic},
  issn = {1540-7063},
  doi = {10.1093/icb/36.4.402},
  urldate = {2022-03-23},
  abstract = {Abstract. This paper concerns the response of foraging animals to variability in rate of gain, or risk. Both the empirical and theoretical literatures relevant},
  langid = {english}
}

@article{Kacelnik1997,
  title = {Risk-Sensitivity: Crossroads for Theories of Decision-Making.},
  shorttitle = {Risk-Sensitivity},
  author = {Kacelnik, Alex and Bateson, M.},
  year = {1997},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {1},
  number = {8},
  pages = {304--309},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/s1364-6613(97)01093-0},
  urldate = {2021-04-28},
  abstract = {Europe PMC is an archive of life sciences journal literature., Risk-sensitivity: crossroads for theories of decision-making.},
  langid = {english},
  pmid = {21223933}
}

@article{Kaplan2015,
  title = {Likelihood of {{Null Effects}} of {{Large NHLBI Clinical Trials Has Increased}} over {{Time}}},
  author = {Kaplan, Robert M. and Irvin, Veronica L.},
  editor = {Garattini, Silvio},
  year = {2015},
  month = aug,
  journal = {PLOS ONE},
  volume = {10},
  number = {8},
  pages = {e0132382},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0132382},
  urldate = {2019-08-28},
  langid = {english}
}

@article{Kerr1998,
  title = {{{HARKing}}: {{Hypothesizing}} after the Results Are Known},
  author = {Kerr, Norbert L.},
  year = {1998},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  issn = {10888683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  pmid = {15647155}
}

@article{Lakens2019b,
  title = {{The value of preregistration for psychological science: A conceptual analysis}},
  shorttitle = {{The value of preregistration for psychological science}},
  author = {Lakens, Dani{\"e}l},
  year = {2019},
  journal = {Japanese Psychological Review},
  volume = {62},
  number = {3},
  pages = {221--230},
  publisher = {},
  issn = {0386-1058, 2433-4650},
  doi = {10.24602/sjpr.62.3_221},
  urldate = {2021-04-18},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish  {\dots}},
  langid = {japanese}
}

@article{Landy2020,
  title = {Crowdsourcing Hypothesis Tests: {{Making}} Transparent How Design Choices Shape Research Results.},
  shorttitle = {Crowdsourcing Hypothesis Tests},
  author = {Landy, Justin F. and Jia, Miaolei (Liam) and Ding, Isabel L. and Viganola, Domenico and Tierney, Warren and Dreber, Anna and Johannesson, Magnus and Pfeiffer, Thomas and Ebersole, Charles R. and Gronau, Quentin F. and Ly, Alexander and {van den Bergh}, Don and Marsman, Maarten and Derks, Koen and Wagenmakers, Eric-Jan and Proctor, Andrew and Bartels, Daniel M. and Bauman, Christopher W. and Brady, William J. and Cheung, Felix and Cimpian, Andrei and Dohle, Simone and Donnellan, M. Brent and Hahn, Adam and Hall, Michael P. and {Jim{\'e}nez-Leal}, William and Johnson, David J. and Lucas, Richard E. and Monin, Beno{\^i}t and Montealegre, Andres and Mullen, Elizabeth and Pang, Jun and Ray, Jennifer and Reinero, Diego A. and Reynolds, Jesse and Sowden, Walter and Storage, Daniel and Su, Runkun and Tworek, Christina M. and Van Bavel, Jay J. and Walco, Daniel and Wills, Julian and Xu, Xiaobing and Yam, Kai Chi and Yang, Xiaoyu and Cunningham, William A. and Schweinsberg, Martin and Urwitz, Molly and Uhlmann, Eric L. and {The Crowdsourcing Hypothesis Tests Collaboration}},
  year = {2020},
  month = jan,
  journal = {Psychological Bulletin},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/bul0000220},
  urldate = {2020-02-24},
  langid = {english}
}

@article{Laudel2014,
  title = {Beyond Breakthrough Research: {{Epistemic}} Properties of Research and Their Consequences for Research Funding},
  shorttitle = {Beyond Breakthrough Research},
  author = {Laudel, Grit and Gl{\"a}ser, Jochen},
  year = {2014},
  month = sep,
  journal = {Research Policy},
  volume = {43},
  number = {7},
  pages = {1204--1216},
  issn = {0048-7333},
  doi = {10.1016/j.respol.2014.02.006},
  urldate = {2022-02-14},
  abstract = {The aim of this paper is to initiate a discussion about links between epistemic properties and institutional conditions for research by providing an exploratory analysis of such links featured by projects funded by the European Research Council (ERC). Our analysis identifies epistemic properties of research processes and links them to necessary and favourable conditions for research, and through these to institutional conditions provided by grants. Our findings enable the conclusion that there is research that is important for the progress of a field but is difficult to fund with common project grants. The predominance and standardisation of grant funding, which can be observed about many European countries, appears to reduce the chances of unconventional projects across all disciplines. Funding programmes of the `ERC-type' (featuring large and flexible budgets, long time horizons, and risk-tolerant selection processes) constitute an institutional innovation because they enable such research. However, while the ERC funding and other new funding schemes for exceptional research attempt to cover these requirements, they are unlikely to suffice.},
  langid = {english},
  keywords = {Epistemic properties of research,High risk -- high reward research,Intellectual innovation,Research funding}
}

@article{Liner2009,
  title = {Research Requirements for Promotion and Tenure at {{PhD}} Granting Departments of Economics},
  author = {Liner, Gaines H. and Sewell, Ellen},
  year = {2009},
  month = may,
  journal = {Applied Economics Letters},
  publisher = {Taylor \& Francis},
  doi = {10.1080/13504850701221998},
  urldate = {2021-10-11},
  abstract = {(2009). Research requirements for promotion and tenure at PhD granting departments of economics. Applied Economics Letters: Vol. 16, No. 8, pp. 765-768.},
  copyright = {Copyright Taylor and Francis Group, LLC},
  langid = {english}
}

@article{Mahoney1977,
  title = {Publication {{Prejudices}}: {{An Experimental Study}} of {{Confirmatory Bias}} in the {{Peer Review System}}},
  author = {Mahoney, Michael J.},
  year = {1977},
  journal = {Cognitive Therapy and Research},
  volume = {1},
  number = {2},
  pages = {161--175},
  doi = {10.1007/BF01173636}
}

@article{Maki2006,
  title = {Models Are Experiments, Experiments Are Models},
  author = {M{\"a}ki, Uskali},
  year = {2006},
  month = aug,
  journal = {Journal of Economic Methodology},
  publisher = {Taylor \& Francis Group},
  doi = {10.1080/13501780500086255},
  urldate = {2021-04-29},
  abstract = {(2005). Models are experiments, experiments are models. Journal of Economic Methodology: Vol. 12, No. 2, pp. 303-315.},
  copyright = {Copyright Taylor and Francis Group, LLC},
  langid = {english}
}

@article{McElreath2015,
  title = {Replication, {{Communication}}, and the {{Population Dynamics}} of {{Scientific Discovery}}},
  author = {McElreath, Richard and Smaldino, Paul E.},
  year = {2015},
  month = aug,
  journal = {PLOS ONE},
  volume = {10},
  number = {8},
  pages = {e0136088},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0136088},
  urldate = {2021-09-20},
  abstract = {Many published research results are false (Ioannidis, 2005), and controversy continues over the roles of replication and publication policy in improving the reliability of research. Addressing these problems is frustrated by the lack of a formal framework that jointly represents hypothesis formation, replication, publication bias, and variation in research quality. We develop a mathematical model of scientific discovery that combines all of these elements. This model provides both a dynamic model of research as well as a formal framework for reasoning about the normative structure of science. We show that replication may serve as a ratchet that gradually separates true hypotheses from false, but the same factors that make initial findings unreliable also make replications unreliable. The most important factors in improving the reliability of research are the rate of false positives and the base rate of true hypotheses, and we offer suggestions for addressing each. Our results also bring clarity to verbal debates about the communication of research. Surprisingly, publication bias is not always an obstacle, but instead may have positive impacts---suppression of negative novel findings is often beneficial. We also find that communication of negative replications may aid true discovery even when attempts to replicate have diminished power. The model speaks constructively to ongoing debates about the design and conduct of science, focusing analysis and discussion on precise, internally consistent models, as well as highlighting the importance of population dynamics.},
  langid = {english},
  keywords = {Chemical elements,Mathematical models,Pigments,Population dynamics,Publication ethics,Research quality assessment,Research validity,Scientists}
}

@misc{McElreath2021,
  title = {Science {{Is Like A Chicken Coop}}},
  author = {McElreath, Richard},
  year = {2021},
  month = jul,
  address = {online},
  urldate = {2024-07-04},
  abstract = {Creative Commons Attribution license (reuse allowed)},
  annotation = {Abstract: The quality and transparency of scholarship is influenced by\\
professional incentives. So say we all. Naturally much discussion\\
focuses on reforming incentives. But reforming scholarly incentives is\\
not easy, and incentives may matter less than structural and\\
demographic forces. Using analogies from population biology, I sketch\\
some problems and opportunities for effective science reform. First,\\
incentives arise from structure as much as from explicit reward.\\
Second, incentives are not all---demography and development\\
matter as well. Third, there are fundamental limits on the power of\\
incentives when the fates of individuals are largely up to chance.\\
There are reasons to think reform can succeed, especially if we adopt\\
a dynamic and structured view of the cultural evolution of scholarly\\
communities.}
}

@article{Mishra2010,
  title = {You Can't Always Get What You Want: {{The}} Motivational Effect of Need on Risk-Sensitive Decision-Making},
  shorttitle = {You Can't Always Get What You Want},
  author = {Mishra, Sandeep and Lalumi{\`e}re, Martin L.},
  year = {2010},
  month = jul,
  journal = {Journal of Experimental Social Psychology},
  volume = {46},
  number = {4},
  pages = {605--611},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2009.12.009},
  urldate = {2022-03-09},
  abstract = {Risky behavior in humans is typically considered irrational, reckless, and maladaptive. Risk-sensitivity theory, however, suggests that risky behavior may be adaptive in some circumstances: decision-makers should prefer high-risk options in situations of high need, when lower risk options are unlikely to meet those needs. This pattern of decision-making has been well established in the non-human animal literature, but little research has been conducted on humans. We demonstrate in a two-part experimental study that young men and women (n=115) behave as predicted by risk-sensitivity theory, shifting from risk-aversion to risk-proneness in situations of high need. This shift occurred whether decisions were made from description or from experience, and was observed controlling for sex and individual differences in general risk-taking propensity. This study is the first ecologically-relevant demonstration of risk-sensitive decision-making in humans.},
  langid = {english},
  keywords = {Decision-making,Ecological rationality,Individual differences,Need,Personality,Risk,Risk-sensitivity,Sex differences}
}

@article{Mishra2014,
  title = {Decision-{{Making Under Risk}}: {{Integrating Perspectives From Biology}}, {{Economics}}, and {{Psychology}}},
  shorttitle = {Decision-{{Making Under Risk}}},
  author = {Mishra, Sandeep},
  year = {2014},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {18},
  number = {3},
  pages = {280--307},
  issn = {1088-8683, 1532-7957},
  doi = {10.1177/1088868314530517},
  urldate = {2021-08-25},
  abstract = {Decision-making under risk has been variably characterized and examined in many different disciplines. However, interdisciplinary integration has not been forthcoming. Classic theories of decision-making have not been amply revised in light of greater empirical data on actual patterns of decision-making behavior. Furthermore, the meta-theoretical framework of evolution by natural selection has been largely ignored in theories of decision-making under risk in the human behavioral sciences. In this review, I critically examine four of the most influential theories of decision-making from economics, psychology, and biology: expected utility theory, prospect theory, risk-sensitivity theory, and heuristic approaches. I focus especially on risk-sensitivity theory, which offers a framework for understanding decision-making under risk that explicitly involves evolutionary considerations. I also review robust empirical evidence for individual differences and environmental/ situational factors that predict actual risky decision-making that any general theory must account for. Finally, I offer steps toward integrating various theoretical perspectives and empirical findings on risky decision-making.},
  langid = {english}
}

@article{Motyl2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  year = {2017},
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  number = {1},
  pages = {34--58},
  issn = {1939-1315, 0022-3514},
  doi = {10.1037/pspa0000084},
  urldate = {2018-06-14},
  langid = {english}
}

@article{Muller2014,
  title = {Postdoctoral {{Life Scientists}} and {{Supervision Work}} in the {{Contemporary University}}: {{A Case Study}} of {{Changes}} in the {{Cultural Norms}} of {{Science}}},
  shorttitle = {Postdoctoral {{Life Scientists}} and {{Supervision Work}} in the {{Contemporary University}}},
  author = {M{\"u}ller, Ruth},
  year = {2014},
  month = sep,
  journal = {Minerva},
  volume = {52},
  number = {3},
  pages = {329--349},
  issn = {1573-1871},
  doi = {10.1007/s11024-014-9257-y},
  urldate = {2022-02-14},
  abstract = {This paper explores the ways in which postdoctoral life scientists engage in supervision work in academic institutions in Austria. Reward systems and career conditions in academic institutions in most European and other OECD countries have changed significantly during the last two decades. While an increasing focus is put on evaluating research performances, little reward is attached to excellent performances in mentoring and advising students. Postdoctoral scientists mostly inhabit fragile institutional positions and experience harsh competition, as the number of available senior positions is small compared to that of young scientists striving for an academic career. To prevail in this competition, publications and mobility are key. Educational work is rarely rewarded. Nevertheless, postdocs play a key role in educating PhD students, as overburdened senior scientists often pass on practical supervision duties to their postdoctoral fellows. This paper shows how under these conditions, postdocs reframe the students they supervise as potential resources for co-authored publications. What might look like a mutually beneficial solution at a first glance, in practice implies the subordination of the values of education to the logic of production, which marginalizes spaces primarily devoted to education. The author argues that conflicts like this are indicative of broader changes in the cultural norms of science and academic citizenship, rendering community-oriented tasks such as education work less attractive to academic scientists. Since education and supervision work are central cornerstones of any functioning higher education and research system, this could have negative repercussions for the long-term development of academic institutions.},
  langid = {english}
}

@article{Muller2017,
  title = {Thinking with Indicators. {{Exploring}} the Epistemic Impacts of Academic Performance Indicators in the Life Sciences},
  author = {M{\"u}ller, Ruth and {de Rijcke}, Sarah},
  year = {2017},
  month = jul,
  journal = {Research Evaluation},
  volume = {26},
  number = {3},
  pages = {157--168},
  issn = {0958-2029},
  doi = {10.1093/reseval/rvx023},
  urldate = {2022-02-14},
  abstract = {While quantitative performance indicators are widely used by organizations and individuals for evaluative purposes, little is known about their impacts on the epistemic processes of academic knowledge production. In this article we bring together three qualitative research projects undertaken in the Netherlands and Austria to contribute to filling this gap. The projects explored the role of performance metrics in the life sciences, and the interactions between institutional and disciplinary cultures of evaluating research in these fields. Our analytic perspective is focused on understanding how researchers themselves give value to research, and in how far these practices are related to performance metrics. The article zooms in on three key moments in research processes to show how `thinking with indicators' is becoming a central aspect of research activities themselves: (1) the planning and conception of research projects, (2) the social organization of research processes, and (3) determining the endpoints of research processes. Our findings demonstrate how the worth of research activities becomes increasingly assessed and defined by their potential to yield high value in quantitative terms. The analysis makes visible how certain norms and values related to performance metrics are stabilized as they become integrated into routine practices of knowledge production. Other norms and criteria for scientific quality, e.g. epistemic originality, long-term scientific progress, societal relevance, and social responsibility, receive less attention or become redefined through their relations to quantitative indicators. We understand this trend to be in tension with policy goals that seek to encourage innovative, societally relevant, and responsible research.}
}

@article{Munafo2015,
  title = {Using Prediction Markets to Forecast Research Evaluations},
  author = {Munafo, Marcus R. and Pfeiffer, Thomas and Altmejd, Adam and Heikensten, Emma and Almenberg, Johan and Bird, Alexander and Chen, Yiling and Wilson, Brad and Johannesson, Magnus and Dreber, Anna},
  year = {2015},
  month = oct,
  journal = {Royal Society Open Science},
  volume = {2},
  number = {10},
  pages = {150287},
  issn = {2054-5703},
  doi = {10.1098/rsos.150287},
  urldate = {2019-02-19},
  langid = {english},
  keywords = {meta-science,prediction markets,prior elicitation,prior probability}
}

@article{Nissen2016,
  title = {Publication Bias and the Canonization of False Facts},
  author = {Nissen, Silas Boye and Magidson, Tali and Gross, Kevin and Bergstrom, Carl T},
  editor = {Rodgers, Peter},
  year = {2016},
  month = dec,
  journal = {eLife},
  volume = {5},
  pages = {e21451},
  issn = {2050-084X},
  doi = {10.7554/eLife.21451},
  urldate = {2019-09-11},
  abstract = {Science is facing a ``replication crisis'' in which many experimental findings cannot be replicated and are likely to be false. Does this imply that many scientific facts are false as well? To find out, we explore the process by which a claim becomes fact. We model the community's confidence in a claim as a Markov process with successive published results shifting the degree of belief. Publication bias in favor of positive findings influences the distribution of published results. We find that unless a sufficient fraction of negative results are published, false claims frequently can become canonized as fact. Data-dredging, p-hacking, and similar behaviors exacerbate the problem. Should negative results become easier to publish as a claim approaches acceptance as a fact, however, true and false claims would be more readily distinguished. To the degree that the model reflects the real world, there may be serious concerns about the validity of purported facts in some disciplines.},
  keywords = {false positive,hypothesis testing,meta-science,phil sci,publication bias,replication crisis}
}

@article{Nosek2014,
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  shorttitle = {Registered {{Reports}}},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {137--141},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000192},
  urldate = {2018-05-23},
  langid = {english}
}

@article{Nosek2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2600--2606},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  urldate = {2021-04-25},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes---a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  chapter = {Colloquium Paper},
  copyright = {{\copyright} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {29531091},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration}
}

@article{Nosek2022,
  title = {Replicability, {{Robustness}}, and {{Reproducibility}} in {{Psychological Science}}},
  author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aur{\'e}lien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Struhl, Melissa Kline and Nuijten, Mich{\`e}le B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Sch{\"o}nbrodt, Felix D. and Vazire, Simine},
  year = {2022},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {73},
  number = {1},
  pages = {annurev-psych-020821-114157},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-020821-114157},
  urldate = {2021-11-30},
  abstract = {Replication---an important, uncommon, and misunderstood practice---is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.             Expected final online publication date for the Annual Review of Psychology, Volume 73 is January 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  copyright = {All rights reserved},
  langid = {english}
}

@article{OConnor2019,
  title = {The Natural Selection of Conservative Science},
  author = {O'Connor, Cailin},
  year = {2019},
  month = aug,
  journal = {Studies in History and Philosophy of Science Part A},
  volume = {76},
  pages = {24--29},
  issn = {0039-3681},
  doi = {10.1016/j.shpsa.2018.09.007},
  urldate = {2022-03-11},
  abstract = {Social epistemologists have argued that high risk, high reward science has an important role to play in scientific communities. Recently, though, it has also been argued that various scientific fields seem to be trending towards conservatism---the increasing production of what Kuhn (1962) might have called `normal science'. This paper will explore a possible explanation for this sort of trend: that the process by which scientific research groups form, grow, and dissolve might be inherently hostile to such science. In particular, I employ a paradigm developed by Smaldino and McElreath (2016) that treats a scientific community as a population undergoing selection. As will become clear, perhaps counter-intuitively this sort of process in some ways promotes high risk, high reward science. But, as I will point out, risky science is, in general, the sort of thing that is hard to repeat. While more conservative scientists will be able to train students capable of continuing their successful projects, and so create thriving lineages, successful risky science may not be the sort of thing one can easily pass on. In such cases, the structure of scientific communities selects against high risk, high rewards projects. More generally, this project makes clear that there are at least two processes to consider in thinking about how incentives shape scientific communities---the process by which individual scientists make choices about their careers and research, and the selective process governing the formation of new research groups.},
  langid = {english}
}

@article{OSC2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  urldate = {2019-12-19},
  abstract = {Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 Structured Abstract INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  copyright = {Copyright {\copyright} 2015, American Association for the Advancement of Science},
  langid = {english},
  pmid = {26315443}
}

@article{Pennycook2018,
  title = {An Analysis of the {{Canadian}} Cognitive Psychology Job Market (2006-2016)},
  author = {Pennycook, Gordon and Thompson, Valerie A.},
  year = {2018},
  month = jun,
  journal = {Canadian Journal of Experimental Psychology = Revue Canadienne De Psychologie Experimentale},
  volume = {72},
  number = {2},
  pages = {71--80},
  issn = {1878-7290},
  doi = {10.1037/cep0000149},
  abstract = {How accomplished does one need to be to compete in the Canadian cognitive psychology job market? We looked at the publication record of everyone who was hired as an assistant professor in Canadian cognitive psychology divisions with PhD programs between 2006 and 2016 (N = 64). Individuals who were hired from 2006 to 2011 averaged 10 journal-article publications up to and including the year they were hired. However, this number increased by 57\% to 18 publications between 2012 and 2016. Notably, this increase (a) occurred despite an increase in the number of positions since 2010, (b) was not restricted to top-ranked institutions, (c) did not come at the cost of decreasing quality in research (based on citations), and (d) was not driven by longer postdoctoral fellowships. To supply context, we obtained data on the publication records of 98 eminent and early-career award-winning cognitive psychologists when they obtained their first faculty positions. The correlation between year of hire and publication number in the full sample was strongly positive (r = .47) and driven primarily by a substantial increase in recent years, which suggests that the increasingly competitive job market is not specific to Canada. Finally, we found that behaviour (as opposed to neuroscience) researchers and those who obtained their PhDs from Canadian universities may be at particular risk in the job market. At a time when increasing numbers of PhDs are graduating from cognitive psychology programs, it has likely never been more difficult to obtain a faculty position. (PsycINFO Database Record},
  langid = {english},
  pmid = {29902028},
  keywords = {Behavioral Sciences,Canada,Cognition,Employment,Health Workforce,Humans,Psychology}
}

@book{R-base,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2019},
  address = {Vienna, Austria},
  organization = {R Foundation for Statistical Computing}
}

@book{R-bookdown,
  title = {Bookdown: {{Authoring}} Books and Technical Documents with {{R}} Markdown},
  author = {Xie, Yihui},
  year = {2016},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, Florida}
}

@book{R-ggplot2,
  title = {Ggplot2: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {Springer-Verlag New York},
  isbn = {978-3-319-24277-4}
}

@book{R-here,
  title = {Here: {{A}} Simpler Way to Find Your Files},
  author = {M{\"u}ller, Kirill},
  year = {2017}
}

@book{R-knitr,
  title = {Dynamic Documents with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2015},
  edition = {2nd},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, Florida}
}

@book{R-papaja,
  title = {{{papaja}}: {{Create APA}} Manuscripts with {{R Markdown}}},
  author = {Aust, Frederik and Barth, Marius},
  year = {2018}
}

@book{R-rmarkdown,
  title = {R Markdown: {{The}} Definitive Guide},
  author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton, Florida}
}

@article{Roos2010,
  title = {Decision Making under Risk in {{Deal}} or {{No Deal}}},
  author = {de Roos, Nicolas and Sarafidis, Yianis},
  year = {2010},
  journal = {Journal of Applied Econometrics},
  volume = {25},
  number = {6},
  pages = {987--1027},
  issn = {1099-1255},
  doi = {10.1002/jae.1110},
  urldate = {2021-08-07},
  abstract = {We analyse the choices of 399 contestants in the Australian version of the television game show Deal or No Deal. We calculate risk aversion bounds for each contestant, revealing considerable heterogeneity. We then estimate a structural stochastic choice model that captures the dynamic decision problem faced by contestants. To address individual heterogeneity, we nest the dynamic problem within the settings of both a random effects and a random coefficients probit model. Our structural model produces plausible estimates of risk aversion, confirms the role of individual heterogeneity and suggests that a model of stochastic choice is indeed appropriate. We find mixed evidence of greater risk aversion by females. We also examine generalizations to expected utility theory, finding that the rank-dependent utility model adds non-negligible explanatory power and indicates optimism in probability weighting. Finally, we test, but are unable to confirm, the existence of an endowment effect for lotteries. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  langid = {english}
}

@article{Rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  issn = {00332909},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any gien research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type 1 errors, while the file drawers are filled with the 95\% of the studies that show non-significant resluts. Quantitative procedures for computing the tolerance for filed and future results are reported and illustrated, and the implications are discussed.},
  pmid = {53},
  keywords = {tolerance for null results bias in publication of}
}

@book{RStudioTeam2019,
  title = {{{RStudio}}: {{Integrated}} Development Environment for r},
  author = {{RStudio Team}},
  year = {2019},
  address = {Boston, MA},
  organization = {RStudio, Inc.}
}

@article{Sanbonmatsu2015,
  title = {Why a {{Confirmation Strategy Dominates Psychological Science}}},
  author = {Sanbonmatsu, David M. and Posavac, Steven S. and Behrends, Arwen A. and Moore, Shannon M. and Uchino, Bert N.},
  year = {2015},
  month = sep,
  journal = {PLOS ONE},
  volume = {10},
  number = {9},
  pages = {e0138197},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0138197},
  urldate = {2024-07-10},
  abstract = {Our research explored the incidence and appropriateness of the much-maligned confirmatory approach to testing scientific hypotheses. Psychological scientists completed a survey about their research goals and strategies. The most frequently reported goal is to test the non-absolute hypothesis that a particular relation exists in some conditions. As expected, few scientists reported testing universal hypotheses. Most indicated an inclination to use a confirmation strategy to test the non-absolute hypotheses that a particular relation sometimes occurs or sometimes does not occur, and a disconfirmation strategy to test the absolute hypotheses that a particular relation always occurs or never occurs. The confirmatory search that dominates the field was found to be associated with the testing of non-absolute hypotheses. Our analysis indicates that a confirmatory approach is the normatively correct test of the non-absolute hypotheses that are the starting point of most studies. It also suggests that the strategy of falsification that was once proposed by Popper is generally incorrect given the infrequency of tests of universal hypotheses.},
  langid = {english},
  keywords = {Clinical psychology,Developmental psychology,Psychologists,Psychology,Scientists,Social psychology,Survey research,Surveys}
}

@article{Schafer2019,
  title = {The {{Meaningfulness}} of {{Effect Sizes}} in {{Psychological Research}}: {{Differences Between Sub-Disciplines}} and the {{Impact}} of {{Potential Biases}}},
  shorttitle = {The {{Meaningfulness}} of {{Effect Sizes}} in {{Psychological Research}}},
  author = {Sch{\"a}fer, Thomas and Schwarz, Marcus A.},
  year = {2019},
  journal = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.00813},
  urldate = {2019-04-15},
  abstract = {Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes---when is an effect small, medium, or large?---has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = .36) were much larger than effects from the latter (median r = .16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.},
  langid = {english},
  keywords = {Cohen,effect size,effect sizes,meta-science,power,preregistration,Publication Bias,replicability,Replication,RRs,Sample Size}
}

@article{Scheel2021a,
  title = {An {{Excess}} of {{Positive Results}}: {{Comparing}} the {{Standard Psychology Literature With Registered Reports}}},
  shorttitle = {An {{Excess}} of {{Positive Results}}},
  author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Dani{\"e}l},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {251524592110074},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/25152459211007467},
  urldate = {2021-04-20},
  abstract = {Selectively publishing results that support the tested hypotheses (``positive'' results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs ( N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature ( N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
  copyright = {All rights reserved},
  langid = {english}
}

@article{Schotter2014,
  title = {Belief {{Elicitation}} in the {{Laboratory}}},
  author = {Schotter, Andrew and Trevino, Isabel},
  year = {2014},
  month = aug,
  journal = {Annual Review of Economics},
  volume = {6},
  number = {1},
  pages = {103--128},
  issn = {1941-1383, 1941-1391},
  doi = {10.1146/annurev-economics-080213-040927},
  urldate = {2019-03-14},
  langid = {english}
}

@article{Shafir2005,
  title = {Caste-Specific Differences in Risk Sensitivity in Honeybees, {{Apis}} Mellifera},
  author = {Shafir, Sharoni and Menda, Gil and Smith, Brian H.},
  year = {2005},
  month = apr,
  journal = {Animal Behaviour},
  volume = {69},
  number = {4},
  pages = {859--868},
  issn = {0003-3472},
  doi = {10.1016/j.anbehav.2004.07.011},
  urldate = {2022-10-16},
  abstract = {Honeybee workers (foragers) are risk averse to variability in volume of reward when measured by conditioning of the proboscis extension response, and the level of risk aversion depends on the coefficient of variation of the variable distribution. Since drones do not forage on flowers, they may not have been under selection for risk-sensitive choice behaviour. We compared risk sensitivity of workers and drones and their ability to discriminate between the reward volumes used in the risk sensitivity experiments. Both castes discriminated better between 0 and 0.4{$\mu$}l than between 0.4 and 1.2{$\mu$}l, consistent with Weber's law of relative discrimination. Workers discriminated between both volume pairs better than drones, and workers showed greater risk aversion than drones. This is the first demonstration of caste-specific differences in risk sensitivity. These differences do not appear to be the result of differences in energy budgets, since both castes were on positive energy budgets. Levels of risk aversion were consistent with the coefficient of variation model. We calculated the relative associative strengths of subjects to the reward volumes from their choice proportions in the discrimination tests. The relative associative strengths of workers were greater than those of drones, and in both castes the relative associative strength of 0.4{$\mu$}l relative to 0{$\mu$}l was greater than that of 1.2{$\mu$}l relative to 0.4{$\mu$}l. Owing to Jensen's inequality, the decreasing functions of differences in relative associative strengths could explain differences in degree of risk aversion between the castes. Our findings are consistent with both mechanistic and functional explanations.},
  langid = {english}
}

@article{Silberzahn2018,
  title = {Many {{Analysts}}, {{One Data Set}}: {{Making Transparent How Variations}} in {{Analytic Choices Affect Results}}},
  shorttitle = {Many {{Analysts}}, {{One Data Set}}},
  author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahn{\'i}k, {\v S}. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and {Gamez-Djokic}, M. and Glenz, A. and {Gordon-McKeon}, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and H{\"o}gden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schl{\"u}ter, E. and Sch{\"o}nbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Sp{\"o}rlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {337--356},
  issn = {2515-2459},
  doi = {10.1177/2515245917747646},
  urldate = {2019-02-19},
  abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts' prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
  langid = {english},
  keywords = {inference,meta-science,p-hacking,prior elicitation,prior probability,replication crisis,stats}
}

@article{Simmons2011,
  title = {False-Positive Psychology: {{Undisclosed}} Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {14679280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  pmid = {22006061},
  keywords = {disclosure,methodology,motivated reasoning,publication}
}

@article{Smaldino2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, R.},
  year = {2016},
  journal = {Royal Society Open Science},
  volume = {3},
  pages = {160384},
  doi = {10.1098/rsos.160384}
}

@article{Snyder2021,
  title = {Time and {{Chance}}: {{Using Age Partitioning}} to {{Understand How Luck Drives Variation}} in {{Reproductive Success}}},
  shorttitle = {Time and {{Chance}}},
  author = {Snyder, Robin E. and Ellner, Stephen P. and Hooker, Giles},
  year = {2021},
  month = apr,
  journal = {The American Naturalist},
  volume = {197},
  number = {4},
  pages = {E110-E128},
  publisher = {The University of Chicago Press},
  issn = {0003-0147},
  doi = {10.1086/712874},
  urldate = {2024-07-04},
  abstract = {Over the course of individual lifetimes, luck usually explains a large fraction of the between-individual variation in life span or lifetime reproductive output (LRO) within a population, while variation in individual traits or ``quality'' explains much less. To understand how, where in the life cycle, and through which demographic processes luck trumps trait variation, we show how to partition by age the contributions of luck and trait variation to LRO variance and how to quantify three distinct components of luck. We apply these tools to several empirical case studies. We find that luck swamps effects of trait variation at all ages, primarily because of randomness in individual state dynamics (``state trajectory luck''). Luck early in life is most important. Very early state trajectory luck generally determines whether an individual ever breeds, likely by ensuring that they are not dead or doomed quickly. Less early luck drives variation in success among those breeding at least once. Consequently, the importance of luck often has a sharp peak early in life or it has two peaks. We suggest that ages or stages where the importance of luck peaks are potential targets for interventions to benefit a population of concern, different from those identified by eigenvalue elasticity analysis.},
  keywords = {Artemisia tridentata,individual stochasticity,lifetime reproductive success,reproductive skew,Rissa tridactyla,trait variation}
}

@article{Soderberg2021,
  title = {Initial Evidence of Research Quality of Registered Reports Compared with the Standard Publishing Model},
  author = {Soderberg, Courtney K. and Errington, Timothy M. and Schiavone, Sarah R. and Bottesini, Julia and Thorn, Felix Singleton and Vazire, Simine and Esterling, Kevin M. and Nosek, Brian A.},
  year = {2021},
  month = aug,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {8},
  pages = {990--997},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01142-4},
  urldate = {2021-10-27},
  abstract = {In registered reports (RRs), initial peer review and in-principle acceptance occur before knowing the research outcomes. This combats publication bias and distinguishes planned from unplanned research. How RRs could improve the credibility of research findings is straightforward, but there is little empirical evidence. Also, there could be unintended costs such as reducing novelty. Here, 353 researchers peer reviewed a pair of papers from 29 published RRs from psychology and neuroscience and 57 non-RR comparison papers. RRs numerically outperformed comparison papers on all 19 criteria (mean difference 0.46, scale range -4 to +4) with effects ranging from RRs being statistically indistinguishable from comparison papers in novelty (0.13, 95\% credible interval [-0.24, 0.49]) and creativity (0.22, [-0.14, 0.58]) to sizeable improvements in rigour of methodology (0.99, [0.62, 1.35]) and analysis (0.97, [0.60, 1.34]) and overall paper quality (0.66, [0.30, 1.02]). RRs could improve research quality while reducing publication bias and ultimately improve the credibility of the published literature.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Psychology,Publishing},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Psychology;Publishing\\
Subject\_term\_id: psychology;publishing}
}

@article{Sterling1959,
  title = {Publication {{Decisions}} and Their {{Possible Effects}} on {{Inferences Drawn}} from {{Tests}} of {{Significance---or Vice Versa}}},
  author = {Sterling, Theodore D.},
  year = {1959},
  journal = {Journal of the American Statistical Association},
  volume = {54},
  number = {285},
  eprint = {http://dx.doi.org/10.1080/01621459.1959.10501497},
  pages = {30--34},
  doi = {10.1080/01621459.1959.10501497}
}

@article{Sterling1995,
  title = {Publication {{Decisions Revisited}}: {{The Effect}} of the {{Outcome}} of {{Statistical Tests}} on the {{Decision}} to {{Publish}} and {{Vice Versa}}},
  shorttitle = {Publication {{Decisions Revisited}}},
  author = {Sterling, Theodore D. and Rosenbaum, W. L. and Weinkam, J. J.},
  year = {1995},
  month = feb,
  journal = {The American Statistician},
  volume = {49},
  number = {1},
  eprint = {2684823},
  eprinttype = {jstor},
  pages = {108},
  issn = {00031305},
  doi = {10.2307/2684823},
  urldate = {2019-08-15},
  keywords = {meta-science,NHST,publication bias}
}

@article{Stewart2021,
  title = {The Natural Selection of Good Science},
  author = {Stewart, Alexander J. and Plotkin, Joshua B.},
  year = {2021},
  month = may,
  journal = {Nature Human Behaviour},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01111-x},
  urldate = {2021-05-19},
  abstract = {Scientists in some fields are concerned that many published results are false. Recent models predict selection for false positives as the inevitable result of pressure to publish, even when scientists are penalized for publications that fail to replicate. We model the cultural evolution of research practices when laboratories are allowed to expend effort on theory, enabling them, at a cost, to identify hypotheses that are more likely to be true, before empirical testing. Theory can restore high effort in research practice and suppress false positives to a technical minimum, even without replication. The mere ability to choose between two sets of hypotheses, one with greater prior chance of being correct, promotes better science than can be achieved with effortless access to the set of stronger hypotheses. Combining theory and replication can have synergistic effects. On the basis of our analysis, we propose four simple recommendations to promote good science.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english}
}

@article{Tiokhin2021,
  title = {Competition for Priority Harms the Reliability of Science, but Reforms Can Help},
  author = {Tiokhin, Leonid and Yan, Minhua and Morgan, Thomas J. H.},
  year = {2021},
  month = jul,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {7},
  pages = {857--867},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-01040-1},
  urldate = {2021-07-28},
  abstract = {Incentives for priority of discovery are hypothesized to harm scientific reliability. Here, we evaluate this hypothesis by developing an evolutionary agent-based model of a competitive scientific process. We find that rewarding priority of discovery causes populations to culturally evolve towards conducting research with smaller samples. This reduces research reliability and the information value of the average study. Increased start-up costs for setting up single studies and increased payoffs for secondary results (also known as scoop protection) attenuate the negative effects of competition. Furthermore, large rewards for negative results promote the evolution of smaller sample sizes. Our results confirm the logical coherence of scoop protection reforms at several journals. Our results also imply that reforms to increase scientific efficiency, such as rapid journal turnaround times, may produce collateral damage by incentivizing lower-quality research; in contrast, reforms that increase start-up costs, such as pre-registration and registered reports, may generate incentives for higher-quality research.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Economics;Human behaviour\\
Subject\_term\_id: economics;human-behaviour}
}

@article{Tiokhin2021a,
  title = {Honest Signaling in Academic Publishing},
  author = {Tiokhin, Leonid and Panchanathan, Karthik and Lakens, Daniel and Vazire, Simine and Morgan, Thomas and Zollman, Kevin},
  year = {2021},
  month = feb,
  journal = {PLOS ONE},
  volume = {16},
  number = {2},
  pages = {e0246675},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0246675},
  urldate = {2022-03-07},
  abstract = {Academic journals provide a key quality-control mechanism in science. Yet, information asymmetries and conflicts of interests incentivize scientists to deceive journals about the quality of their research. How can honesty be ensured, despite incentives for deception? Here, we address this question by applying the theory of honest signaling to the publication process. Our models demonstrate that several mechanisms can ensure honest journal submission, including differential benefits, differential costs, and costs to resubmitting rejected papers. Without submission costs, scientists benefit from submitting all papers to high-ranking journals, unless papers can only be submitted a limited number of times. Counterintuitively, our analysis implies that inefficiencies in academic publishing (e.g., arbitrary formatting requirements, long review times) can serve a function by disincentivizing scientists from submitting low-quality work to high-ranking journals. Our models provide simple, powerful tools for understanding how to promote honest paper submission in academic publishing.},
  langid = {english},
  keywords = {Asymmetric information,Communications,Conflicts of interest,Deception,Peer review,Research quality assessment,Scientific publishing,Scientists}
}

@article{Traag2021,
  title = {Inferring the Causal Effect of Journals on Citations},
  author = {Traag, V. A.},
  year = {2021},
  month = jul,
  journal = {Quantitative Science Studies},
  volume = {2},
  number = {2},
  pages = {496--504},
  issn = {2641-3337},
  doi = {10.1162/qss_a_00128},
  urldate = {2022-07-25},
  abstract = {Articles in high-impact journals are, on average, more frequently cited. But are they cited more often because those articles are somehow more ``citable''? Or are they cited more often simply because they are published in a high-impact journal? Although some evidence suggests the latter, the causal relationship is not clear. We here compare citations of preprints to citations of the published version to uncover the causal mechanism. We build on an earlier model of citation dynamics to infer the causal effect of journals on citations. We find that high-impact journals select articles that tend to attract more citations. At the same time, we find that high-impact journals augment the citation rate of published articles. Our results yield a deeper understanding of the role of journals in the research system. The use of journal metrics in research evaluation has been increasingly criticized in recent years and article-level citations are sometimes suggested as an alternative. Our results show that removing impact factors from evaluation does not negate the influence of journals. This insight has important implications for changing practices of research evaluation.}
}

@article{vanDalen2012,
  title = {Intended and Unintended Consequences of a Publish-or-Perish Culture: {{A}} Worldwide Survey},
  shorttitle = {Intended and Unintended Consequences of a Publish-or-Perish Culture},
  author = {{van Dalen}, Hendrik P. and Henkens, K{\`e}ne},
  year = {2012},
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {63},
  number = {7},
  pages = {1282--1293},
  issn = {1532-2890},
  doi = {10.1002/asi.22636},
  urldate = {2022-02-14},
  abstract = {How does publication pressure in modern-day universities affect the intrinsic and extrinsic rewards in science? By using a worldwide survey among demographers in developed and developing countries, the authors show that the large majority perceive the publication pressure as high, but more so in Anglo-Saxon countries and to a lesser extent in Western Europe. However, scholars see both the pros (upward mobility) and cons (excessive publication and uncitedness, neglect of policy issues, etc.) of the so-called publish-or-perish culture. By measuring behavior in terms of reading and publishing, and perceived extrinsic rewards and stated intrinsic rewards of practicing science, it turns out that publication pressure negatively affects the orientation of demographers towards policy and knowledge sharing. There are no signs that the pressure affects reading and publishing outside the core discipline.},
  langid = {english},
  keywords = {bibliometrics,scientists,surveys}
}

@article{Wacholder2004,
  title = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}: {{An Approach}} for {{Molecular Epidemiology Studies}}},
  shorttitle = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}},
  author = {Wacholder, S. and Chanock, S. and {Garcia-Closas}, M. and {El ghormli}, L. and Rothman, N.},
  year = {2004},
  month = mar,
  journal = {JNCI Journal of the National Cancer Institute},
  volume = {96},
  number = {6},
  pages = {434--442},
  issn = {0027-8874, 1460-2105},
  doi = {10.1093/jnci/djh075},
  urldate = {2019-08-28},
  langid = {english}
}

@article{Wagenmakers2012,
  title = {An {{Agenda}} for {{Purely Confirmatory Research}}},
  author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and {van der Maas}, Han L. J. and Kievit, Rogier A.},
  year = {2012},
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {632--638},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612463078},
  urldate = {2018-06-14},
  langid = {english}
}

@article{Winterhalder1999,
  title = {Risk-Senstive Adaptive Tactics: {{Models}} and Evidence from Subsistence Studies in Biology and Anthropology},
  shorttitle = {Risk-Senstive Adaptive Tactics},
  author = {Winterhalder, Bruce and Lu, Flora and Tucker, Bram},
  year = {1999},
  month = dec,
  journal = {Journal of Archaeological Research},
  volume = {7},
  number = {4},
  pages = {301--348},
  issn = {1573-7756},
  doi = {10.1007/BF02446047},
  urldate = {2021-04-29},
  abstract = {Risk-sensitive analysis of subsistence adaptations is warranted when (i) outcomes are to some degree unpredictable and (ii) they have nonlinear consequences for fitness and/or utility. Both conditions are likely to be common among peoples studied by ecologicll anthropologists and archaeologists. We develop a general conceptual model of risk. We then review and summarize the extensive empirical literatures from biology and anthropology for methodological insights and for their comparative potential. Risk-sensitive adaptive tactics are diverse and they are taxonomically widespread. However, the anthropological literature rarely makes use of formal models of risk-sensitive adaptation, while the biological literature lacks naturalistic observations of risk-sensitive behavior. Both anthropology and biology could benefit from greater interdisciplinary exchange.},
  langid = {english}
}

@article{Wiseman2019,
  title = {Registered Reports: An Early Example and Analysis},
  shorttitle = {Registered Reports},
  author = {Wiseman, Richard and Watt, Caroline and Kornbrot, Diana},
  year = {2019},
  month = jan,
  journal = {PeerJ},
  volume = {7},
  pages = {e6232},
  issn = {2167-8359},
  doi = {10.7717/peerj.6232},
  urldate = {2019-12-28},
  abstract = {The recent `replication crisis' in psychology has focused attention on ways of increasing methodological rigor within the behavioral sciences. Part of this work has involved promoting `Registered Reports', wherein journals peer review papers prior to data collection and publication. Although this approach is usually seen as a relatively recent development, we note that a prototype of this publishing model was initiated in the mid-1970s by parapsychologist Martin Johnson in the European Journal of Parapsychology (EJP). A retrospective and observational comparison of Registered and non-Registered Reports published in the EJP during a seventeen-year period provides circumstantial evidence to suggest that the approach helped to reduce questionable research practices. This paper aims both to bring Johnson's pioneering work to a wider audience, and to investigate the positive role that Registered Reports may play in helping to promote higher methodological and statistical standards.},
  langid = {english},
  keywords = {history of science,meta-science,parapsychology,RRs}
}

@article{Yang2020a,
  title = {Estimating the Deep Replicability of Scientific Findings Using Human and Artificial Intelligence},
  author = {Yang, Yang and Youyou, Wu and Uzzi, Brian},
  year = {2020},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {20},
  pages = {10762--10768},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1909046117},
  urldate = {2020-09-03},
  langid = {english}
}

@article{Yoshimura1991,
  title = {Individual Adaptations in Stochastic Environments},
  author = {Yoshimura, Jin and Clark, Colin W.},
  year = {1991},
  month = apr,
  journal = {Evolutionary Ecology},
  volume = {5},
  number = {2},
  pages = {173--192},
  issn = {1573-8477},
  doi = {10.1007/BF02270833},
  urldate = {2021-10-16},
  abstract = {Many natural populations undergo radical and unpredictable fluctuations, associated with stochastic environmental conditions. Under such circumstances, fitness of a genotype (or `strategy') is defined as the geometric mean of the intergenerational genotypic population growth ratel(t). Unfortunately, this population-level criterion has proved difficult to apply at the level of individual organisms.},
  langid = {english}
}

@article{Young2020,
  title = {Theory and Measurement of Environmental Unpredictability},
  author = {Young, Ethan S. and Frankenhuis, Willem E. and Ellis, Bruce J.},
  year = {2020},
  month = nov,
  journal = {Evolution and Human Behavior},
  series = {Current Debates in Human Life History Research},
  volume = {41},
  number = {6},
  pages = {550--556},
  issn = {1090-5138},
  doi = {10.1016/j.evolhumbehav.2020.08.006},
  urldate = {2021-11-28},
  abstract = {Over the past decade, there is increasing interest in the ways in which environmental unpredictability shapes human life history development. However, progress is hindered by two theoretical ambiguities. The first is that conceptual definitions of environmental unpredictability are not precise enough to be able to express them in statistical terms. The second is that there are different implicit hypotheses about the proximate mechanisms that detect unpredictability, which have not been explicitly described and compared. The first is the ancestral cue perspective, which proposes that humans evolved to detect cues (e.g., loss of a parent, residential changes) that indicated high environmental unpredictability across evolutionary history. The second is the statistical learning perspective, which proposes that organisms estimate the level of unpredictability from lived experiences across development (e.g., prediction errors encountered through time). In this paper, we address both sources of ambiguity. First, we describe the possible statistical properties of unpredictability. Second, we outline the ancestral cue and statistical learning perspectives and their implications for the measurement of environmental unpredictability. Our goal is to provide concrete steps toward better conceptualization and measurement of environmental unpredictability from both approaches. Doing so will refine our understanding of environmental unpredictability and its connection to life history development.},
  langid = {english},
  keywords = {Ancestral cues,Environmental unpredictability,Life history development,Statistical learning}
}

@article{Zhang2017,
  title = {Toward a Model of Risky Decisions: {{Synergistic}} Effect of Affect Intensity and Affective Processing on Risk-Seeking as a Function of Decision Domain},
  shorttitle = {Toward a Model of Risky Decisions},
  author = {Zhang, Yufeng and Chen, Zhuo Job and Li, Hong},
  year = {2017},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  volume = {73},
  pages = {235--242},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.06.006},
  urldate = {2018-04-06},
  abstract = {Four studies investigated the causal link of affect intensity with risky decisions, and showed a striking contrast of life-saving and valuables-saving domains. When social distance is small people are more risk-seeking in the life-saving domain but less risk-seeking in the valuables-saving domain (Study 1), and the results remain robust under different framings (Study 2). Relatedly, people rely more on affective processing when social distance is small in the life-saving domain, but not in the valuables-saving domain (Study 3). Furthermore, in the life-saving domain social distance exerts an effect on risk preference under affective processing but not under deliberate processing, whereas, in the valuables-saving domain, social distance influences risk preference under deliberate processing but not under affective processing (Study 4). A unified, causal model of risky decisions is proposed to account for these findings and the fundamental differences among decision domains in light of their relationships with the affective processing. The model has a potential to generalize into other decision domains.},
  keywords = {Affect intensity,Decision making,Domain,Framing,OSBadgeOpenData,OSBadgeOpenMaterials,Risk}
}

@article{Zollman2009,
  title = {Optimal {{Publishing Strategies}}},
  author = {Zollman, Kevin J. S.},
  year = {2009},
  month = jun,
  journal = {Episteme},
  volume = {6},
  number = {2},
  pages = {185--199},
  issn = {1742-3600, 1750-0117},
  doi = {10.3366/E174236000900063X},
  urldate = {2021-06-14},
  abstract = {Journals regulate a significant portion of the communication between scientists. This paper devises an agent-based model of scientific practice and uses it to compare various strategies for selecting publications by journals. Surprisingly, it appears that the best selection method for journals is to publish relatively few papers and to select those papers it publishes at random from the available ``above threshold'' papers it receives. This strategy is most effective at maintaining an appropriate type of diversity that is needed to solve a particular type of scientific problem. This problem and the limitation of the model is discussed in detail.},
  langid = {english}
}

@article{Zondervan-Zwijnenburg2017,
  title = {Application and {{Evaluation}} of an {{Expert Judgment Elicitation Procedure}} for {{Correlations}}},
  author = {{Zondervan-Zwijnenburg}, Mari{\"e}lle and {van de Schoot-Hubeek}, Wenneke and Lek, Kimberley and Hoijtink, Herbert and {van de Schoot}, Rens},
  year = {2017},
  journal = {Frontiers in Psychology},
  volume = {8},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00090},
  urldate = {2019-02-25},
  abstract = {The purpose of the current study was to apply and evaluate a procedure to elicit expert judgments about correlations, and to update this information with empirical data. The result is a face-to-face group elicitation procedure with as its central element a trial roulette question that elicits experts' judgments expressed as distributions. During the elicitation procedure, a concordance probability question was used to provide feedback to the experts on their judgments. We evaluated the elicitation procedure in terms of validity and reliability by means of an application with a small sample of experts. Validity means that the elicited distributions accurately represent the experts' judgments. Reliability concerns the consistency of the elicited judgments over time. Four behavioral scientists provided their judgments with respect to the correlation between cognitive potential and academic performance for two separate populations enrolled at a specific school in the Netherlands that provides special education to youth with severe behavioral problems: youth with autism spectrum disorder (ASD), and youth with diagnoses other than ASD. Measures of face-validity, feasibility, convergent validity, coherence, and intra-rater reliability showed promising results. Furthermore, the current study illustrates the use of the elicitation procedure and elicited distributions in a social science application. The elicited distributions were used as a prior for the correlation, and updated with data for both populations collected at the school of interest. The current study shows that the newly developed elicitation procedure combining the trial roulette method with the elicitation of correlations is a promising tool, and that the results of the procedure are useful as prior information in a Bayesian analysis.},
  langid = {english},
  keywords = {Bayes,Bayesian Analysis,correlation,Correlation,Elicitation procedure,expert elicitation,Expert judgment,informative priors,prior elicitation,prior probability}
}
