% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ,man,mask,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\usepackage[normalem]{ulem}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{british}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\usepackage{float}
\usepackage{framed}
\usepackage{caption}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{wrapfig}
\captionsetup[figure]{font={stretch=1, small}, skip=10pt}
\captionsetup[textbox]{name=Box,labelsep=period,labelfont=it}
\newfloat{textbox}{thp}{lop}
\floatname{textbox}{Box}
\usepackage[most]{tcolorbox}
\definecolor{electricviolet}{rgb}{0.56, 0.0, 1.0}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Incentives for Registered Reports from a risk sensitivity perspective},
  pdflang={en-UK},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Incentives for Registered Reports from a risk sensitivity perspective}
\author{Anne M. Scheel\textsuperscript{1}, Leo Tiokhin\textsuperscript{1}, \& Daniël Lakens\textsuperscript{1}}
\date{}


\shorttitle{Risk-sensitive publication strategies}

\authornote{

Correspondence concerning this article should be addressed to Anne M. Scheel, Den Dolech 1, Atlas 9.417, 5600 MB, Eindhoven, The Netherlands. E-mail: \href{mailto:a.m.scheel@tue.nl}{\nolinkurl{a.m.scheel@tue.nl}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Eindhoven University of Technology}

\note{test}

\begin{document}
\maketitle

Registered Reports are an article format designed to reduce publication bias and `questionable research practices' (QRPs), which distort the published record of research findings in many scientific disciplines (Chambers, 2013; de Vries et al., 2018; Dickersin \& Min, 1993; Driessen, Hollon, Bockting, Cuijpers, \& Turner, 2015; Franco, Malhotra, \& Simonovits, 2014; Franco, Malhotra, \& Simonovits, 2016; Fraser, Parker, Nakagawa, Barnett, \& Fidler, 2018; Gerber, Green, \& Nickerson, 2001; Gopalakrishna et al., 2022; John, Loewenstein, \& Prelec, 2012; Kepes, Keener, McDaniel, \& Hartman, 2022; Makel, Hodges, Cook, \& Plucker, 2021; O'Boyle, Banks, \& Gonzalez-Mulé, 2017; Simmons, Nelson, \& Simonsohn, 2011; Stefan \& Schönbrodt, 2023).
In this format, peer review takes place before data collection and the decision to publish is made before authors, reviewers, and editors know the study results.
This is thought to remove incentives for authors to hide, embellish, or misrepresent unfavourable results because publication no longer depends on the study's findings (Chambers, Dienes, McIntosh, Rotshtein, \& Willmes, 2015).
Initial evidence from psychology and neighbouring disciplines shows that Registered Reports indeed contain much higher rates of negative results than the standard literature (Allen \& Mehler, 2019; O'Mahony, 2023; Scheel, Schijen, \& Lakens, 2021).

Advocates of the format have argued that the pre-data publication guarantee should make Registered Reports particularly attractive to researchers (e.g., Chambers \& Tzavella, 2021).
The argument is that Registered Reports reduce uncertainty about whether and where a study will be published before authors have invested in conducting the study, and that such risk reduction is appealing
in a research climate that involves substantial publication pressure in many countries and disciplines (Gopalakrishna et al., 2022; Miller, Taylor, \& Bedeian, 2011; Paruzel-Czachura, Baran, \& Spendel, 2021; Tijdink, Vergouwen, \& Smulders, 2013; van Dalen, 2021; van Dalen \& Henkens, 2012; Waaijer, Teelken, Wouters, \& van der Weijden, 2018).
However, if strategic concerns about publishability indeed influence researchers' choices for or against Registered Reports, it is unlikely that they would always cause risk aversion (i.e., favouring Registered Reports as a low-risk option).
Researchers' willingness to take risks regarding publication success may instead vary depending on factors such as available resources, time pressure, or competition.
This could create situations in which Registered Reports remain unpopular and would never gain traction without additional incentives or interventions.
And indeed, although uptake is growing exponentially (Chambers \& Tzavella, 2021), the market share of Registered Reports is currently still much smaller than one might expect if authors saw them as unreservedly beneficial for their careers.
Here, we examine these possibilities with an agent-based simulation and model authors' choices between publication formats as decision making under risk to identify circumstances in which Registered Reports might be used highly selectively or not at all.

\hypertarget{design-and-intended-functions-of-registered-reports}{%
\subsection{Design and intended functions of Registered Reports}\label{design-and-intended-functions-of-registered-reports}}

The review process of Registered Reports is split into two stages.
At Stage 1, reviewers evaluate a pre-study protocol containing the research questions, hypotheses, methods, and planned analyses of a proposed study.
In case of a positive decision, the journal issues an `in-principle acceptance' and commits to publishing the eventual report, regardless of the direction of the results.
Only after in-principle acceptance has been issued do authors move on to data collection and analysis and eventually complete the manuscript.
At Stage 2, the final report (now including the results) is subjected to a second round of peer review, but this time only to ensure that the study was carried out as planned, that the data pass any pre-specified quality checks, and that authors' conclusions are justified by the evidence.

Through this process, Registered Reports address publication bias as well as so-called `questionable research practices' (QRPs).
These two problems are considered important contributors to psychology's replication crisis (Ferguson \& Heene, 2012; Wagenmakers, Wetzels, Borsboom, van der Maas, \& Kievit, 2012) and to research waste in the biomedical sciences (Chalmers \& Glasziou, 2009) because they skew the available evidence for scientific claims, causing overconfidence and higher rates of false-positive inferences.
Publication bias can result from editors and reviewers disproportionately rejecting submissions with negative results (`reviewer bias,' Atkinson, Furlong, \& Wampold, 1982; Greenwald, 1975; Mahoney, 1977) or from researchers failing to submit negative results for publication (`file-drawering,' Franco et al., 2014; Rosenthal, 1979).
In Registered Reports, the
in-principle acceptance issued at Stage 1 reduces both of these issues: Editors and reviewers cannot reject the Stage-2 report based on the direction of the results, which also reduces the incentives for authors to file-drawer the study in case of negative results.
QRPs are practices that exploit undisclosed flexibility in data collection and analysis, for example when analysing different justifiable combinations of variables, subsamples, and decision criteria, and only reporting the ones with favourable results, or by presenting \emph{post hoc} inferences as having been predicted \emph{a priori} (Agnoli, Wicherts, Veldkamp, Albiero, \& Cubelli, 2017; Fiedler \& Schwarz, 2016; Fraser et al., 2018; John et al., 2012; Simmons et al., 2011).
Registered Reports minimise the risk of QRPs via the two-stage review process, in which the Stage-1 protocol acts as a preregistration and reviewers' task during Stage-2 review is to flag any undisclosed deviations from it.

\hypertarget{efficacy-of-registered-reports}{%
\subsection{Efficacy of Registered Reports}\label{efficacy-of-registered-reports}}

Registered Reports were first launched in 2013 at the journal \emph{Cortex} (Chambers, 2013) and are now offered by over 300 journals, predominantly in the behavioural sciences and life sciences (see \url{cos.io/rr}).
Nearly 600 Registered Reports had been published by 2021, with uptake growing exponentially (Chambers \& Tzavella, 2021).
In Chapter 2, we analysed the first cohort of published Registered Reports in psychology and showed that the first hypothesis reported in these articles was supported in only \(44\%\) of cases, compared to \(96\%\) in a random sample of standard reports (Scheel et al., 2021).
Similarly low proportions of positive results were found in partially overlapping samples of Registered Reports in psychology and neuroscience (\(39.5\%\), Allen \& Mehler, 2019) and in psychology, neuroscience, health, and education (\(50.1\%\), O'Mahony, 2023).
These findings suggest that Registered Reports indeed reduce biases that inflate the rate of positive results in the standard literature.
However, the existing estimates are based on purely observational evidence and may thus be confounded by other systematic differences between Registered Reports and standard reports.

Systematic differences would act as confounders if they affected either the probability of a positive result when testing a true hypothesis or the base rate of true hypotheses.
The first option is not supported by current evidence:
A study comparing Registered Reports with matched controls found that Registered Reports have higher median sample sizes and, in blind reviews, are judged to be more rigorous in methodology and analysis and of higher overall quality (Soderberg et al., 2021), meaning that the increased amount of negative results in Registered Reports is unlikely to be an artifact of lower statistical power or poorer methods.
But the second option\(\,\)---\(\,\)a difference in the rate of true hypotheses, or the (prior) probability that the tested hypothesis is true\(\,\)---\(\,\)has not yet been directly studied.
It is not implausible to think that Registered Reports might contain fewer true hypotheses:
If researchers expect that negative results are difficult to publish in standard reports but pose no problem in Registered Reports, they might selectively choose the Registered Report route when studying hypotheses that they think
will yield negative results.
If researchers additionally perceive the standard publication route as less costly (e.g., more habitual, more flexible, faster, requiring lower sample sizes, etc.), standard reports would plausibly remain the preferred option for hypotheses that researchers are more certain are true and will yield publishable results.

Such an effect could explain why both we and Allen \& Mehler (2019) found that replication studies in the Registered Reports literature had descriptively lower rates of positive results than original studies, although the difference was not significant in either case (\(39\%\) vs \(50\%\) in Scheel et al., 2021, and \(34\%\) vs \(45.5\%\) in Allen \& Mehler, 2019, but note that the samples of the two studies partially overlap).
As we discussed in Chapter 2, replication attempts may more often than novel research be driven by the suspicion that the tested hypothesis is not true (and hence that the result of the original study was a false positive).
It could also partially explain differences between our results and those of O'Mahony (2023), who compared Registered Reports to standard reports that were matched on based on the publishing journal, time of publication, research topic, design, and studied population (though last three factors had lower priority).
O'Mahony finds the difference in the positive result rate of Registered Reports and standard reports to be half as large as in our study (26 vs 52 percentage points), which compared Registered Reports with a random sample of standard reports (matched only on discipline).
Matching articles more closely could lead to more comparable prior probabilities of the hypotheses tested in both formats and thus account for part of this discrepancy.
However, the two studies also differ in the target population and estimand (O'Mahony analysed all tested hypotheses whereas Scheel et al.~focused on the first hypothesis per article), which makes the estimates difficult to compare.

Although differences between hypotheses tested in Registered Reports and standard reports remain speculative at this point, this consideration highlights the importance of understanding the costs and benefits of Registered Reports from the authors' perspective.
If current incentives cause Registered Reports to be used selectively in specific situations or for specific research questions, meta-scientists studying this emerging literature would need to take such factors into account.
Perhaps even more importantly, a better understanding of the incentive structure can help determine where, when, and by whom Registered Reports are likely to be used or avoided.
Such knowledge could then be used to identify areas in which Registered Reports may not gain popularity naturally and anticipate the need for further intervention (e.g., via policy) when there is a demand for unbiased results.

\hypertarget{author-incentives-for-registered-reports}{%
\subsection{Author incentives for Registered Reports}\label{author-incentives-for-registered-reports}}

Registered Reports are generally thought to `{[}neutralise{]} bad incentives' (Chambers, 2013, p. 609), in particular the incentive to exaggerate or misrepresent a study's results in order to make them more publishable in the standard literature.
This assumption is conditioned on the format:
Once authors have decided to take the Registered Report route, they can improve their publication chances only via the proposed research question and methods in Stage-1 review, and editors have an interest in selecting informative study designs because they are bound to publishing the study's results even when they turn out to be negative.
In contrast to standard reports, the results are thus no longer a main target to `hack' or select on, which should make them less biased and more trustworthy.

The incentives for choosing the Registered Reports route in the first place, however, are less clear.
Advocates of the format have argued that it `serve{[}s{]} the interests of individual scientists' (p.~12, Chambers \& Tzavella, 2021) because it reduces scientists' risk of investing in research projects whose results turn out to be difficult to publish.
The argument is based on the assumptions that researchers a) are under pressure to amass journal publications (which still are a central currency for hiring and promotion decisions) and b) face shortfalls in publication output when their studies yield negative results (which are more difficult to publish in the standard literature due to publication bias).
From this perspective, research results\(\,\)---\(\,\)which, absent QRPs, are not under the researcher's control\(\,\)---\(\,\)affect the variance of (career-relevant) publication outcomes in standard reports, but not in Registered Reports.
The following quote from a talk by Chris Chambers (\href{https://youtu.be/FiVI3cwVMZI?list=PLChfyH8TVDGmYENpXUDPaeeq2SLh8q9dt\&t=1047}{September 2021}) summarises this sentiment:

\begin{quote}
And the second main benefit, the one that really is the main big one, the big draw, is that as a researcher you can get your paper accepted before you even start your research and regardless of how the results turn out in the end. So no more playing the \emph{p}-value lottery, gambling on certain results going a certain way, otherwise you won't have your PhD or you won't get your next fellowship or your next grant\(\,\)---\(\,\)takes all of that pointless, and I think quite foolish, gambling out of the equation (\ldots)\footnote{\url{https://youtu.be/FiVI3cwVMZI?list=PLChfyH8TVDGmYENpXUDPaeeq2SLh8q9dt\&t=1047}, from minute 17:27}
\end{quote}

Registered Reports are designed to serve the research community and other consumers of the scientific literature by protecting against publication bias and QRPs.
A key selling point, however,
is that they are thought to `serve the interests of individual scientists' (p.~12, Chambers \& Tzavella, 2021) at the same time.
The underlying argument is that because scientists a) need to amass journal publications (which still are a central currency for hiring and promotion decisions) and b) face shortfalls in publication output when their studies yield negative results (which are more difficult to publish in the standard literature due to publication bias), a publication guarantee before data collection
should be highly valuable.

New Intro:

\begin{itemize}
\tightlist
\item
  \sout{What are RRs}
\item
  \sout{Why: pub bias \& QRPs}
\item
  \sout{Chapter 2: RRs indeed associated with lower rate of positive results}
\item
  Problem:

  \begin{itemize}
  \tightlist
  \item
    RRs may be used strategically for low priors
  \item
    assuming that they work, uptake not as high as we'd like, and not in all fields \(\rightarrow\) could be just basic diffusion of innovation process, but could also be because there are obstacles (e.g., in certain research areas, at certain career stages)
  \end{itemize}
\item
  \(\rightarrow\) what are the incentives for/against RRs? Here, we'll look at this with a computational model
\item
  RRs marketed as aligned with existing incentives: `safe' choice for researchers
\item
  But if that's true and they're a safe choice, we \emph{wouldn't} expect them to always be preferred
\item
  Risk-sensitivity theory

  \begin{itemize}
  \tightlist
  \item
    Intro to RST with example
  \item
    Brief explanation of relationship with utility theory and prospect theory
  \item
    Appliation to RR problem
  \end{itemize}
\item
  Goals of the chapter: apply RST to find out when \& where RRs are expected to be particularly popular vs unpopular \(\rightarrow\) implications for policy and meta-science
\end{itemize}

However, although it is objectively true that Registered Reports provide more certainty about eventual publication success early in a project, this certainty may not always be preferred over the `gamble'

being a `safe' alternative to the `gamble' of the standard publication route

But if the standard publications are indeed a gamble and Registered Reports a safe alternative, does it follow that Registered Reports

Peer-reviewed publications are a central currency for the careers of academic researchers, both in terms of publication quantity and publication impact (R. Müller, 2014; van Dalen \& Henkens, 2012).
In the standard publication model, researchers face uncertainty about whether and where they will be able to publish the results of their study. Translated into currency terms, the career benefit a researcher receives for conducting a study can vary extremely\(\,\)---\(\,\)from near zero when the resulting manuscript is rejected by all consulted journals (or when the author file-drawers the study because the chances of success do not justify the cost of repeated submissions and revisions) to an extremely high, perhaps career-making amount when a manuscript is published in a very high-impact journal like \emph{Nature} or \emph{Science}.
In other words, success in the standard system is highly variable and highly volatile since it hinges on the one factor that is supposed to be outside of researchers' control --- the study results.
This unfortunate combination can be excessively stressful for researchers (especially junior scientists without secure positions) and tempt them to hype, spin, or even fabricate their results.

Compared to this, Registered Reports are a relatively safe, stress-free alternative because authors receive a results-independent publication guarantee before investing in data collection or analysis.
As Registered-Reports inventor Chris Chambers put it in a recent talk (\href{https://youtu.be/FiVI3cwVMZI?list=PLChfyH8TVDGmYENpXUDPaeeq2SLh8q9dt\&t=1047}{September 2021}):

\begin{quote}
And the second main benefit, the one that really is the main big one, the big draw, is that as a researcher you can get your paper accepted before you even start your research and regardless of how the results turn out in the end. So no more playing the \emph{p}-value lottery, gambling on certain results going a certain way, otherwise you won't have your PhD or you won't get your next fellowship or your next grant\(\,\)---\(\,\)takes all of that pointless, and I think quite foolish, gambling out of the equation completely. (from minute 17:27)
\end{quote}

But would researchers ever choose the gamble over the safe publication?
Unless the net benefit of a Registered Report is always at least as valuable as the best possible outcome that could be achieved through the standard publication route, the answer is `probably yes'.
Authors deciding between Registered Reports and the standard publication route face the choice between a payoff with low variability (a relatively safe publication in the journal the Stage-1 protocol was submitted to) and a payoff with high variability (anywhere between no publication and a high-impact publication, or even several publications if the project yields enough `fodder').
Situations like these are commonly termed \emph{decision-making under risk}.
`Risk' is defined as `unpredictable variation in the outcome of a behavior, with consequences for an organism's fitness or utility' (Winterhalder, Lu, \& Tucker, 1999, p. 302).
Organisms are \emph{risk sensitive} when they are not only sensitive to the mean outcomes of different behavioural options but also to their variance.

Framing authors' choice between Registered Reports and standard publications as risk-averse versus risk-prone behaviour allows us to examine the problem with Risk-Sensitivity Theory, a normative theory developed in behavioural ecology to explain the foraging behaviour of animals.
Risk-Sensitivity Theory was designed to determine the optimal food-acquisition strategy for an animal faced with a choice between a relatively safe (low-variance) food source and a risky (high-variance) source that sometimes yields large payoffs and sometimes small payoffs (or none at all).
Despite this initial narrow scope, Risk-Sensitivity Theory has proven itself as a powerful framework for explaining risk-sensitive behaviour in a wide range of situations and species, including humans (Kacelnik \& Bateson, 1996; Kacelnik \& Bateson, 1997; Mishra, 2014).

\texttt{To\ do:}

\begin{itemize}
\tightlist
\item
  Explain that RST is superior to utility theory and can incorporate prospect theory (Mishra, 2014)
\item
  Better explain the evolutionary angle and why it matters
\end{itemize}

\hypertarget{goals-of-the-chapter}{%
\subsection{Goals of the chapter}\label{goals-of-the-chapter}}

In this chapter, we use a simulation model to explore how properties of academic careers and academic incentive structures that are relevant to risk sensitivity may affect the strategies of researchers choosing between Registered Reports and the standard publication format.
The research goal is to understand in which circumstances Registered Reports should be particularly attractive, particularly unattractive, or particularly prone to highly selective use.
The results of this analysis may help anticipate where the format is unlikely to take foot without additional changes to norms, incentives, or policy, and flag situations in which the results of published Registered Reports may be particularly difficult to compare to the normal literature.
The following sections outline central concepts of Risk-Sensitivity Theory, relate them to characteristics of academic careers, and describe an evolutionary simulation model in which their effects on researchers' risk-sensitive publication decisions are examined.

\hypertarget{conceptual-application-of-risk-sensitivity-theory-to-publication-decisions}{%
\section{Conceptual application of Risk-Sensitivity Theory to publication decisions}\label{conceptual-application-of-risk-sensitivity-theory-to-publication-decisions}}

This section describes general factors that affect the role of risk for individual's fitness and connects these factors to relevant elements of academic careers.
In this context, Risk-Sensitivity Theory's focus on reproductive fitness as the central outcome may be seen as misguided.
But although researchers do not forage, grow, reproduce, and die in the \emph{biological} sense (except in their role as human beings in general, of course), they undoubtedly are concerned with factors that influence 1) their survival and 2) the propagation of their traits in an \emph{academic} sense.
Even if we were to assume that researchers are not consciously trying to maximise their `academic fitness', a competitive job market will by definition select for individuals whose past behaviour increased their prospects.
Such competition can create bottlenecks between early-career and tenured positions in many academic disciplines, which inevitably induce a selection pressure for career-promoting behaviours (Smaldino \& McElreath, 2016).

In applying Risk-Sensitivity Theory to researchers' publishing behaviour, we will therefore use a general notion of career success as the central outcome variable in place of reproductive fitness.
This decision does not imply that career success is the only or the proximal motivation for researchers' behaviour in practice, just as evolutionary theory does not imply that reproductive success is the only or the proximal motivation for human behaviour in everyday life.
However, we do assume that selection for career-promoting behaviours has a noticeable impact on research practice.



\par\vspace{.8\baselineskip}
\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{rr-risk-sensitivity_files/figure-latex/fitnessplot-1} 

}

\caption{Consequences of non-linear fitness functions. Payoffs \(b_-\), \(b_{safe}\), and \(b_+\) are converted into fitness with a diminishing (blue), linear (grey), or increasing (red) returns function.}\label{fig:fitnessplot}
\end{figure}
\par\vspace{.5\baselineskip}

\hypertarget{non-linear-fitness-functions}{%
\paragraph{Non-linear fitness functions}\label{non-linear-fitness-functions}}

The first and perhaps most ubiquitous factor leading individuals to be risk sensitive are non-linear relationships between the outcomes of an individual's behaviour (e.g., harvested food items, publications) and its reproductive success (Kacelnik \& Bateson, 1997).
Consider two options, \(O_{safe}\) and \(O_{risky}\).
\(O_{safe}\) always gives the same payoff \(b_{safe}\), whereas \(O_{risky}\) gives either a low payoff \(b_-\) or a high payoff \(b_+\), each with probability \(\frac{1}{2}\).
When \(b_{safe} = \frac{(b_- + b_+)}{2}\), \(O_{safe}\) and \(O_{risky}\) have the same expected payoff.
However, we would only expect an individual to be indifferent between the two options if the consequences of their payoffs for the individual's fitness are linear.
When the function relating payoffs to fitness is instead convex or concave (yielding increasing or diminishing returns, respectively), the expected fitness of \(O_{safe}\) and \(O_{risky}\) will differ and shift the individual's preference towards risk proneness or risk aversion.
An illustration of this example is shown in Figure~\ref{fig:fitnessplot}:
While the payoffs \(b_-\), \(b_{safe}\), and \(b_+\) are equidistant on the x-axis, \(b_{safe}\) is associated with greater fitness than the average of \(b_-\) and \(b_+\) when the function is concave, and with lower fitness when the function is convex.
In other words, \(O_{safe}\) has greater expected fitness than \(O_{risky}\) when returns are diminishing, and \(O_{risky}\) has greater expected fitness than \(O_{safe}\) when returns are increasing.

Non-linear relationships are arguably the norm in the natural world and linear relationships the exception.
This plausibly holds for academia as well, where the effect of publication success on researchers' career success might change over time:
For early-career researchers, small increases in the number or impact of publications may have an accelerated effect on career success, whereas established professors may care little about any one additional publication to their record.



\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{rr-risk-sensitivity_files/figure-latex/varianceplot-1} 

}

\caption{Survival thresholds. When fitness drops to zero below the low threshold (dashed line), individuals should be risk-averse because the outcomes of the low-risk option (narrow distribution) are guaranteed to lie above the threshold and the outcomes of the high-risk option (wide distribution) have a non-negligible risk of falling below the threshold. When fitness drops to zero below the high threshold (dotted line), individuals should be risk-prone because only the high-risk option provides a chance of passing the threshold.}\label{fig:varianceplot}
\end{figure}

\hypertarget{survival-thresholds-and-competition}{%
\paragraph{Survival thresholds and competition}\label{survival-thresholds-and-competition}}

A second important factor for risk-sensitive behaviour are thresholds for survival and reproduction (Hurly, 2003; Winterhalder et al., 1999).
Survival thresholds are cutoff points below which an individual's fitness drops to zero, for example due to starvation.
Risk-Sensitivity Theory predicts that an individual will be risk averse when the resources provided by a low-variance option are sufficient to meet the threshold and risk-prone when they are not (Mishra, 2014).
For example, a hummingbird that needs to acquire a certain amount of calories to survive the night will prefer a low-risk food source if this option's expected payoff is above the threshold, but avoid the low-risk source if only a higher-risk source provides a chance of survival.
One such situation is depicted in Figure~\ref{fig:varianceplot}.

Although comparable cutoff points in academic careers may have somewhat less severe consequences, they certainly exist:
Amount and impact of a researcher's publications are common and often explicit criteria in decisions that are central to the individual's career, such as whether they will be awarded a PhD, whether they will receive grant funding, whether they will be offered a tenure-track position, or whether they will be granted tenure.
In some of these situations, the cutoff points are absolute and thus resemble survival thresholds in the biological sense, for example PhD-programme regulations that determine a minimal number of peer-reviewed publications for a candidate to be awarded with a PhD, or tenure contracts that specify minimal publication targets.
In other situations, the cutoff points are relative and depend on the number of eligible candidates, for example when grant funding is awarded to the 10 highest-ranked research proposals or a job is offered to the best candidate from a pool of applicants.
In cases like these, one individual's success diminishes the chances of another --- they represent \emph{competition}.
In the following, survival thresholds and competition will be treated as separate concepts to examine their differential effects on researchers' publication behaviour.

\hypertarget{number-of-decision-events-before-evaluation}{%
\paragraph{Number of decision events before evaluation}\label{number-of-decision-events-before-evaluation}}

A final risk-relevant factor considered here is the number of decision events taking place before an individual's fitness is evaluated.
When a risky option is chosen repeatedly, the average of the accumulating payoffs gets closer and closer to the long-run expected payoff.
This means that the danger of loosing out completely by only acquiring the lowest possible payoff of the risky option diminishes, making the risky option relatively more attractive.
However, this relationship only holds for repeated decision events \emph{before} an individual's fitness is evaluated.
When fitness is evaluated after a single decision event, a risky option is more likely to yield an extreme outcome that translates to zero fitness (i.e., death or an ultimate failure to reproduce).

In situations like this, when a single risky decision might cost an individual's life or offspring, average fitness is best described by the geometric mean instead of the arithmetic mean (Haaland, Wright, \& Ratikainen, 2019).
The geometric mean is more sensitive to variance because it is multiplicative, capturing the fact that one failure to reproduce can end a genetic lineage.
This circumstance has been shown to produce bet-hedging:
Risk-averse strategies may be more adaptive across many generations even when more risk-prone strategies produce better outcomes in any one generation, simply because the latter are also more likely to lead to extinction by sheer bad luck (Haaland et al., 2019).
While average fitness across generations is best represented with the geometric mean, average fitness \emph{within} a generation is better captured by the arithmetic mean, reflecting the additive accumulation of payoffs from decision events before fitness is evaluated.
Therefore, as the number of decision events per generation (i.e., before fitness is evaluated) increases, the variance-sensitive geometric mean of acquired payoffs becomes relatively less important and the less variance-sensitive arithmetic mean becomes more important.
Consequently, an individual's behaviour should switch from relative risk-aversion to relative risk-proneness.

For the purpose of the present study, `decision events' refer to researchers' decisions of whether to conduct a Registered Report or pursue the standard publication route.
Because Registered Reports must be submitted before data collection, such decisions occur whenever researchers start a new empirical project that they later may want to publish.\footnote{At the current moment, most researchers likely never consciously consider Registered Reports as a publication option. However, the fact that they \emph{could} nonetheless renders their pursuit of standard publications a choice, albeit an implicit one.}
The number of decision events before evaluation thus reflects the number of empirical projects that a researcher can conduct before their publication record is considered for hiring, promotion, or grant funding decisions.
We will call this parameter `empirical pace'.

Key factors influencing empirical pace are the time and resources required to conduct a study and the time and resources researchers have available.
Empirical pace may thus differ between 1) research areas that vary in speed and/or cost of data collection (e.g., a field relying on online questionnaires \emph{vs} a field relying on fMRI studies), 2) research labs that vary in funding and manpower, and 3) career stages, for example because career progress often comes with increased funding and the supervision of junior researchers whose efforts boost the supervisors' output (R. Müller, 2014), or because junior researchers often have short-term contracts that limit the available time for producing research output before their CVs are evaluated for the next application.

Each of the risk-relevant factors described above\(\,\)---\(\,\)non-linear fitness functions, survival thresholds, competition, and empirical pace\(\,\)---\(\,\)likely impacts researchers' decision strategies, including their choices between low-risk and high-risk publication options.
To better understand when a low-risk option like Registered Reports should be particularly attractive or unattractive, we examine the individual and interactive effects of these factors in a simulation model.

\hypertarget{simulation-model}{%
\section{Simulation model}\label{simulation-model}}

We develop an evolutionary agent-based model which simulates a population of researchers who test hypotheses, (attempt to) publish the results either as Registered Reports or as standard reports, accumulate the payoffs for successful publications, and pass their publication strategies on to the next generation of researchers.

\hypertarget{research-phase}{%
\paragraph{Research phase}\label{research-phase}}

Consider a population of \(n = 500\) researchers.
Each researcher has a fixed publication strategy \(s\), the so-called submission threshold.
In each round of the research phase, researchers randomly choose a hypothesis to test in a study.
Hypotheses are true with prior probability \(p\), which is uniformly distributed between 0 and 1 and known to the researcher.
Before testing their chosen hypothesis, a researcher compares the prior \(p\) of their hypothesis with their publication strategy \(s\).
When \(p < s\), the researcher chooses to play it safe and conduct a Registered Report to test the hypothesis.
When \(p \geq s\), the researcher chooses to gamble and test the hypothesis in a regular study which is then submitted as a standard report.

For simplicity, we assume that \(p\) is an ideal objective prior and that researchers' hypothesis tests are free from additional sources of error.
Thus, when a researcher tests hypothesis \(i\), they obtain a positive result with probability \(p_i\) and a negative result with probability \(1-p_i\).
If the researcher chose to submit a Registered Report, their study is published regardless of the result and the researcher receives a payoff \(b_{RR}\).
However, if the researcher chose to submit a standard report, they face rampant publication bias:
Only positive results are publishable as standard reports and yield a payoff \(b_{SR+} = 1\), whereas negative results are rejected or file-drawered and yield no payoff, \(b_{SR-} = 0\).
For all variations of the model tested here, we assume that the payoff for a Registered Report falls between these bounds, such that \(b_{SR-} < b_{RR} < b_{SR+}\).
This assumption reflects the following considerations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Due to publication bias in the standard literature, negative results are less valuable than positive results (\(b_{SR-} < b_{SR+}\)), for example because they do not lead to a publication at all, because only very low-impact journals are willing to publish them, or because getting them published requires a lot of extra effort (e.g., via frequent resubmissions following rejection or substantial revisions demanded by reviewers), which diminishes the net reward.
\item
  For these same reasons, Registered Reports are on average more valuable than standard reports with negative results (\(b_{SR-} < b_{RR}\)), for example because Registered Reports are offered by journals that may display publication bias for standard reports (rejecting standard report submissions with negative results), or simply because Registered Reports need to be resubmitted less often or require less extensive revisions.
\item
  On average, standard reports with positive results are more valuable than Registered Reports (\(b_{RR} < b_{SR+}\)), for example because many high-impact journals do not (yet) offer Registered Reports, because not registering one's study \emph{a priori} makes it easier to spin the results to appear more impactful and thus increases the chances to be published in a high-impact journal, or because Registered Reports may require more effort due to their stricter quality criteria, lowering the net reward.
  While proponents of Registered Reports may argue that the format has such tremendous advantages that authors' resulting career benefits are superior to any alternative, this chapter is predicated on the assumption that most researchers currently do not share this view.
  Once this changes, the present investigation may happily become redundant.
\end{enumerate}

This entire research cycle\(\,\)---\(\,\)choosing a hypothesis, choosing a publication route by comparing its prior \(p\) to one's publication strategy \(s\), testing the hypothesis, and receiving payoff \(b_{RR}\) for a Registered Report or \(b_{SR-}\) or \(b_{SR+}\) for a positive and negative standard report, respectively\(\,\)---\(\,\)is repeated \(m\) times.

\hypertarget{evaluation-phase}{%
\paragraph{Evaluation phase}\label{evaluation-phase}}

At the end of the research phase, researchers' accumulated publication payoffs \(b_1 + b_2 + ... + b_m\) are translated into fitness \(f\).
Fitness is calculated with a function characterised by exponent \(\epsilon\), which determines the shape of the function. \(\epsilon = 1\) yields a linear function, \(0 < \epsilon < 1\) yields a concave function with diminishing returns, and \(\epsilon > 1\) yields a convex function with increasing returns (see Figure~\ref{fig:fitnessplot}):

\begin{align}
f = (\sum_{i=1}^{m} b_i)^\epsilon
\end{align}

However, two situations may cause a researcher's fitness to fall to zero even when their accumulated payoffs are non-zero.
First, the sum of their payoffs may fall below an absolute survival threshold \(\delta\), for example when a researcher fails to meet an agreed publication target by the time their `tenure clock' runs out.
Thus, when \(\sum_{i=1}^{m} b_i < \delta\), \(f = 0\).
Second, the sum of their payoffs may fall below a relative threshold \(\gamma\), which reflects the intensity of competition (e.g., for scarce research grants or positions).
\(\gamma\) is the proportion of researchers who are considered for reproduction.
When \(\gamma = 1\), all researchers in the population are considered for reproduction and their fitness is calculated according to Eq. 1.
When \(\gamma < 1\), the \((1 - \gamma)*500\) least successful researchers receive zero fitness and cannot reproduce.\footnote{In the simulation, \(\gamma\) is applied \emph{after} fitness has been calculated, not before. This change has purely technical reasons and leads to the same result as applying \(\gamma\) to accumulated payoffs and then calculating fitness because all fitness functions are monotonic increasing and fitness functions do not vary within a population. That is, applying the fitness function does not affect the rank order of researchers in the population.}
For example, \(\gamma = 0.1\) means that only those researchers with accumulated payoffs in the top \(10\%\) of the population can reproduce, and the fitness of the remaining \(90\%\) is set to zero.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1273}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6364}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2364}}@{}}
\caption{Parameter definitions and values}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Value {[}range{]}
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Value {[}range{]}
\end{minipage} \\
\midrule()
\endhead
\(n\) & population size & 500 \\
\(g\) & number of generations & 250 \\
\(p\) & prior probability of hypotheses & uniform {[}0--1{]} \\
\(b_{SR-}\) & payoff for negative standard report & 0 \\
\(b_{SR+}\) & payoff for positive standard report & 1 \\
\(b_{RR}\) & payoff for Registered Report & {[}.1, .2, \ldots, .9{]} \\
\(\epsilon\) & fitness function exponent & {[}0.2, 1, 5{]} \\
\(m\) & research cycles per generation (`empirical pace') & {[}1, 2, 4, 8, 16, 32{]} \\
\(\delta\) & survival threshold below which fitness = 0, expressed as proportion of m & {[}0, .25, .5, .75{]} \\
\(\gamma\) & proportion of most successful researchers selected for reproduction (competition) & {[}1, .9, .5, .1, .05, .01{]} \\
\bottomrule()
\end{longtable}

\hypertarget{reproduction-phase}{%
\paragraph{Reproduction phase}\label{reproduction-phase}}

Finally, the researchers in the current population retire and a new (non-overlapping) generation of researchers is created.
A researcher in the new generation inherits their publication strategy \(s\) from a researcher in the previous generation with the probability of the previous researcher's fitness (i.e., the new generation's publication strategies are sampled with replacement from the previous generation, probability-weighted by fitness).
The new generation's publication strategies are inherited with a small amount of random noise, such that \(s_{new} = s_{old} + w\), with \(w \sim N(\mu = 0, \sigma = 0.01)\).
Authors of similar evolutionary agent-based models have described such hereditary transmission as reflecting mentorship and teaching (e.g., when established professors advise mentees to copy their strategies) or simply a generic social learning process in which successful researchers are more likely to be imitated by others (Smaldino \& McElreath, 2016).
Although this interpretation may be useful, the main purpose of this aspect of the model is purely technical and not specifically intended to reflect reality\(\,\)---\(\,\)it simply provides the machinery for determining which publication strategies are optimal in the various situations we are investigating.

\hypertarget{outcome-variable-s}{%
\subsubsection{\texorpdfstring{Outcome variable \(s\)}{Outcome variable s}}\label{outcome-variable-s}}

We study how the evolution of researchers' publication strategies \(s\) is affected by the payoff for Registered Reports \(b_{RR}\) (relative to the payoffs for standard reports, which are fixed at \(b_{SR-} = 0\) and \(b_{SR+} = 1\)), by the shape of the fitness function determined by exponent \(\epsilon\), by the number of research cycles per generation \(m\), by survival threshold \(\delta\), and by competition \(\gamma\) (see Table 1 for an overview of the model parameters and their values considered in the simulation).
It is important to keep in mind that a researcher's publication strategy \(s\) is
not an absolute decision:
It determines \emph{how} the choice between Registered Reports and standard reports is made, not which format is chosen.
As such, \(s\) indicates the amount of risk a researcher is willing to take.
Very low values of \(s\) reflect risk proneness:
The researcher prefers to gamble and chooses the standard publication route for almost all hypotheses they encounter, using the Registered Report route only for hypotheses that are virtually guaranteed to be false (and yield negative results).
Very high values of \(s\) reflect risk aversion:
The researcher is unwilling to risk a negative result in a standard report and studies almost all hypotheses they encounter in the Registered Report format, reserving the standard publication route for hypotheses that are virtually guaranteed to be true (and yield positive results).

\hypertarget{simulation-approach}{%
\subsubsection{Simulation approach}\label{simulation-approach}}

We use the evolutionary mechanism of this agent-based model as a means for identifying optimal behaviour under different conditions.
But this goal can also be achieved in other ways.
One non-evolutionary alternative is to calculate expected fitness (i.e., the long-run average) for a wide range of \(s\) and determine which strategy maximises it in each condition.
A drawback of this approach is that it does not account for population dynamics and therefore cannot easily simulate the effects of competition.
Because of this limitation, our study is based on the evolutionary model.
However, we validate all analyses except those involving competition on the expected-fitness model and show that both models produce virtually identical results (see Appendix).



\begin{figure}

{\centering \includegraphics[width=5.04in]{plots/plot_evo} 

}

\caption{Evolution of publication strategy \(s\) with 3 different payoffs for Registered Reports (\(b_{RR}\)). Simulations are based on a population of \(n = 500\) researchers over 250 generations, with payoffs for standard reports fixed at 0 for negative results (\(b_{SR-} = 0\)) and 1 for positive results (\(b_{SR+} = 1\)), a linear fitness function \(\epsilon = 1\), one research cycle per generation (\(m = 1\)), no survival threshold (\(\delta = 0\)) and no competition (\(\gamma = 1\)). Each condition was run 10 times. Thin lines represent the median publication strategy of the population in each run, shaded areas represent the inter-quartile range of publication strategies in the population in each run, and thick lines represent the median of run medians per condition.}\label{fig:evoplot}
\end{figure}

\hypertarget{simulation-results}{%
\section{Simulation results}\label{simulation-results}}

The results of the simulation models will be presented in order of increasing model complexity.
We start by explaining the very simple scenarios shown in Figure \ref{fig:evoplot} and Figure \ref{fig:epsilonplot}.
These scenarios are identical to situations discussed above and the results should thus be unsurprising.
However, while they may seem trivial to some, we hope that these explanations will help unfamiliar readers understand the basic functioning of our model as well as the less intuitive results presented later.

When interpreting the results below, one should bear in mind that the analysed parameter values are inherently arbitrary.
Although the model parameters are intended to capture important characteristics of real-world concepts, their values do not represent real-world units.
The goal of this analysis is to understand the relative effects of the model parameters in a simplified, artificial system, which means that the results are only meaningful in relation to each other.

\hypertarget{single-research-cycle-per-generation-linear-fitness-function}{%
\subsection{Single research cycle per generation, linear fitness function}\label{single-research-cycle-per-generation-linear-fitness-function}}

The first generation of researchers in each simulation run is initialised with randomly distributed publication strategies \(s\) (drawn from a uniform distribution {[}0--1{]}), which are then allowed to evolve over the subsequent generations.
Figure \ref{fig:evoplot} shows the effect of varying the payoffs for Registered Reports when the fitness function is linear (\(\epsilon = 1\)), with no survival threshold (\(\delta = 0\)), no competition (\(\gamma = 1\)), and one research cycle per generation (\(m = 1\)).
In this very simple scenario, evolved publication strategies (\(s\)) approximate the payoff for Registered Reports in each condition, indicating that the optimal publication strategy is always equal to \(b_{RR}\) (\(s_{optimal} = 0.2\) when \(b_{RR} = 0.2\), \(s_{optimal} = 0.5\) when \(b_{RR} = 0.5\), \(s_{optimal} = 0.8\) when \(b_{RR} = 0.8\)).
The reason behind this is the uniform distribution {[}0--1{]} of hypothesis priors, the payoff structure \(b_{SR-} = 0\) and \(b_{SR+} = 1\), and the linear fitness function (\(\epsilon = 1\) means that fitness equals payoff).
In this constellation, the expected fitness obtained from a standard report is always equal to the prior of the tested hypothesis:

\begin{align}
E[f_{SR}] = (p * b_{SR+} + (1-p) * b_{SR-})^1 = p * 1 +  (1-p) * 0 = p
\end{align}

For example, testing a hypothesis with \(p = 0.2\) in a standard report would yield the expected fitness \(E[f_{SR}] = (0.2 * 1 + 0.8 * 0)^1 = 0.2\).
The optimal strategy is to submit a Registered Report whenever the expected fitness provided by a standard report is lower than the fitness provided by a Registered Report, \(E[f_{SR}] < b_{RR}\), and thus whenever \(p < b_{RR}\).
This ensures that researchers always get the best of both worlds, minimising shortfalls when priors are (too) low and maximising winning chances when priors are (sufficiently) high.
For example, \(b_{RR} = 0.5\) is larger than \(E[f_{SR}]\) for all hypotheses with \(p < 0.5\) but lower than \(E[f_{SR}]\) for all hypotheses with \(p > 0.5\).
In this situation, researchers who submit Registered Reports whenever \(p<0.5\) and standard reports whenever \(p>0.5\) protect themselves against losing a bad bet by instead taking the fixed payoff \(b_{RR} = 0.5\), but always play a good bet and thus maximise their chances of winning \(b_{SR+} = 1\).
Every alternative is inferior in the long run because researchers with \(s > b_{RR}\) lose out on increased chances of publishing a standard report and researchers with \(s < b_{RR}\) take unnecessary risks and go empty-handed too often.



\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{plots/plot_b_epsilon} 

}

\caption{Effect of fitness functions on evolved publication strategies. Shown are median publication strategies in the final (\(250^{th}\)) generations of 50 runs for different values of \(b_{RR}\) (x-axis) and different fitness functions (characterised by exponent \(\epsilon\)), with one research cycle per generation (\(m = 1\)), no survival threshold (\(\delta = 0\)) and no competition (\(\gamma = 1\)). Fitness functions with \(\epsilon = 0.2\) and \(\epsilon = 0.5\) (blue lines) are concave with diminishing returns, functions with \(\epsilon = 2\) and \(\epsilon = 5\) (red lines) are convex with increasing returns, and the function with \(\epsilon = 1\) (grey line) is linear. Small dots represent median \(s\) of the final generation in each run, large dots represent the median of these 50 run medians per condition. Error bars represent the \(95\%\) capture probability around the median of medians.}\label{fig:epsilonplot}
\end{figure}

\hypertarget{allowing-for-non-linear-fitness-functions}{%
\subsection{Allowing for non-linear fitness functions}\label{allowing-for-non-linear-fitness-functions}}

Arguably, the career benefits researchers receive from publications in the real world are rarely, if ever, linear.
In early career, we may assume a convex fitness function, with each addition to the short publication record of a young researcher yielding increasing returns for their prospects on the job market and their ability to obtain grant funding.
A notable exception may be PhD students who plan to leave academia after obtaining their degree, and for whom the career returns of publications exceeding the PhD requirements are thus strongly decreasing (concave fitness function).
Researchers who stay in academia may experience that the career returns for each additional publication begin to decrease as their publication record grows, meaning that advanced career stages may also be characterised by a concave fitness function.

Figure \ref{fig:epsilonplot} contrasts the effects of two concave fitness functions (\(\epsilon = 0.2\) and \(\epsilon = 0.5\), shown in blue shades) and two convex fitness functions (\(\epsilon = 2\) and \(\epsilon = 5\), shown in red shades) with a linear function (\(\epsilon = 1\), grey line) for different payoffs for Registered Reports, in the same simple scenario with only one research cycle per generation.
The grey line for \(\epsilon = 1\) represents the already familiar situation from Figure \ref{fig:evoplot} above:
When the fitness function is linear, the optimal strategy is \(s_{optimal} = b_{RR}\).
Non-linear fitness functions deviate from this pattern exactly as expected based on Figure \ref{fig:fitnessplot}.
When additional payoffs yield diminishing returns (\(\epsilon <1\)), Registered Reports become more attractive even when they are worth less than the expected payoff for standard reports.
As explained above, this is because concave functions `shrink' the difference between moderate and high payoffs relative to the difference between low and moderate payoffs.
Conversely, when additional payoffs yield increasing returns (\(\epsilon > 1\)), Registered Reports are unattractive unless their payoffs are almost as large as those for published standard reports because convex functions increase the difference between moderate and high payoffs relative to low versus moderate payoffs.

When different fitness functions are taken to reflect different career stages,
this pattern suggests that Registered Reports should be more attractive for senior researchers and a tough sell for early-career researchers.
Interestingly, preliminary empirical evidence suggests the opposite:
Registered Reports appear to be more likely to have early-career researchers as first authors than standard reports (77\% vs 67\% in the journal \emph{Cortex}, Chambers \& Tzavella, 2021).
One explanation for this counterintuitive result could be that Registered Reports are disproportionally used by early-career researchers who intend to leave academia and thus have a concave fitness function.
Alternatively, factors or dynamics not considered in this simulation may swamp out the effects of concave \emph{vs} convex fitness functions, such as younger researchers being more likely to adopt new methods.
However, as we will see below, the effects of different fitness functions are not always as straightforward as in the simple case illustrated in Figure \ref{fig:epsilonplot} but produce different results in interaction with other risk-related factors.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{plots/plot_m_evo_rocket} 

}

\caption{Effect of research cycles per generation on evolved publication strategies. Shown are median evolved publication strategies (\(s\)) after 250 generations in 50 runs (tile colour represents the median of 50 run medians) depending on the number of research cycles per generation (\(m\), y-axis), different values of \(b_{RR}\) (x-axis), and different fitness functions (characterised by exponent \(\epsilon\)) with no survival threshold (\(\delta = 0\)) and no competition (\(\gamma = 1\)).}\label{fig:mplot}
\end{figure}

\hypertarget{varying-the-number-of-research-cycles-per-generation}{%
\subsection{Varying the number of research cycles per generation}\label{varying-the-number-of-research-cycles-per-generation}}

The analyses presented so far focused on the simple case of one research cycle (or decision event) per generation, meaning that researchers' fitness was calculated based on the payoff from one single study.
As discussed above, increasing numbers of decision events prior to evaluation may make individuals more risk-prone because single negative outcomes are less catastrophic for reproduction (Haaland et al., 2019).
However, Figure \ref{fig:mplot} shows that this is not universally true\(\,\)---\(\,\)rather, the effect of increasing numbers of research cycles per generation (\(m\)) depends on the shape of the fitness function.
Moving up on the y-axis of each panel, we see that \(s\) decreases (indicating greater risk proneness) only when the fitness function is concave (\(\epsilon = 0.2\), left panel) but stay constant when it is linear (\(\epsilon = 1\), middle panel) and even \emph{increases} when it is convex (\(\epsilon = 5\), right panel).

Why does \(m\) appear to have opposite effects for concave and convex fitness functions?
As a starting point, it helps to first consider only the bottom row of each panel, where \(m = 1\).
These three rows contain the same results as the top, middle, and bottom curves in Figure \ref{fig:epsilonplot} and show risk aversion when \(\epsilon = 0.2\) (i.e., Registered Reports are attractive even when they yield a low payoff), risk proneness when \(\epsilon = 5\) (Registered Reports are unattractive even when they yield a high payoff), and a linear strategy \(s_{optimal} = b_{RR}\) when \(\epsilon = 1\).
From this starting point, the two panels with non-linear fitness functions start to approximate the linear case as \(m\) increases.
This pattern reflects the idea that fitness is better captured by the geometric mean when \(m\) is low, and better captured by the arithmetic mean when \(m\) is high (Haaland et al., 2019).

To better understand this dynamic, let's consider two researchers with extreme submission strategies:
Regina Register conducts only Registered Reports (\(s_{Regina} = 1\)), Darren Daring conducts only standard reports (\(s_{Darren} = 0\)).
The payoff for Registered Reports is fixed at \(b_{RR} = 0.5\).
After one research cycle, Regina receives a payoff of 0.5 and Darren receives either 0 or 1 (with 50/50 odds).
If fitness is calculated after this one round with \(\epsilon = 0.2\) (concave function, yielding diminishing returns), Regina's fitness is \(f_{Regina} = \frac{1}{2}^{\frac{1}{5}} = 0.87\), and Darren's fitness is either \(f_{Darren-} = 0^{\frac{1}{5}} = 0\) or \(f_{Darren+} = 1^{\frac{1}{5}} = 1\).
In a population of 100 Reginas and 100 Darrens, there will be roughly 50 lucky Darrens who get a positive result and 50 Darrens who get a negative result.
Lucky Darrens have a narrow fitness advantage over all Reginas (1 versus 0.87), while unlucky Darrens lose to all Reginas by a wide margin (0 versus 0.87).
Since there are twice as many Reginas as lucky Darrens, the Regina strategy is relatively more successful.

Let's now consider the same scenario with \(m = 4\) research cycles per generation.
Reginas receive the same payoff in every round and accumulate \(b_{total} = \frac{1}{2} * 4 = 2\).
Lucky Darrens (who win every time) accumulate \(b_{total} = 1*4 = 4\), while unlucky Darrens (who lose every time) again receive 0 total payoff.
Now, however, the probabilistic outcomes over 4 rounds lead to three additional versions of Darren: moderately lucky (winning 3/4 times), average (2/4, receiving the same total payoff as Reginas), and moderately unlucky (1/4).
Translating payoffs into fitness, the Regina strategy (\(f_{Regina} = 2^\frac{1}{5} = 1.15\)) still yields an enormous advantage compared to unlucky Darrens (\(f_{Darren_{unlucky}} = 0\)) and only a small disadvantage compared to lucky Darrens (\(f_{Darren_{lucky}} = 4^\frac{1}{5} = 1.32\)).
But this time, there are fewer Darrens who are less successful than Reginas because Reginas now share their place with average Darrens.
The relative fitness advantage of the Regina strategy thus decreases.
As the rate of research cycles per generation grows, the law of large numbers dictates that more and more Darrens achieve average total payoffs, while fewer and fewer Darrens achieve extreme total payoffs (winning 32 times in a row is much less probable than winning 4 times in a row).
This reduces the width of the Darren distribution until it approximates the Regina distribution\(\,\)---\(\,\)meaning that optimal publication strategies become identical to those optimal for a linear fitness function.

When the fitness function is convex (\(\epsilon = 5\), yielding increasing returns), the overall effect of increasing values of \(m\) is the same, with the only difference that Reginas are initially disadvantaged (because their fitness distance to the lucky half of Darrens is much greater than than to the unlucky Darrens).
With larger \(m\), more and more Darrens receive average total payoffs and share Regina's disadvantaged position (decreasing Regina's relative disadvantage), until the Darren distribution is again virtually equal to the Regina distribution.
Rather than causing absolute risk aversion, increasing values of \(m\) thus counter the effect of \(\epsilon\) and reduce the effects of concave and convex fitness functions to the linear case.
Consequently, the top rows (\(m = 32\)) of the top and bottom panels in Figure \ref{fig:mplot} resemble the stable pattern across all \(m\) shown in the middle panel.

Translated into terms of academic careers, this less intuitive pattern indicates that
being able to complete empirical studies at a higher rate\(\,\)---\(\,\)e.g., when working in a field where data collection is fast and cheap or when having more resources for data collection available\(\,\)---\(\,\)may cancel out the effects of different career stages.
This could partly explain why Registered Reports appear to be less popular among senior researchers (Chambers \& Tzavella, 2021) than we would expect based on the effects of different fitness functions alone:
Although additional publications likely yield diminishing returns in later career stages (concave fitness function), academic seniority often comes with resources that boost research output per time (e.g., more lab members).
As a consequence, established professors may be relatively indifferent to Registered Reports.
Regarding junior researchers (for whom additional publications have increasing returns on career success), the results suggest that they may be especially reluctant to use Registered Reports when they have very limited time or resources to produce publications before an important selection event, such as on short-term postdoc contracts (R. Müller \& de Rijcke, 2017).



\begin{figure}
\includegraphics[width=1\linewidth]{plots/plot_delta_tile_evo_epsilon_mako} \caption{Effect of survival thresholds on evolved publication strategies. Shown are median publication strategies (\(s\)) after 250 generations in 50 runs (tile colour represents the median of 50 run medians) depending on survival thresholds (\(\delta\), shown as vertical yellow line), fitness functions (characterised by exponent \(\epsilon\)), numbers of research cycles per generation (\(m\)), and values of \(b_{RR}\), in the absence of competition (\(\gamma = 1\)). Survival thresholds are set as proportions of \(m\), i.e., as a percentage of the maximum possible payoff in each condition. To reproduce, researchers must accumulate a total payoff exceeding \(\delta * m\).}\label{fig:deltaplot}
\end{figure}

\hypertarget{absolute-survival-thresholds}{%
\subsection{Absolute survival thresholds}\label{absolute-survival-thresholds}}

The survival thresholds (\(\delta\)) in our model represent absolute publication targets that researchers must meet in order to progress in their career.
The clearest examples for such thresholds are PhD regulations and tenure agreements.
To be awarded with a PhD, many institutions and faculties require candidates to have a certain number of their thesis chapters published in peer-reviewed journals.
Similarly, tenure agreements may include publication targets in the form of a minimum number of peer-reviewed publications within a certain time, sometimes also specifying minimal journal ranks (Liner \& Sewell, 2009).
Such requirements may represent low, medium, or high survival thresholds depending on how demanding they are (e.g., the proportion of thesis chapters that must be published).

We investigate the effects of survival thresholds representing 25\%, 50\%, and 75\% of the maximum possible payoff researchers can achieve in one generation.
When \(\delta > b_{RR}\), Registered Reports alone are not sufficient to reach the survival threshold (\(b_{RR}\) values to the left of the yellow line in Figure \ref{fig:deltaplot}).
For example, at \(m = 4\), a survival threshold of 75\% (\(\delta = .75\)) means that researchers must gain at least 3 points to be able to reproduce.
When \(b_{RR} = .7\), submitting four Registered Reports will only amount to 2.8 points in total, just short of meeting the threshold.
On the other hand, when \(b_{RR} = .8\) (i.e., just above \(\delta\)), four Registered Reports would yield 3.2 points and thus ensure reproduction.
Choosing the standard route some of the time can increase fitness even further, but also increases the risk of not meeting the survival threshold.
As a consequence, one may intuitively expect Registered Reports to be popular whenever \(\delta \leq b_{RR}\) and unpopular whenever \(\delta > b_{RR}\).

Figure \ref{fig:deltaplot} shows that this is true in many, but not all conditions.
First, we can see that survival thresholds have their biggest effect when the number of research cycles per generation is low\(\,\)---\(\,\)at high values of \(m\), publication strategies are virtually unaffected in all conditions.
Second, survival thresholds have a stronger effect when the fitness function is linear (\(\epsilon = 1\)) or concave (\(\epsilon = 0.2\)).
In these two conditions, they produce very similar patterns:
The Registered Report route is almost never chosen when \(b_{RR}\) is too low to meet the survival threshold (particularly at \(\delta = .25\) and \(\delta = .5\); less so at \(\delta = .75\)), and this effect tapers off as the number of research cycles increases.
Compared to baseline, the change is particularly striking for the concave fitness function (\(\epsilon = 0.2\), left column in Fig. \ref{fig:deltaplot}), where RRs are normally preferred at low \(m\).
When the survival threshold is high (\(\delta = .75\)) or the fitness function is concave, we can also see that Registered Reports become \emph{more} popular than baseline when they are worth just enough to pass the survival threshold.
For the convex fitness function (\(\epsilon = 5\)) on the other hand, survival thresholds of 25\% and 50\% seem to have no effect at all.
Only a high threshold of 75\% makes RRs even less popular when they have low value (\(b_{RR}\leq 0.4\)), especially when the number of research cycles is low.

What does this mean in practice?
In our model, fitness (according to the three different fitness functions) is calculated after the survival threshold has been met.
This is meant to mimic publication requirements that are expressed in raw numbers.
Importantly, it also means that our simulation shows which strategies during a PhD or on the tenure track lead to maximal fitness \emph{after} researchers have successfully obtained their PhD or have been granted tenure.
With this in mind, it becomes easier to understand the meaning of the different fitness functions.
As discussed above, PhD candidates plausibly receive increasing returns for additional publications (convex fitness function), unless they intend not to stay in academia, in which case returns are strongly decreasing (concave fitness function).
For researchers on the tenure track, the fitness function after achieving tenure is also likely concave, assuming a) that achieving tenure is one of the most important career goals for many (making further progress relatively less important) and b) that such individuals have already built up substantial publication records, to which any single addition makes less and less of a difference.
However, exceptions from this scenario may well exist, for example in situations where tenured researchers are under great pressure to obtain grant funding.

Translated to real-world scenarios, our results thus suggest the following implications:
First, survival thresholds are almost irrelevant when
researchers can complete large numbers of studies before they are evaluated (reflecting characteristics of the research field, available resources, or length of the evaluation period).
Second, researchers with a convex fitness function\(\,\)---\(\,\)such as PhD candidates who are pursuing an academic career\(\,\)---\(\,\)are only affected by high survival thresholds, which lead them to choose Registered Reports even less often than normal when their value is low.
Third, researchers with a concave fitness function\(\,\)---\(\,\)such as tenure candidates or PhD students who aim for careers outside of academia\(\,\)---\(\,\)are highly sensitive to the value of Registered Reports:
They virtually never conduct Registered Reports when their value is too low for meeting the survival threshold, but strongly prefer them when their value is sufficient (especially when empirical pace is low and/or the survival threshold is high).



\begin{figure}
\includegraphics[width=1\linewidth]{plots/plot_gamma_evo_epsilon_inferno} \caption{Effect of competition on evolved publication strategies. Shown are median evolved publication strategies (\(s\)) after 250 generations in 50 runs (tile colour represents the median of 50 run medians) depending on the intensity of competition (\(\gamma\), y-axis), numbers of research cycles per generation (\(m\)), different values of \(b_{RR}\) (x-axis), and different fitness functions (characterised by exponent \(\epsilon\)) with no survival threshold (\(\delta = 0\)). To reproduce, researchers must accumulate a total payoff in the top \(\gamma\) proportion of the population.}\label{fig:competitionplot}
\end{figure}

\hypertarget{competition}{%
\subsection{Competition}\label{competition}}

Competition occurs whenever the demand for academic positions or grant funding exceeds the supply.
Figure \ref{fig:competitionplot} shows that competition generally leads to an aversion of Registered Reports, as can be seen by the darkening of the plots when moving up from the bottom row of panels.
The only exception to this rule is very low competition:
When the top 90\% are allowed to reproduce (and only the bottom 10\% are rejected, \(\gamma = .9\)), Registered Reports become more popular than they are in the absence of competition.
This effect is strongest for the concave fitness function (\(\epsilon = 0.2\)), where it holds for almost all values of \(b_{RR}\) at very low numbers of \(m\) and for high values of \(b_{RR}\) at high numbers of \(m\).
When the fitness function is linear or convex, Registered Reports are chosen more often only when both \(b_{RR}\) and \(m\) are high.
At higher levels of competition (\(\gamma > .5\)), the differences between the fitness functions disappear.
In all three cases, Registered Reports are essentially wiped out for low numbers of research cycles (\(m\)), and this effect increases with competition (the higher the competition, the higher \(m\) must be for Registered Reports to still be viable).
Intense competition also negatively affects Registered Reports at high numbers of \(m\), but here the general pattern of the baseline condition (a linear increase of Registered Reports popularity with \(b_{RR}\)) remains intact.

Looking at the first three rows of panels in Figure \ref{fig:competitionplot} (1\%, 5\%, and 10\% competition), the extreme effect of competition at low \(m\) appears to decrease slightly when competition is highest (\(\gamma = .01\)), indicated by the dark bar at the bottom of each panel becoming slightly lighter.
This paradoxial result is not due to Registered Reports being more lucrative in those conditions.
Rather, competition is so extreme that the natural selection in our model starts operating more on chance than on individuals' traits.
Essentially, only individuals with the maximum possible payoff (publishing only standard reports with positive results) are able to reproduce.
Most likely to receive this maximum payoff are individuals who investigate hypotheses with high prior probabilities.
In our model, this is not a trait that can be passed on, but determined by random chance.
Among individuals who experience this kind of luck, the variance of publication strategy \(s\) should be high:
A hypothesis with prior \(p = .95\) will be submitted as a standard report and likely yield a positive result (and thus the maximum payoff) regardless of whether the researcher's publication strategy is as low as \(s = .1\) or has high as \(s = .9\).
The higher average \(s\) at low \(m\) under extreme competition thus reflects relaxed selection pressure on \(s\).
This is also evident by the shades of the dark bar at the bottom of the panels for \(\gamma = .01\) (Fig. \ref{fig:competitionplot}), which fluctuate randomly for each level of \(m\) rather than showing a specific pattern.
A clearer illustration of the effect can be found in Figure XXX in the appendix, which shows large increases in the variance of evolved publication strategies in these conditions.
At higher \(m\), selection on \(s\) stays intact simply because much fewer individuals will be very lucky 4, 8, 16, or 32 times in a row than once or twice in a row, and publication strategy thus remains an important factor.

This effect of relaxed selection is not an arbitrary feature of our model, but commonly encountered in natural populations (Snyder, Ellner, \& Hooker, 2021).
In many species, luck can have an outsized impact on survival and reproduction, rendering the effects of individual traits relatively less important.
Luck does not eliminate natural selection\footnote{This is also apparent in Figure XXX (Appendix): Although the variance of evolved \(s\) increases dramatically with high competition, it never spans the entire range of \(s\).}, but it can significantly slow it.
XXX TRY LONGER SIM RUNS \& REPORT HERE XXX
The phenomenon is related to one form of survivorship bias:
Looking at `survivors' of a highly selective process, one may erroneously infer that specific observable traits or behaviours of such individuals were the cause of their success when those were actually merely coincidental.

In the academic world, researchers compete for tenured positions and grants.
The level of competition may vary between research areas, countries, institutions, grant programmes, and so on.
Our findings suggest that intense competition may be a significant threat for the viability of Registered Reports, regardless of career stage.
This effect is particularly extreme when very few research cycles can be completed before an evaluation event (e.g., in fields with low empirical pace, in labs with few resources, or on short-term contracts):
In such situations, publication strategies that involve any amount of Registered Reports are only viable when competition is so high that success requires extraordinary luck.
In contrast, very low but non-zero levels of competition increase the popularity of Registered Reports, especially when their value is high, when the fitness function is concave (e.g., in later career stages), and when researchers can complete many studies before being evaluated.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

In the artificial world of the model presented here, the standard publication route is a coin toss\(\,\)---\(\,\)the probability of obtaining a publishable result is 50\% on average\footnote{This is the case because we modelled the prior probability of tested hypotheses as being uniformly distributed between 0 and 1 and as being identical to the probability of obtaining a positive (i.e., publishable) result.}, translating to an expected payoff of 0.5 points per study.
If Registered Reports are a safe alternative to this gamble and guarantee publication in every case, one might think that payoff-maximising researchers would prefer them whenever they are worth more than
the expected payoff from standard reports and avoid them whenever they are worth less.
This intuition, however, rests on the assumption
that the career benefits researchers receive from publications are linear and involve no step changes.\footnote{Linearity is violated when the fitness function is concave or convex (\(\epsilon \neq |1|\)), but also in the presence of survival thresholds or competition, because these effectively introduce a step-change in the fitness function (low but non-zero payoffs yield zero career benefits).}
We argue that this assumption is violated in many, if not all, real-world situations.
Here, we investigated the impact of four factors that likely shape real-world situations:
convex vs concave fitness functions (additional publications yielding either increasing or decreasing returns, reflecting early vs later career stages), empirical pace (reflecting differences in speed and cost of data collection, available resources, or available time), survival thresholds (reflecting absolute publication targets researchers must meet in a given time), and competition for jobs or grants.
Our results show that in isolation or combined, many of these factors
would lead researchers with career-maximising strategies to avoid Registered Reports\(\,\)---\(\,\)even when Registered Reports are worth more than the expected payoff from standard reports.

To summarise the results, it is useful to take
the middle panel of Figure \ref{fig:mplot} (\(\epsilon = 1\)) as a baseline.
In this panel, publication payoffs translate into linear career benefits (the fitness curve is linear and there is no survival threshold and no competition), and the outcome is highly intuitive:
Researchers prefer Registered Reports whenever they are worth more than 0.5 points, their preference is exactly proportional to \(b_{RR}\), and it is not affected by empirical pace.
Compared to this baseline, Registered Reports are \emph{less} popular when a) additional publications yield increasing returns (e.g., in early career) and empirical pace is low, b) when researchers face a survival threshold that cannot be met with Registered Reports alone, especially when publications yield decreasing returns (e.g., in advanced career stages) and empirical pace is low, and c) when there is substantial competition.
Competition has the most extreme effect and can cause a complete avoidance of Registered Reports when empirical pace is low.
Conversely, Registered Reports are \emph{more} popular than at baseline when a) additional publications yield decreasing returns and empirical pace is low, b) Registered Reports are worth just enough to reach a survival threshold and publications yield decreasing returns, especially when empirical pace is low, and c) when there is very low but non-zero competition, especially when publications yield decreasing returns or empirical pace is high.

Looking at the interactions of the different factors, three observations stand out.
First, high empirical pace attenuates the effects of all other factors\(\,\)---\(\,\)at the highest pace we considered (\(m = 32\)), outcomes are identical to baseline in almost all conditions.
The only exception to this rule is high competition, but although Registered Reports are relatively less attractive in this condition, the basic pattern is preserved and they remain viable when their value is high.
Second, the effect of survival thresholds strongly depends on the shape of the fitness function, suggesting that publication targets may have the strongest impact in advanced career stages.
Third, the opposite is true for high competition, which cancels out the effects of different fitness functions and thus appears to have virtually the same impact across career stages.

\hypertarget{implications}{%
\subsection{Implications}\label{implications}}

\begin{itemize}
\tightlist
\item
  Fields with low pace/labs with low resources are most susceptible to other factors
\item
  Tenure track: value of RRs extremely important
\item
  Grants: strategy to only sift out the worst application and raffle among the rest would favour RR-heavy strategy
\item
  competition: relate to competition for priority \& potential interaction with up-front cost of RRs
\end{itemize}

\texttt{To\ do:}

\begin{itemize}
\tightlist
\item
  Implications of results

  \begin{itemize}
  \tightlist
  \item
    cautious mapping of model factors to real-world situations
  \item
    potential implications for meta-science
  \item
    potential implications for policy
  \end{itemize}
\end{itemize}

\hypertarget{limitations}{%
\subsection{Limitations}\label{limitations}}

\begin{itemize}
\item
  Narrow focus on one specific (and highly stylised) difference between Registered Reports and standard reports; there are many others. Model ignores a myriad other factors that influences who chooses Registered Reports for which studies when
\item
  Concept of publication bias as filtering positive results of hypothesis tests (and the respective connection to hypothesis priors such that high priors --\textgreater{} better) is cartoonish and not entirely accurate for the simple reason that positive results of trivial (or otherwise boring) hypotheses are usually not highly valued (also, this approach only focuses on hypothesis testing, which is widely used in psychology but by far not the only means of doing science).
  A more valid solution may be the concept of publication bias as favouring belief-shifting results presented by Gross \& Bergstrom (2021).
  Adapting the model presented here to capture this concept of bias could be an interesting future direction.
  However, the present version of the model also allows a conservative interpretation in which the prior probability of hypotheses simply reflects authors' predictions of the eventual publication value of different research questions.
  This interpretation is still concordant with Registered Reports and standard reports differing in risk, because the publication value of standard reports certainly depends more strongly on the study results than the publication value of Registered Reports (even if not in the simplistic sense of positive hypothesis tests having higher value).
\item
  Fitness concept: one caveat is that
\item
  RRs may actually \emph{slow} the empirical pace, introducing an interaction that our model doesn't take into account
\item
  Fitness curves: more senior researchers may also take the needs of their early-career mentees into account
\end{itemize}

\hypertarget{future-directions}{%
\subsection{Future directions}\label{future-directions}}

\hypertarget{ability-based-risk-taking}{%
\paragraph{Ability-based risk taking}\label{ability-based-risk-taking}}

The model presented in this chapter only considers the effects of situational factors on individuals' risk sensitivity.
However, risk sensitivity can also be influenced by individual differences, such that individuals with traits or abilities that increase their expected payoff from a risky option (e.g., traits that increase their winning chances or the payoff when winning or that buffer the impact of losses) should be more risk-prone (Barclay, Mishra, \& Sparks, 2018).
Such factors may be important to consider in the context of research and publication practices.
For example, researchers who are better at choosing research questions that are likely to result in high-impact publications (e.g., through talent or experience) may find Registered Reports less attractive.
As a more nefarious version of this idea, Registered Reports may be relatively unpopular among researchers who are more inclined to using questionable research practices (or even fraud) to obtain publishable or impactful results.

\hypertarget{registered-reports-and-post-publication-peer-review}{%
\paragraph{Registered Reports and post-publication peer review}\label{registered-reports-and-post-publication-peer-review}}

The post-publication peer review platform \emph{Peer Community In} (PCI) recently launched a new model of Registered Reports (PCI Registered Reports) in which authors are no longer tied to a specific journal.
PCI offers authors the regular process of stage-1 and stage-2 review, the end result of a successful submission is `only' a preprint with a so-called `recommendation' from PCI.
Authors can subsequently publish their manuscript in one of several journals who partnered with PCI and either rely on the PCI review process alone or offer a streamlined review process for PCI-recommended preprints, or they can submit to any other journal as if their manuscript were a standard report.
This innovation gives Registered-Reports authors significantly more freedom to capitalise on the results of their study because a submission to PCI Registered Reports does not preclude the chance of a high-impact publication.
PCI Registered Reports thus constitute a significant change to the relative incentives and risk structure of Registered Reports compared to standard reports that merits a closer investigation in the future.

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

\hypertarget{disclosures}{%
\subsection{Disclosures}\label{disclosures}}

\hypertarget{data-materials-and-online-resources}{%
\subsubsection{Data, materials, and online resources}\label{data-materials-and-online-resources}}

This manuscript was created using RStudio (1.2.5019, RStudio Team, 2019) and R (Version 4.2.1; R Core Team, 2019) and the R-packages \emph{bookdown} (Version 0.34; Xie, 2016), \emph{ggplot2} (Version 3.5.0; Wickham, 2016), \emph{here} (Version 1.0.1; K. Müller, 2017), \emph{knitr} (Version 1.46; Xie, 2015), \emph{papaja} (Version 0.1.1.9001; Aust \& Barth, 2018), \emph{rmarkdown} (Version 2.26; Xie, Allaire, \& Grolemund, 2018), \emph{stringr} (Version 1.5.1; Wickham, 2023), and \emph{tinylabels} (Version 0.2.3; Barth, 2022).

\hypertarget{references}{%
\section{References}\label{references}}

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}


\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Agnoli2017}{}}%
Agnoli, F., Wicherts, J. M., Veldkamp, C. L. S., Albiero, P., \& Cubelli, R. (2017). Questionable research practices among italian research psychologists. \emph{PLOS ONE}, \emph{12}(3), e0172792. \url{https://doi.org/10.1371/journal.pone.0172792}

\leavevmode\vadjust pre{\hypertarget{ref-Allen2019}{}}%
Allen, C., \& Mehler, D. M. A. (2019). Open science challenges, benefits and tips in early career and beyond. \emph{PLOS Biology}, \emph{17}(5), e3000246. \url{https://doi.org/10.1371/journal.pbio.3000246}

\leavevmode\vadjust pre{\hypertarget{ref-Atkinson1982}{}}%
Atkinson, D. R., Furlong, M. J., \& Wampold, B. E. (1982). Statistical significance, reviewer evaluations, and the scientific process: {Is} there a (statistically) significant relationship? \emph{Journal of Counseling Psychology}, \emph{29}(2), 189--194. \url{https://doi.org/10.1037/0022-0167.29.2.189}

\leavevmode\vadjust pre{\hypertarget{ref-R-papaja}{}}%
Aust, F., \& Barth, M. (2018). \emph{{papaja}: {Create APA} manuscripts with {R Markdown}}.

\leavevmode\vadjust pre{\hypertarget{ref-Barclay2018}{}}%
Barclay, P., Mishra, S., \& Sparks, A. M. (2018). State-dependent risk-taking. \emph{Proceedings of the Royal Society B: Biological Sciences}, \emph{285}(1881), 20180180. \url{https://doi.org/10.1098/rspb.2018.0180}

\leavevmode\vadjust pre{\hypertarget{ref-R-tinylabels}{}}%
Barth, M. (2022). \emph{{tinylabels}: Lightweight variable labels}. Retrieved from \url{https://cran.r-project.org/package=tinylabels}

\leavevmode\vadjust pre{\hypertarget{ref-Chalmers2009}{}}%
Chalmers, I., \& Glasziou, P. (2009). Avoidable waste in the production and reporting of research evidence. \emph{The Lancet}, \emph{374}(9683), 86--89. \url{https://doi.org/10.1016/S0140-6736(09)60329-9}

\leavevmode\vadjust pre{\hypertarget{ref-Chambers2013}{}}%
Chambers, C. D. (2013). Registered reports: {A} new publishing initiative at {Cortex}. \emph{Cortex}, \emph{49}, 606--610. \url{https://doi.org/10.1016/j.cortex.2012.12.016}

\leavevmode\vadjust pre{\hypertarget{ref-Chambers2015}{}}%
Chambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., \& Willmes, K. (2015). Registered {Reports}: {Realigning} incentives in scientific publishing. \emph{Cortex}, \emph{66}, 1--2. \url{https://doi.org/10.1016/j.cortex.2015.03.022}

\leavevmode\vadjust pre{\hypertarget{ref-Chambers2021}{}}%
Chambers, C. D., \& Tzavella, L. (2021). The past, present and future of {Registered Reports}. \emph{Nature Human Behaviour}, 1--14. \url{https://doi.org/10.1038/s41562-021-01193-7}

\leavevmode\vadjust pre{\hypertarget{ref-deVries2018}{}}%
de Vries, Y. A., Roest, A. M., Jonge, P. de, Cuijpers, P., Munafò, M. R., \& Bastiaansen, J. A. (2018). The cumulative effect of reporting and citation biases on the apparent efficacy of treatments: The case of depression. \emph{Psychological Medicine}, \emph{48}(15), 2453--2455. \url{https://doi.org/10.1017/S0033291718001873}

\leavevmode\vadjust pre{\hypertarget{ref-Dickersin1993}{}}%
Dickersin, K., \& Min, Y. I. (1993). Publication bias: The problem that won't go away. \emph{Annals of the New York Academy of Sciences}, \emph{703}, 135-146; discussion 146-148. \url{https://doi.org/10.1111/j.1749-6632.1993.tb26343.x}

\leavevmode\vadjust pre{\hypertarget{ref-Driessen2015}{}}%
Driessen, E., Hollon, S. D., Bockting, C. L. H., Cuijpers, P., \& Turner, E. H. (2015). Does {Publication Bias Inflate} the {Apparent Efficacy} of {Psychological Treatment} for {Major Depressive Disorder}? {A Systematic Review} and {Meta-Analysis} of {US National Institutes} of {Health-Funded Trials}. \emph{PLOS ONE}, \emph{10}(9), e0137864. \url{https://doi.org/10.1371/journal.pone.0137864}

\leavevmode\vadjust pre{\hypertarget{ref-Ferguson2012}{}}%
Ferguson, C. J., \& Heene, M. (2012). A {Vast Graveyard} of {Undead Theories}: {Publication Bias} and {Psychological Science}'s {Aversion} to the {Null}. \emph{Perspectives on Psychological Science}, \emph{7}(6), 555--561. \url{https://doi.org/10.1177/1745691612459059}

\leavevmode\vadjust pre{\hypertarget{ref-Fiedler2016}{}}%
Fiedler, K., \& Schwarz, N. (2016). Questionable {Research Practices Revisited}. \emph{Social Psychological and Personality Science}, \emph{7}(1), 45--52. \url{https://doi.org/10.1177/1948550615612150}

\leavevmode\vadjust pre{\hypertarget{ref-Franco2014}{}}%
Franco, A., Malhotra, N., \& Simonovits, G. (2014). Publication bias in the social sciences: {Unlocking} the file drawer. \emph{Science}, \emph{345}(6203), 1502--1505. \url{https://doi.org/10.1126/science.1255484}

\leavevmode\vadjust pre{\hypertarget{ref-Franco2016}{}}%
Franco, A., Malhotra, N., \& Simonovits, G. (2016). Underreporting in {Psychology Experiments}: {Evidence From} a {Study Registry}. \emph{Social Psychological and Personality Science}, \emph{7}(1), 8--12. \url{https://doi.org/10.1177/1948550615598377}

\leavevmode\vadjust pre{\hypertarget{ref-Fraser2018}{}}%
Fraser, H., Parker, T., Nakagawa, S., Barnett, A., \& Fidler, F. (2018). Questionable research practices in ecology and evolution. \emph{PLOS ONE}, \emph{13}(7), e0200303. \url{https://doi.org/10.1371/journal.pone.0200303}

\leavevmode\vadjust pre{\hypertarget{ref-Gerber2001}{}}%
Gerber, A. S., Green, D. P., \& Nickerson, D. (2001). Testing for {Publication Bias} in {Political Science}. \emph{Political Analysis}, \emph{9}(4), 385--392. \url{https://doi.org/10.1093/oxfordjournals.pan.a004877}

\leavevmode\vadjust pre{\hypertarget{ref-Gopalakrishna2022}{}}%
Gopalakrishna, G., Wicherts, J. M., Vink, G., Stoop, I., Akker, O. R. van den, Riet, G. ter, \& Bouter, L. M. (2022). \emph{Prevalence of responsible research practices among academics in {The Netherlands}}. \emph{11}(471). \url{https://doi.org/10.12688/f1000research.110664.2}

\leavevmode\vadjust pre{\hypertarget{ref-Greenwald1975}{}}%
Greenwald, A. G. (1975). Consequences of {Prejudice Against} the {Null Hypothesis}. \emph{Psychological Bulletin}, \emph{82}(1), 1--20.

\leavevmode\vadjust pre{\hypertarget{ref-Gross2021}{}}%
Gross, K., \& Bergstrom, C. T. (2021). Why ex post peer review encourages high-risk research while ex ante review discourages it. \emph{Proceedings of the National Academy of Sciences}, \emph{118}(51). \url{https://doi.org/10.1073/pnas.2111615118}

\leavevmode\vadjust pre{\hypertarget{ref-Haaland2019}{}}%
Haaland, T. R., Wright, J., \& Ratikainen, I. I. (2019). Bet-hedging across generations can affect the evolution of variance-sensitive strategies within generations. \emph{Proceedings of the Royal Society B}. \url{https://doi.org/10.1098/rspb.2019.2070}

\leavevmode\vadjust pre{\hypertarget{ref-Hurly2003}{}}%
Hurly, A. T. (2003). The twin threshold model: Risk-intermediate foraging by rufous hummingbirds, {Selasphorus} rufus. \emph{Animal Behaviour}, \emph{66}(4), 751--761. \url{https://doi.org/10.1006/anbe.2003.2278}

\leavevmode\vadjust pre{\hypertarget{ref-John2012}{}}%
John, L. K., Loewenstein, G., \& Prelec, D. (2012). Measuring the {Prevalence} of {Questionable Research Practices With Incentives} for {Truth Telling}. \emph{Psychological Science}, \emph{23}(5), 524--532. \url{https://doi.org/10.1177/0956797611430953}

\leavevmode\vadjust pre{\hypertarget{ref-Kacelnik1996}{}}%
Kacelnik, A., \& Bateson, M. (1996). Risky {Theories}---{The Effects} of {Variance} on {Foraging Decisions}. \emph{Integrative and Comparative Biology}, \emph{36}(4), 402--434. \url{https://doi.org/10.1093/icb/36.4.402}

\leavevmode\vadjust pre{\hypertarget{ref-Kacelnik1997}{}}%
Kacelnik, A., \& Bateson, M. (1997). Risk-sensitivity: Crossroads for theories of decision-making. \emph{Trends in Cognitive Sciences}, \emph{1}(8), 304--309. \url{https://doi.org/10.1016/s1364-6613(97)01093-0}

\leavevmode\vadjust pre{\hypertarget{ref-Kepes2022}{}}%
Kepes, S., Keener, S. K., McDaniel, M. A., \& Hartman, N. S. (2022). Questionable research practices among researchers in the most research-productive management programs. \emph{Journal of Organizational Behavior}, \emph{43}(7), 1190--1208. \url{https://doi.org/10.1002/job.2623}

\leavevmode\vadjust pre{\hypertarget{ref-Liner2009}{}}%
Liner, G. H., \& Sewell, E. (2009). Research requirements for promotion and tenure at {PhD} granting departments of economics. \emph{Applied Economics Letters}. \url{https://doi.org/10.1080/13504850701221998}

\leavevmode\vadjust pre{\hypertarget{ref-Mahoney1977}{}}%
Mahoney, M. J. (1977). Publication {Prejudices}: {An Experimental Study} of {Confirmatory Bias} in the {Peer Review System}. \emph{Cognitive Therapy and Research}, \emph{1}(2), 161--175. \url{https://doi.org/10.1007/BF01173636}

\leavevmode\vadjust pre{\hypertarget{ref-Makel2021}{}}%
Makel, M. C., Hodges, J., Cook, B. G., \& Plucker, J. A. (2021). Both {Questionable} and {Open Research Practices Are Prevalent} in {Education Research}: \emph{Educational Researcher}. \url{https://doi.org/10.3102/0013189X211001356}

\leavevmode\vadjust pre{\hypertarget{ref-Miller2011a}{}}%
Miller, A. N., Taylor, S. G., \& Bedeian, A. G. (2011). Publish or perish: Academic life as management faculty live it. \emph{Career Development International}, \emph{16}(5), 422--445. \url{https://doi.org/10.1108/13620431111167751}

\leavevmode\vadjust pre{\hypertarget{ref-Mishra2014}{}}%
Mishra, S. (2014). Decision-{Making Under Risk}: {Integrating Perspectives From Biology}, {Economics}, and {Psychology}. \emph{Personality and Social Psychology Review}, \emph{18}(3), 280--307. \url{https://doi.org/10.1177/1088868314530517}

\leavevmode\vadjust pre{\hypertarget{ref-R-here}{}}%
Müller, K. (2017). \emph{Here: {A} simpler way to find your files}.

\leavevmode\vadjust pre{\hypertarget{ref-Muller2014}{}}%
Müller, R. (2014). Postdoctoral {Life Scientists} and {Supervision Work} in the {Contemporary University}: {A Case Study} of {Changes} in the {Cultural Norms} of {Science}. \emph{Minerva}, \emph{52}(3), 329--349. \url{https://doi.org/10.1007/s11024-014-9257-y}

\leavevmode\vadjust pre{\hypertarget{ref-Muller2017}{}}%
Müller, R., \& de Rijcke, S. (2017). Thinking with indicators. {Exploring} the epistemic impacts of academic performance indicators in the life sciences. \emph{Research Evaluation}, \emph{26}(3), 157--168. \url{https://doi.org/10.1093/reseval/rvx023}

\leavevmode\vadjust pre{\hypertarget{ref-OBoyle2017}{}}%
O'Boyle, E. H., Banks, G. C., \& Gonzalez-Mulé, E. (2017). The {Chrysalis Effect}: {How Ugly Initial Results Metamorphosize Into Beautiful Articles}. \emph{Journal of Management}, \emph{43}(2), 376--399. \url{https://doi.org/10.1177/0149206314527133}

\leavevmode\vadjust pre{\hypertarget{ref-OMahony2023}{}}%
O'Mahony, A. (2023). \emph{Comparative analysis of {Registered Reports} and the standard research literature} (PhD thesis). Cardiff University.

\leavevmode\vadjust pre{\hypertarget{ref-Paruzel-Czachura2021}{}}%
Paruzel-Czachura, M., Baran, L., \& Spendel, Z. (2021). Publish or be ethical? {Publishing} pressure and scientific misconduct in research. \emph{Research Ethics}, \emph{17}(3), 375--397. \url{https://doi.org/10.1177/1747016120980562}

\leavevmode\vadjust pre{\hypertarget{ref-R-base}{}}%
R Core Team. (2019). \emph{R: {A} language and environment for statistical computing}. Vienna, Austria: R Foundation for Statistical Computing.

\leavevmode\vadjust pre{\hypertarget{ref-Rosenthal1979}{}}%
Rosenthal, R. (1979). The file drawer problem and tolerance for null results. \emph{Psychological Bulletin}, \emph{86}(3), 638--641. \url{https://doi.org/10.1037/0033-2909.86.3.638}

\leavevmode\vadjust pre{\hypertarget{ref-RStudioTeam2019}{}}%
RStudio Team. (2019). \emph{{RStudio}: {Integrated} development environment for r}. Boston, MA: RStudio, Inc.

\leavevmode\vadjust pre{\hypertarget{ref-Scheel2021a}{}}%
Scheel, A. M., Schijen, M. R. M. J., \& Lakens, D. (2021). An {Excess} of {Positive Results}: {Comparing} the {Standard Psychology Literature With Registered Reports}. \emph{Advances in Methods and Practices in Psychological Science}, \emph{4}(2), 251524592110074. \url{https://doi.org/10.1177/25152459211007467}

\leavevmode\vadjust pre{\hypertarget{ref-Simmons2011}{}}%
Simmons, J. P., Nelson, L. D., \& Simonsohn, U. (2011). False-positive psychology: {Undisclosed} flexibility in data collection and analysis allows presenting anything as significant. \emph{Psychological Science}, \emph{22}(11), 1359--1366. \url{https://doi.org/10.1177/0956797611417632}

\leavevmode\vadjust pre{\hypertarget{ref-Smaldino2016}{}}%
Smaldino, P. E., \& McElreath, R. (2016). The natural selection of bad science. \emph{Royal Society Open Science}, \emph{3}, 160384. \url{https://doi.org/10.1098/rsos.160384}

\leavevmode\vadjust pre{\hypertarget{ref-Snyder2021}{}}%
Snyder, R. E., Ellner, S. P., \& Hooker, G. (2021). Time and {Chance}: {Using Age Partitioning} to {Understand How Luck Drives Variation} in {Reproductive Success}. \emph{The American Naturalist}, \emph{197}(4), E110--E128. \url{https://doi.org/10.1086/712874}

\leavevmode\vadjust pre{\hypertarget{ref-Soderberg2021}{}}%
Soderberg, C. K., Errington, T. M., Schiavone, S. R., Bottesini, J., Thorn, F. S., Vazire, S., \ldots{} Nosek, B. A. (2021). Initial evidence of research quality of registered reports compared with the standard publishing model. \emph{Nature Human Behaviour}, \emph{5}(8), 990--997. \url{https://doi.org/10.1038/s41562-021-01142-4}

\leavevmode\vadjust pre{\hypertarget{ref-Stefan2023}{}}%
Stefan, A. M., \& Schönbrodt, F. D. (2023). Big little lies: A compendium and simulation of p-hacking strategies. \emph{Royal Society Open Science}, \emph{10}(2), 220346. \url{https://doi.org/10.1098/rsos.220346}

\leavevmode\vadjust pre{\hypertarget{ref-Tijdink2013}{}}%
Tijdink, J. K., Vergouwen, A. C. M., \& Smulders, Y. M. (2013). Publication {Pressure} and {Burn Out} among {Dutch Medical Professors}: {A Nationwide Survey}. \emph{PLOS ONE}, \emph{8}(9), e73381. \url{https://doi.org/10.1371/journal.pone.0073381}

\leavevmode\vadjust pre{\hypertarget{ref-vanDalen2021}{}}%
van Dalen, H. P. (2021). How the publish-or-perish principle divides a science: The case of economists. \emph{Scientometrics}, \emph{126}(2), 1675--1694. \url{https://doi.org/10.1007/s11192-020-03786-x}

\leavevmode\vadjust pre{\hypertarget{ref-vanDalen2012}{}}%
van Dalen, H. P., \& Henkens, K. (2012). Intended and unintended consequences of a publish-or-perish culture: {A} worldwide survey. \emph{Journal of the American Society for Information Science and Technology}, \emph{63}(7), 1282--1293. \url{https://doi.org/10.1002/asi.22636}

\leavevmode\vadjust pre{\hypertarget{ref-Waaijer2018}{}}%
Waaijer, C. J. F., Teelken, C., Wouters, P. F., \& van der Weijden, I. C. M. (2018). Competition in {Science}: {Links Between Publication Pressure}, {Grant Pressure} and the {Academic Job Market}. \emph{Higher Education Policy}, \emph{31}(2), 225--243. \url{https://doi.org/10.1057/s41307-017-0051-y}

\leavevmode\vadjust pre{\hypertarget{ref-Wagenmakers2012}{}}%
Wagenmakers, E.-J., Wetzels, R., Borsboom, D., van der Maas, H. L. J., \& Kievit, R. A. (2012). An {Agenda} for {Purely Confirmatory Research}. \emph{Perspectives on Psychological Science}, \emph{7}(6), 632--638. \url{https://doi.org/10.1177/1745691612463078}

\leavevmode\vadjust pre{\hypertarget{ref-R-ggplot2}{}}%
Wickham, H. (2016). \emph{Ggplot2: {Elegant} graphics for data analysis}. Springer-Verlag New York.

\leavevmode\vadjust pre{\hypertarget{ref-R-stringr}{}}%
Wickham, H. (2023). \emph{Stringr: Simple, consistent wrappers for common string operations}. Retrieved from \url{https://CRAN.R-project.org/package=stringr}

\leavevmode\vadjust pre{\hypertarget{ref-Winterhalder1999}{}}%
Winterhalder, B., Lu, F., \& Tucker, B. (1999). Risk-senstive adaptive tactics: {Models} and evidence from subsistence studies in biology and anthropology. \emph{Journal of Archaeological Research}, \emph{7}(4), 301--348. \url{https://doi.org/10.1007/BF02446047}

\leavevmode\vadjust pre{\hypertarget{ref-R-knitr}{}}%
Xie, Y. (2015). \emph{Dynamic documents with {R} and knitr} (2nd ed.). Boca Raton, Florida: {Chapman and Hall/CRC}.

\leavevmode\vadjust pre{\hypertarget{ref-R-bookdown}{}}%
Xie, Y. (2016). \emph{Bookdown: {Authoring} books and technical documents with {R} markdown}. Boca Raton, Florida: {Chapman and Hall/CRC}.

\leavevmode\vadjust pre{\hypertarget{ref-R-rmarkdown}{}}%
Xie, Y., Allaire, J. J., \& Grolemund, G. (2018). \emph{R markdown: {The} definitive guide}. Boca Raton, Florida: {Chapman and Hall/CRC}.

\end{CSLReferences}

\end{document}
